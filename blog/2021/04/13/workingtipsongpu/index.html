<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>WorkingTipsOnGpu &middot; Dash</title>

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css">
  <link rel="stylesheet" href="https://purplepalmdash.github.io/css/poole.css?ref=abc124">
  <link rel="stylesheet" href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124">
  <link rel="stylesheet" href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124">
  <link rel="stylesheet" href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124">
  <link rel="stylesheet" href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124">
  <link rel="stylesheet" href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124">
  <link rel="stylesheet" href="https://purplepalmdash.github.io/css/highlight/googlecode.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css">
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
  <script type="text/javascript" src="/js/html2canvas.js"></script>
  <script type='text/javascript'>
  function genPostShot() { 
          var rightNow = new Date();
          var imageName = rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");
          imageName += '.jpg'
          html2canvas(document.getElementsByClassName('post'), {
              background :'#FFFFFF',
              onrendered: function(canvas) {
  		
          	$('#test').attr('href', canvas.toDataURL("image/jpeg"));
          	$('#test').attr('download',imageName);
          	$('#test')[0].click();
              }
          });
  }; 
  
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124">
  <link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel="icon">

  
  
  
  
  <meta name="description" content="WorkingTipsOnGpu">
  <meta name="keywords" content="Technology">
  
  
</head>
<body class="theme-base-0c">
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <img src="http://purplepalmdash.github.io/images/mylogo.jpeg" alt="gravatar">
      <h1><a href="http://purplepalmdash.github.io/">Get busy living, or get busy dying.</a></h1>
      <a href="http://purplepalmdash.github.io/"><p>Dash</p></a>
    </div>

    <ul class="sidebar-nav">
      
      <li class="sidebar-nav-item"><a href="http://purplepalmdash.github.io/post/">All Posts</a></li>
      
      <li class="sidebar-nav-item"><a href="http://purplepalmdash.github.io/categories/technology/">Technology</a></li>
      
      <li class="sidebar-nav-item"><a href="http://purplepalmdash.github.io/categories/life/">Life</a></li>
      
      <li class="sidebar-nav-item"><a href="http://purplepalmdash.github.io/categories/linuxtips/">LinuxTips</a></li>
      
    </ul>

    <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
      <a href="https://github.com/purplepalmdash"><i class="fa fa-github-square fa-3x"></i></a>
      
      <a href="https://cn.linkedin.com/in/yang-feipeng-1b909319"><i class="fa fa-linkedin-square fa-3x"></i></a>
      <a href="https://plus.google.com/u/0/106572959364703833986"><i class="fa fa-google-plus-square fa-3x"></i></a>
      <a href="https://www.facebook.com/yang.feipeng"><i class="fa fa-facebook-square fa-3x"></i></a>
      <a href="https://twitter.com/dashwillfly"><i class="fa fa-twitter-square fa-3x"></i></a>
      
      </li>
    </ul>

    

  </div>
</div>


<div class="content container">
  <div class="post">
    <h1>WorkingTipsOnGpu</h1>
    <p align="right">
    <a href="javascript:genPostShot()">
	    TurnToJPG --> <i class="fa fa-camera-retro fa-2x"></i>
    </a>
    <a id="test"></a>
    </p>
    <hr>
    <span class="post-date">Apr 13, 2021 
    
    <br/>
    
          <a class="a_cat" href="http://purplepalmdash.github.io/categories/technology">Technology</a>
      
      

    
    </span>
      <div id="_toc" class="toc">
        <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#1-环境配置信息">1. 环境配置信息</a></li>
        <li><a href="#2-部署ccse集群">2. 部署CCSE集群</a></li>
        <li><a href="#3-升级内核">3. 升级内核</a></li>
        <li><a href="#4-gpu-operator文件准备">4. gpu-operator文件准备</a></li>
        <li><a href="#5-安装nvidiagpu-operator">5. 安装NVIDIA/gpu-operator</a></li>
        <li><a href="#6-测试gpu">6. 测试GPU</a></li>
      </ul>
    </li>
  </ul>
</nav>
      </div>
    <h3 id="1-环境配置信息">1. 环境配置信息</h3>
<p>整个验证环境的配置信息如下:</p>
<pre><code>gpumaster: 10.168.100.2	4核16G
gpunode1: 10.168.100.3	4核16G PCI直通B5:00 Tesla V100
gpunode2: 10.168.100.4	4核16G PCI直通B2:00 Tesla V100
</code></pre><p>节点的操作系统配置如下, CentOS 7.6最小化安装方式:</p>
<pre><code># uname -a
Linux gpumaster 3.10.0-957.el7.x86_64 #1 SMP Thu Nov 8 23:39:32 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
# cat /etc/redhat-release 
CentOS Linux release 7.6.1810 (Core)
</code></pre><p>其中master节点上外挂了一块500G 的数据盘，需要手动挂载至<code>/dcos</code>目录:</p>
<pre><code>[root@gpumaster ~]# df -h | grep dcos
/dev/vdb1                493G   73M  467G   1% /dcos
[root@gpumaster ~]# cat /etc/fstab | grep dcos
/dev/vdb1        /dcos                       ext4       defaults        0 0
</code></pre><p>3个节点依次关闭selinux/firewalld:</p>
<pre><code># vi /etc/selinux/config
...
SELINUX=disabled
...
# systemctl disable firewalld
# reboot
</code></pre><h3 id="2-部署ccse集群">2. 部署CCSE集群</h3>
<p>依次添加节点:</p>
<p><img src="/images/2021_04_19_09_01_57_825x247.jpg" alt="/images/2021_04_19_09_01_57_825x247.jpg"></p>
<p>新增一个名为<code>gpucluster</code>的集群:</p>
<p><img src="/images/2021_04_19_09_06_40_828x248.jpg" alt="/images/2021_04_19_09_06_40_828x248.jpg"></p>
<p>集群创建完毕后，新增两个GPU节点：</p>
<p><img src="/images/2021_04_19_09_16_24_1099x449.jpg" alt="/images/2021_04_19_09_16_24_1099x449.jpg"></p>
<p>添加完成后，检查集群状态:</p>
<pre><code>[root@gpumaster ~]# kubectl get node
NAME           STATUS   ROLES    AGE     VERSION
10.168.100.2   Ready    master   6m19s   v1.17.3
10.168.100.3   Ready    node     78s     v1.17.3
10.168.100.4   Ready    node     78s     v1.17.3
</code></pre><h3 id="3-升级内核">3. 升级内核</h3>
<p>在三个节点上，依次执行以下操作以升级内核。</p>
<p>配置离线软件库:</p>
<pre><code># cd /etc/yum.repos.d
# mkdir back
# mv CentOS-* back
# vi nvidia.repo
[nvidia]
name=nvidia
baseurl=http://10.168.100.144:8200/repo/x86_64/nvidiarpms
gpgcheck=0
enabled=1
proxy=_none_
# yum install -y kernel-ml
</code></pre><p>配置grub启动:</p>
<pre><code># vi /etc/default/grub
...
GRUB_DEFAULT=0
...
GRUB_CMDLINE_LINUX=&quot;crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet rd.driver.blacklist=nouveau nouveau.modeset=0&quot;
...
# grub2-mkconfig -o /boot/grub2/grub.cfg
</code></pre><p>完全禁用系统自带的<code>nouveau</code>驱动:</p>
<pre><code># echo 'install nouveau /bin/false' &gt;&gt; /etc/modprobe.d/nouveau.conf
</code></pre><p>执行完上述操作后需重启机器并验证内核是否更改成功:</p>
<pre><code># uname -a
Linux gpunode2 4.19.12-1.el7.elrepo.x86_64 #1 SMP Fri Dec 21 11:06:36 EST 2018 x86_64 x86_64 x86_64 GNU/Linux
</code></pre><h3 id="4-gpu-operator文件准备">4. gpu-operator文件准备</h3>
<p>Harbor中预上传的镜像文件列表如下(nvcr.io及nvidia):</p>
<p><img src="/images/2021_04_19_11_22_13_829x264.jpg" alt="/images/2021_04_19_11_22_13_829x264.jpg"></p>
<p>从<code>10.168.100.1</code>上scp以下目录到所有节点:</p>
<pre><code>$ scp -r docker@10.168.100.1:/home/docker/nvidia_items .
</code></pre><p>预Load nfd镜像:</p>
<pre><code># docker load&lt;quay.tar
...
Loaded image: quay.io/kubernetes_incubator/node-feature-discovery:v0.6.0
</code></pre><h3 id="5-安装nvidiagpu-operator">5. 安装NVIDIA/gpu-operator</h3>
<p>登录到<code>gpumaster</code>节点，从文件创建一个部署charts时需用到的configmap:</p>
<pre><code># cat ccse.repo
[ccse-k8s]
name=Centos local yum repo for k8s
baseurl=http://10.168.100.144:8200/repo/x86_64/k8s-offline-pkgs
gpgcheck=0
enabled=1
proxy=_none_

[ccse-centos7-base]
name=Centos local yum repo for k8s
baseurl=http://10.168.100.144:8200/repo/x86_64/centos7-base
gpgcheck=0
enabled=1
proxy=_none_

[fuck]
name=Centos local yum repo for k8s 111
baseurl=http://10.168.100.144:8200/repo/x86_64/nvidiarpms
gpgcheck=0
enabled=1
proxy=_none_
# kubectl create namespace gpu-operator-resources
namespace/gpu-operator-resources created
# kubectl create configmap repo-config -n gpu-operator-resources --from-file=ccse.repo
configmap/repo-config created
</code></pre><p>现在创建gpu-operator实例:</p>
<pre><code># cd gpu-operator/
#  helm install --generate-name . -f values.yaml
</code></pre><p>检查实例运行情况:</p>
<pre><code># kubectl get po
NAME                                                              READY   STATUS    RESTARTS   AGE
chart-1618803326-node-feature-discovery-master-655c6997cd-fp465   1/1     Running   0          65s
chart-1618803326-node-feature-discovery-worker-7flft              1/1     Running   0          65s
chart-1618803326-node-feature-discovery-worker-mkqm7              1/1     Running   0          65s
chart-1618803326-node-feature-discovery-worker-w2d44              1/1     Running   0          65s
gpu-operator-945878fff-l22vc                                      1/1     Running   0          65s
</code></pre><p>给GPU节点手动添加标签，<code>gpu-operator-resources</code>命名空间下的实例运行情况:</p>
<p>使能GPU驱动安装:</p>
<pre><code># kubectl label nodes 10.168.100.3 nvidia.com/gpu.deploy.driver=true       
node/10.168.100.3 labeled
# kubectl label nodes 10.168.100.4 nvidia.com/gpu.deploy.driver=true       
node/10.168.100.4 labeled
</code></pre><p>检查GPU驱动编译情况:</p>
<pre><code># kubectl  get po -n gpu-operator-resources
NAME                            READY   STATUS    RESTARTS   AGE
nvidia-driver-daemonset-w6d2q   1/1     Running   0          86s
nvidia-driver-daemonset-zmf9l   1/1     Running   0          86s
# kubectl logs po nvidia-driver-daemonset-zmf9l -n gpu-operator-resources
Installation of the kernel module for the NVIDIA Accelerated Graphics Driver for Linux-x86_64 (version 460.32.03) is now complete.

Loading IPMI kernel module...
Loading NVIDIA driver kernel modules...
Starting NVIDIA persistence daemon...
Mounting NVIDIA driver rootfs...
Done, now waiting for signal

</code></pre><p>使能<code>device-plugin</code>, <code>dcgm-exporter</code>等：</p>
<pre><code># kubectl label nodes 10.168.100.4 nvidia.com/gpu.deploy.container-toolkit=true
# kubectl label nodes 10.168.100.4 nvidia.com/gpu.deploy.device-plugin=true
# kubectl label nodes 10.168.100.4 nvidia.com/gpu.deploy.dcgm-exporter=true
# kubectl label nodes 10.168.100.4 nvidia.com/gpu.deploy.gpu-feature-discovery=true

# kubectl label nodes 10.168.100.3 nvidia.com/gpu.deploy.container-toolkit=true
# kubectl label nodes 10.168.100.3 nvidia.com/gpu.deploy.device-plugin=true
# kubectl label nodes 10.168.100.3  nvidia.com/gpu.deploy.dcgm-exporter=true
# kubectl label nodes 10.168.100.3 nvidia.com/gpu.deploy.gpu-feature-discovery=true
</code></pre><p>检查<code>toolkit-daemonset</code>运行情况，会发现<code>Init:ImagePullBackOff</code>报错信息:</p>
<pre><code># kubectl get po -n gpu-operator-resources
NAME                                       READY   STATUS                  RESTARTS   AGE
nvidia-container-toolkit-daemonset-6kqq5   0/1     Init:ImagePullBackOff   0          2m16s
nvidia-container-toolkit-daemonset-cbww2   0/1     Init:ImagePullBackOff   0          4m1s
# kubectl logs nvidia-container-toolkit-daemonset-cbww2 -n gpu-operator-resources
  Normal   BackOff         3m31s (x7 over 4m46s)  kubelet, 10.168.100.4  Back-off pulling image &quot;10.168.100.144:8021/nvcr.io/nvidia/k8s/cuda@sha256:ed723a1339cddd75eb9f2be2f3476edf497a1b189c10c9bf9eb8da4a16a51a59&quot;
  Warning  Failed          3m31s (x7 over 4m46s)  kubelet, 10.168.100.4  Error: ImagePullBackOff
  Normal   Pulling         3m20s (x4 over 4m48s)  kubelet, 10.168.100.4  Pulling image &quot;10.168.100.144:8021/nvcr.io/nvidia/k8s/cuda@sha256:ed723a1339cddd75eb9f2be2f3476edf497a1b189c10c9bf9eb8da4a16a51a59&quot;
</code></pre><p>这是因为pod拉取的镜像tag不对所导致，需要手动修改image的tag:</p>
<pre><code># kubectl get ds -n gpu-operator-resources
NAME                                 DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                  AGE
nvidia-container-toolkit-daemonset   2         2         0       2            0           nvidia.com/gpu.deploy.container-toolkit=true   133m
nvidia-driver-daemonset              2         2         2       2            2           nvidia.com/gpu.deploy.driver=true              135m
# kubectl edit ds nvidia-container-toolkit-daemonset -n gpu-operator-resources
        #image: 10.168.100.144:8021/nvcr.io/nvidia/k8s/cuda@sha256:ed723a1339cddd75eb9f2be2f3476edf497a1b189c10c9bf9eb8da4a16a51a59
        image: 10.168.100.144:8021/nvcr.io/nvidia/cuda:11.2.1-base-ubi8

</code></pre><p>刷新pod运行情况，可以看到<code>nvidia-container-toolkit-daemonset</code>及<code>nvidia-device-plugin-daemonset</code>运行正常，而<code>nvidia-device-plugin-validation</code>则<code>Init:CreashLoopBackOff</code>失败:</p>
<pre><code># kubectl get po -n gpu-operator-resources
NAME                                       READY   STATUS                  RESTARTS   AGE
nvidia-container-toolkit-daemonset-27qj8   1/1     Running                 0          52s
nvidia-container-toolkit-daemonset-g5ndb   1/1     Running                 0          51s
nvidia-device-plugin-daemonset-sqfdc       1/1     Running                 0          26s
nvidia-device-plugin-daemonset-wldkd       1/1     Running                 0          26s
nvidia-device-plugin-validation            0/1     Init:CrashLoopBackOff   1          9s
nvidia-driver-daemonset-m4xjv              1/1     Running                 0          137m
nvidia-driver-daemonset-vkrz5              1/1     Running                 5          137m
</code></pre><p>定位该validation所在的节点名(此例中为<code>10.168.100.3</code>):</p>
<pre><code># kubectl get po nvidia-device-plugin-validation -n  gpu-operator-resources -o wide
NAME                              READY   STATUS                  RESTARTS   AGE     IP              NODE           NOMINATED NODE   READINESS GATES
nvidia-device-plugin-validation   0/1     Init:CrashLoopBackOff   4          2m55s   172.26.222.10   10.168.100.3   &lt;none&gt;           &lt;none&gt;
</code></pre><p>获取启动失败原因:</p>
<pre><code># kubectl describe po nvidia-device-plugin-validation -n gpu-operator-resources
......
  Warning  Failed            56s (x5 over 2m21s)   kubelet, 10.168.100.3  Error: failed to start container &quot;device-plugin-validation-init&quot;: Error response from daemon: linux runtime spec devices: error gathering device information while adding custom device &quot;/dev/nvidiactl&quot;: no such file or directory
</code></pre><p>登录<code>10.168.100.3</code>节点，获取<code>/dev</code>下驱动程序设备名:</p>
<pre><code># docker ps | grep nvidia-device-plugin-daemonset | grep -v pause
abbea480fdf2        10.168.100.144:8021/nvcr.io/nvidia/k8s-device-plugin       &quot;nvidia-device-plugin&quot;   6 minutes ago       Up 6 minutes                            k8s_nvidia-device-plugin-ctr_nvidia-device-plugin-daemonset-sqfdc_gpu-operator-resources_b9988b02-82a6-4637-a7f0-fdee5a448d60_0
# docker exec -it k8s_nvidia-device-plugin-ctr_nvidia-device-plugin-daemonset-sqfdc_gpu-operator-resources_b9988b02-82a6-4637-a7f0-fdee5a448d60_0 /bin/bash
[root@nvidia-device-plugin-daemonset-sqfdc /]# ls /dev/nvidia* -l -h
crw-rw-rw- 1 root root 195, 254 Apr 19 03:52 /dev/nvidia-modeset
crw-rw-rw- 1 root root 237,   0 Apr 19 06:08 /dev/nvidia-uvm
crw-rw-rw- 1 root root 237,   1 Apr 19 06:08 /dev/nvidia-uvm-tools
crw-rw-rw- 1 root root 195,   0 Apr 19 03:52 /dev/nvidia0
crw-rw-rw- 1 root root 195, 255 Apr 19 03:52 /dev/nvidiactl
[root@nvidia-device-plugin-daemonset-sqfdc /]# exit
</code></pre><p>在主机级别(<code>10.168.100.3</code>)上手动创建<code>/dev/nvidiactl</code>文件, 依据同样步骤在<code>10.168.100.4</code>上查找到相应的设备驱动号也添加<code>/dev/nvidiactl</code>文件:</p>
<pre><code>[root@gpunode1 ~]# mknod -m 666 /dev/nvidiactl c 195 255
[root@gpunode1 ~]# ls /dev/nvidiactl -l
crw-rw-rw- 1 root root 195, 255 Apr 19 02:19 /dev/nvidiactl
</code></pre><p>delete掉<code>nvidia-device-plugin-validation</code>这个pod后，kubelet将重新拉起一个，此时报错信息有变化，提示缺少<code>/dev/nvidia-uvm</code>设备驱动文件:</p>
<pre><code>  Warning  Failed     10s (x2 over 11s)  kubelet, 10.168.100.4  Error: failed to start container &quot;device-plugin-validation-init&quot;: Error response from daemon: linux runtime spec devices: error gathering device information while adding custom device &quot;/dev/nvidia-uvm&quot;: no such file or directory
</code></pre><p>按照上面创建<code>/dev/nvidiactl</code>的方法创建<code>/dev/nvidia-uvm</code>驱动文件，注意设备号与容器中保持一致:</p>
<pre><code># mknod -m 666 /dev/nvidia-uvm c 237 0
</code></pre><p>删除pod后重新拉起，报错信息为缺少<code>/dev/nvidia-uvm-tools</code>:</p>
<pre><code>  Warning  Failed     9s (x2 over 10s)  kubelet, 10.168.100.4  Error: failed to start container &quot;device-plugin-validation-init&quot;: Error response from daemon: linux runtime spec devices: error gathering device information while adding custom device &quot;/dev/nvidia-uvm-tools&quot;: no such file or directory
</code></pre><p>手动创建<code>nvidia-uvm-tools</code>设备文件后删除pod等待kubelet重新拉起pod:</p>
<pre><code># mknod -m 666 /dev/nvidia-uvm-tools c 237 1
  Warning  Failed     12s (x2 over 12s)  kubelet, 10.168.100.3  Error: failed to start container &quot;device-plugin-validation-init&quot;: Error response from daemon: linux runtime spec devices: error gathering device information while adding custom device &quot;/dev/nvidia-modeset&quot;: no such file or directory
</code></pre><p>手动创建<code>nvidia-modeset</code>设备文件后删除pod等待kubelet重新拉起pod:</p>
<pre><code># mknod -m 666 /dev/nvidia-modeset c 195 254
  Warning  Failed     13s (x2 over 14s)  kubelet, 10.168.100.4  Error: failed to start container &quot;device-plugin-validation-init&quot;: Error response from daemon: linux runtime spec devices: error gathering device information while adding custom device &quot;/dev/nvidia0&quot;: no such file or directory
</code></pre><p>手动创建<code>nvidia0</code>设备文件后删除pod等待kubelet重新拉起pod:</p>
<pre><code># mknod -m 666 /dev/nvidia0 c 195 0
# kubectl  get po -A | grep device-plugin-validation
gpu-operator-resources   nvidia-device-plugin-validation                                   0/1     Completed   0          2m26s
</code></pre><p>此时kubelet将继续拉起剩余的nvidia资源，最终状态应该是:</p>
<pre><code># kubectl  get po -A
NAMESPACE                NAME                                                              READY   STATUS      RESTARTS   AGE
default                  chart-1618804240-node-feature-discovery-master-5f446799f4-sk7vg   1/1     Running     0          163m
default                  chart-1618804240-node-feature-discovery-worker-5sllh              1/1     Running     1          163m
default                  chart-1618804240-node-feature-discovery-worker-86w4w              1/1     Running     0          163m
default                  chart-1618804240-node-feature-discovery-worker-fl52v              1/1     Running     0          163m
default                  gpu-operator-945878fff-88thn                                      1/1     Running     0          163m
gpu-operator-resources   gpu-feature-discovery-p6zqs                                       1/1     Running     0          53s
gpu-operator-resources   gpu-feature-discovery-x88v4                                       1/1     Running     0          53s
gpu-operator-resources   nvidia-container-toolkit-daemonset-27qj8                          1/1     Running     0          26m
gpu-operator-resources   nvidia-container-toolkit-daemonset-g5ndb                          1/1     Running     0          26m
gpu-operator-resources   nvidia-dcgm-exporter-c9vht                                        1/1     Running     0          74s
gpu-operator-resources   nvidia-dcgm-exporter-mz7rh                                        1/1     Running     0          74s
gpu-operator-resources   nvidia-device-plugin-daemonset-sqfdc                              1/1     Running     0          25m
gpu-operator-resources   nvidia-device-plugin-daemonset-wldkd                              1/1     Running     0          25m
gpu-operator-resources   nvidia-device-plugin-validation                                   0/1     Completed   0          2m47s
gpu-operator-resources   nvidia-driver-daemonset-m4xjv                                     1/1     Running     0          163m
gpu-operator-resources   nvidia-driver-daemonset-vkrz5                                     1/1     Running     5          163m
....
</code></pre><h3 id="6-测试gpu">6. 测试GPU</h3>
<p><code>gpu-operator</code>目录下预置了一个<code>test.yaml</code>文件，直接创建:</p>
<pre><code>[root@gpumaster gpu-operator]# kubectl create -f test.yaml
pod/dcgmproftester created
[root@gpumaster gpu-operator]# kubectl  get po -o wide | grep dcgmproftester
dcgmproftester                                                    1/1     Running   0          103s   172.26.243.149   10.168.100.4   &lt;none&gt;           &lt;none&gt;

</code></pre><p>找寻到<code>10.168.100.4</code>上的<code>nvidia-device-plugin-daemonset</code>的pod， 观察该节点上gpu的功耗及显存占用情况，可以看到该工作负载确实使用了gpu中的运算单元:</p>
<pre><code># kubectl exec nvidia-device-plugin-daemonset-wldkd -n gpu-operator-resources nvidia-smi
nvidia 33988608 269 nvidia_modeset,nvidia_uvm, Live 0xffffffffa05dd000 (PO)
Mon Apr 19 06:39:26 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  On   | 00000000:00:08.0 Off |                  Off |
| N/A   61C    P0   208W / 250W |    493MiB / 32510MiB |     84%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
</code></pre><p>测试完毕后该pod将处于<code>completed</code>状态，观察其输出:</p>
<pre><code># kubectl  get po -o wide | grep dcgm
dcgmproftester                                                    0/1     Completed   0          4m12s   172.26.243.149   10.168.100.4   &lt;none&gt;           &lt;none&gt;
# kubectl  logs dcgmproftester
.....
TensorEngineActive: generated ???, dcgm 0.000 (74380.8 gflops)
TensorEngineActive: generated ???, dcgm 0.000 (75398.9 gflops)
TensorEngineActive: generated ???, dcgm 0.000 (75787.6 gflops)
TensorEngineActive: generated ???, dcgm 0.000 (77173.9 gflops)
TensorEngineActive: generated ???, dcgm 0.000 (75669.5 gflops)
Skipping UnwatchFields() since DCGM validation is disabled
</code></pre>
  </div>
  
</div>






<script src="http://purplepalmdash.github.io/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
</body>
</html>

