<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta name=generator content="Hugo 0.64.0"><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Dash &#183; Dash</title><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124"><link rel=stylesheet href=https://purplepalmdash.github.io/css/highlight/googlecode.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js></script><script type=text/javascript src=/js/html2canvas.js></script><script type=text/javascript>function genPostShot(){var rightNow=new Date();var imageName=rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");imageName+='.jpg'
html2canvas(document.getElementsByClassName('post'),{background:'#FFFFFF',onrendered:function(canvas){$('#test').attr('href',canvas.toDataURL("image/jpeg"));$('#test').attr('download',imageName);$('#test')[0].click();}});};</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124"><link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel=icon><link href=%7balternate%20%7bRSS%20application/rss+xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://purplepalmdash.github.io/index.xml%7d rel=alternate type=application/rss+xml title="Dash &#183; Dash"><meta name=description content><meta name=keywords content="unix,virtualization,embedded,linux"></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><img src=http://purplepalmdash.github.io/images/mylogo.jpeg alt=gravatar><h1><a href=http://purplepalmdash.github.io/>很惭愧，就做了一点微小的工作</a></h1><a href=http://purplepalmdash.github.io/><p>Dash</p></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/post/>All Posts</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/technology/>Technology</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/life/>Life</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/linuxtips/>LinuxTips</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/purplepalmdash><i class="fa fa-github-square fa-3x"></i></a><a href=https://cn.linkedin.com/in/yang-feipeng-1b909319><i class="fa fa-linkedin-square fa-3x"></i></a><a href=https://plus.google.com/u/0/106572959364703833986><i class="fa fa-google-plus-square fa-3x"></i></a><a href=https://www.facebook.com/yang.feipeng><i class="fa fa-facebook-square fa-3x"></i></a><a href=https://twitter.com/dashwillfly><i class="fa fa-twitter-square fa-3x"></i></a></li></ul></div></div><div class="content container"><div class=posts><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/01/29/multipleofflineregistryserver/>MultipleOfflineRegistryServer</a></h1><span class=post-date>Jan 29, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>Suppose your kubernetes clusters are in totally offline environment, you don&rsquo;t
have any connection to Internet, how to deal with the <code>docker pull</code> request
within the cluster? Following are steps for solving this problem.</p><h3 id=docker-composed-registry>Docker-Composed Registry</h3><p>This step is pretty simple, I mainly refers to:</p><p><a href=https://www.digitalocean.com/community/tutorials/how-to-set-up-a-private-docker-registry-on-ubuntu-14-04>https://www.digitalocean.com/community/tutorials/how-to-set-up-a-private-docker-registry-on-ubuntu-14-04</a></p><p>Following the steps in article, I got a running docker-compose triggered
docker registry server.</p><p>Following is my folder structure:</p><pre><code># tree .
├── data
├── docker-compose.yml
├── nginx
│   ├── openssl.cnf
│   ├── registry.conf
│   ├── registry.password
│   ├── server.crt
│   ├── server.csr
│   └── server.key
</code></pre><p>Docker-compose file is listed as:</p><pre><code># cat docker-compose.yml 
nginx:
  image: &quot;nginx:1.9&quot;
  ports:
    - 443:443
  links:
    - registry:registry
  volumes:
    - ./nginx/:/etc/nginx/conf.d:ro

registry:
  image: registry:2
  ports:
    - 127.0.0.1:5000:5000
  environment:
    REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY: /data
  volumes:
    - ./data:/data
</code></pre><h3 id=handle-ssl>Handle SSL</h3><p>The above steps will only create a single hostname ssl enabled docker registry
server, we want to enable multi-domain self-signed SSL certificate.</p><p>Create a file called openssl.cnf with the following details, notice that we
have set <code>docker.io</code>, <code>gcr.io</code>, <code>quay.io</code> and <code>elastic.co</code>.</p><pre><code># vim openssl.cnf
[req]
distinguished_name = req_distinguished_name
req_extensions = v3_req

[req_distinguished_name]
countryName = SL
countryName_default = SL
stateOrProvinceName = Western
stateOrProvinceName_default = Western
localityName = Colombo
localityName_default = Colombo
organizationalUnitName = ABC
organizationalUnitName_default = ABC
commonName = *.docker.io
commonName_max = 64

[ v3_req ]
# Extensions to add to a certificate request
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = *.docker.io
DNS.2 = *.gcr.io
DNS.3 = *.quay.io
DNS.4 = *.elastic.co
DNS.5 = docker.io
DNS.6 = gcr.io
DNS.7 = quay.io
DNS.8 = elastic.co
</code></pre><p>Create the Private key.</p><pre><code># rm -f server.key 
# sudo openssl genrsa -out server.key 2048
Generating RSA private key, 2048 bit long modulus (2 primes)
.....................+++++
......................................+++++
e is 65537 (0x010001)
</code></pre><p>Create Certificate Signing Request (CSR).</p><pre><code># rm -f server.csr 
# sudo openssl req -new -out server.csr -key server.key -config openssl.cnf
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
SL [SL]:
Western [Western]:
Colombo [Colombo]:
ABC [ABC]:
*.docker.io []:
</code></pre><p><code>*.docker.io</code> is from the above defined parts in <code>openssl.cnf</code>.</p><p>Sign the SSL Certificate.</p><pre><code># rm -f server.crt 
# sudo openssl x509 -req -days 36500 -in server.csr -signkey server.key -out server.crt -extensions v3_req -extfile openssl.cnf
Signature ok
subject=C = SL, ST = Western, L = Colombo, OU = ABC
Getting Private key
</code></pre><p>You can check the ssl certificated content via:</p><pre><code># openssl x509 -in server.crt -noout -text 
Certificate:
    Data:
....

        Validity
            Not Before: Jan 29 08:30:20 2019 GMT
            Not After : Jan  5 08:30:20 2119 GMT
....

            X509v3 Subject Alternative Name: 
                DNS:*.docker.io, DNS:*.gcr.io, DNS:*.quay.io, DNS:*.elastic.co, DNS:docker.io, DNS:gcr.io, DNS:quay.io, DNS:elastic.co
</code></pre><h3 id=nginx-conf>nginx conf</h3><p>Add the domain items into registry.conf:</p><pre><code>upstream docker-registry {
  server registry:5000;
}

server {
  listen 443;
  server_name docker.io gcr.io quay.io elastic.co;

  # SSL
  ssl on;
  ssl_certificate /etc/nginx/conf.d/server.crt;
  ssl_certificate_key /etc/nginx/conf.d/server.key;
</code></pre><h3 id=dns-configuration>dns configuration</h3><p>Install bind9 on ubuntu, or install dnsmasq on centos system. take bind9&rsquo;s
configuration for example, add following configurations:</p><pre><code># ls db*
db.docker.io  db.gcr.io  db.quay.io
</code></pre><p>The <code>named.conf.default-zones</code> is listed as following:</p><pre><code># cat named.conf.default-zones 
// prime the server with knowledge of the root servers
zone &quot;.&quot; {
	type hint;
	file &quot;/etc/bind/db.root&quot;;
};

// be authoritative for the localhost forward and reverse zones, and for
// broadcast zones as per RFC 1912

zone &quot;localhost&quot; {
	type master;
	file &quot;/etc/bind/db.local&quot;;
};

zone &quot;127.in-addr.arpa&quot; {
	type master;
	file &quot;/etc/bind/db.127&quot;;
};

zone &quot;0.in-addr.arpa&quot; {
	type master;
	file &quot;/etc/bind/db.0&quot;;
};

zone &quot;255.in-addr.arpa&quot; {
	type master;
	file &quot;/etc/bind/db.255&quot;;
};

zone &quot;docker.io&quot; {
        type master;
        file &quot;/etc/bind/db.docker.io&quot;; 
};

zone &quot;quay.io&quot; {
        type master;
        file &quot;/etc/bind/db.quay.io&quot;; 
};

zone &quot;gcr.io&quot; {
        type master;
        file &quot;/etc/bind/db.gcr.io&quot;; 
};

zone &quot;elastic.co&quot; {
        type master;
        file &quot;/etc/bind/db.elastic.co&quot;; 
};
</code></pre><p>Take <code>elastic.co</code> for example, show its content:</p><pre><code># cat db.elastic.co
$TTL    604800
@       IN      SOA     elastic.co. root.localhost. (
                              1         ; Serial
                         604800         ; Refresh
                          86400         ; Retry
                        2419200         ; Expire
                         604800 )       ; Negative Cache TTL
;
@       IN      NS      localhost.
@       IN      A      192.168.122.154
elastic.co IN      NS      192.168.122.154

docker     IN      A       192.168.122.154
</code></pre><h3 id=using-the-docker-registry>Using the docker registry</h3><p>Point the nodes&rsquo; dns server to your registry nodes, so if you ping elastic.co,
you got reply from <code>192.168.122.154</code>.</p><p>Then add the crt into your docker&rsquo;s trusted sites:</p><pre><code># cp server.crt /usr/local/share/ca-certificates
# update-ca-certificates
# systemctl restart docker
# docker login -u dddd -p gggg docker.io
# docker login -u dddd -p gggg quay.io
# docker login -u dddd -p gggg gcr.io
# docker login -u dddd -p gggg k8s.gcr.io
</code></pre><p>Now you can freely push/pull images from offline registry server, as if you
are on Internet.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/01/28/tipsonhelmchartsprometheus/>TipsOnHelmChartsPrometheus</a></h1><span class=post-date>Jan 28, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=upgrade-helm>Upgrade helm</h3><p>Download the helm from github, and extracted to your system PATH, then:</p><pre><code># helm init --upgrade
</code></pre><p>Notice in your k8s cluster your tiller will be upgraded:</p><pre><code>kube-system   tiller-deploy-68b77f4c57-dwd47                                  0/1       ContainerCreating   0          13s
</code></pre><p>Examine the upgraded version:</p><pre><code># helm version
Client: &amp;version.Version{SemVer:&quot;v2.12.2&quot;, GitCommit:&quot;7d2b0c73d734f6586ed222a567c5d103fed435be&quot;, GitTreeState:&quot;clean&quot;}
Server: &amp;version.Version{SemVer:&quot;v2.12.2&quot;, GitCommit:&quot;7d2b0c73d734f6586ed222a567c5d103fed435be&quot;, GitTreeState:&quot;clean&quot;}
</code></pre><h3 id=working-issue>Working Issue</h3><p>Using stable/prometheus-operator.</p><p>Fetch the helm/charts package via:</p><pre><code># helm fetch stable/prometheus-operator
# ls
prometheus-operator-1.9.0.tgz
</code></pre><p>Deploy it on the minikube and you will get all of the images, then save it
via:</p><pre><code>eval $(minikube docker-env)
docker save -o prometheus-operator.tar grafana/grafana:5.4.3 quay.io/prometheus/node-exporter:v0.17.0 quay.io/coreos/prometheus-config-reloader:v0.26.0 quay.io/coreos/prometheus-operator:v0.26.0 quay.io/prometheus/alertmanager:v0.15.3 quay.io/prometheus/prometheus:v2.5.0 kiwigrid/k8s-sidecar:0.0.6 quay.io/coreos/kube-state-metrics:v1.4.0 quay.io/coreos/configmap-reload:v0.0.1
xz prometheus-operator.tar
</code></pre><p>Then we could write the offline scripts for deploying this helm/charts.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/01/22/workingtipsonubuntuglusterfs/>WorkingTipsOnUbuntuGlusterFS</a></h1><span class=post-date>Jan 22, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=install>Install</h3><p>In 4 nodes, install the latest glusterfs via:</p><pre><code>sudo add-apt-repository -y ppa:gluster/glusterfs-5
sudo apt-get -y update
sudo apt-get -y install glusterfs-server
sudo apt-get -y install thin-provisioning-tools
</code></pre><h3 id=configure>Configure</h3><p>See following steps for configurating the gluster in 4
nodes(10.48.129.101~104):</p><pre><code>root@gluster-1:/home/vagrant# gluster peer status
Number of Peers: 0
root@gluster-1:/home/vagrant# gluster pool list
UUID					Hostname 	State
628ce8f2-622c-4be3-92b0-d8e5241d01b8	localhost	Connected 
root@gluster-1:/home/vagrant# ifconfig eth1
eth1      Link encap:Ethernet  HWaddr 52:54:00:c8:74:01  
          inet addr:10.48.129.101  Bcast:10.48.129.255  Mask:255.255.255.0
          inet6 addr: fe80::5054:ff:fec8:7401/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:2456 errors:0 dropped:17 overruns:0 frame:0
          TX packets:103 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:126151 (126.1 KB)  TX bytes:10351 (10.3 KB)

root@gluster-1:/home/vagrant# gluster peer probe 10.48.129.102
peer probe: success. 
root@gluster-1:/home/vagrant# gluster peer probe 10.48.129.103
peer probe: success. 
root@gluster-1:/home/vagrant# gluster peer probe 10.48.129.104
peer probe: success. 
root@gluster-1:/home/vagrant# gluster peer status
Number of Peers: 3

Hostname: 10.48.129.102
Uuid: 52472b8f-b835-4038-b061-89229d825c42
State: Peer in Cluster (Connected)

Hostname: 10.48.129.103
Uuid: 46161901-954a-4ffe-9aec-a8c14ceded03
State: Peer in Cluster (Connected)

Hostname: 10.48.129.104
Uuid: a597a3ff-8c32-4a5a-b83d-1d25187afe31
State: Peer in Cluster (Connected)
</code></pre><p>After peer probe, the cluster is listed as following:</p><pre><code>root@gluster-1:/home/vagrant# gluster peer status
Number of Peers: 3

Hostname: 10.48.129.102
Uuid: 52472b8f-b835-4038-b061-89229d825c42
State: Peer in Cluster (Connected)

Hostname: 10.48.129.103
Uuid: 46161901-954a-4ffe-9aec-a8c14ceded03
State: Peer in Cluster (Connected)

Hostname: 10.48.129.104
Uuid: a597a3ff-8c32-4a5a-b83d-1d25187afe31
State: Peer in Cluster (Connected)
root@gluster-1:/home/vagrant# gluster pool list
UUID					Hostname     	State
52472b8f-b835-4038-b061-89229d825c42	10.48.129.102	Connected 
46161901-954a-4ffe-9aec-a8c14ceded03	10.48.129.103	Connected 
a597a3ff-8c32-4a5a-b83d-1d25187afe31	10.48.129.104	Connected 
628ce8f2-622c-4be3-92b0-d8e5241d01b8	localhost    	Connected 
</code></pre><h3 id=heketi>heketi</h3><p>Download the heketi from:</p><pre><code># wget https://github.com/heketi/heketi/releases/download/v8.0.0/heketi-v8.0.0.linux.amd64.tar.gz
# tar xzvf heketi-v8.0.0.linux.amd64.tar.gz
# cd heketi
# cp heketi heketi-cli /usr/bin
</code></pre><p>Configuration:</p><pre><code># cd heketi
# cp heketi.json heketi.json.back
# vim heketi.json
# mkdir -p /etc/heketi
# cp heketi.json /etc/heketi
</code></pre><p>Your content is listed as following:</p><pre><code>......
#修改端口，防止端口冲突
  &quot;port&quot;: &quot;18080&quot;,
......
#允许认证
  &quot;use_auth&quot;: true,
......
#admin用户的key改为adminkey
      &quot;key&quot;: &quot;adminkey&quot;
......
#修改执行插件为ssh，并配置ssh的所需证书，注意要能对集群中的机器免密ssh登陆，使用ssh-copy-id把pub key拷到每台glusterfs服务器上
    &quot;executor&quot;: &quot;ssh&quot;,
    &quot;sshexec&quot;: {
      &quot;keyfile&quot;: &quot;/root/.ssh/id_rsa&quot;,
      &quot;user&quot;: &quot;root&quot;,
      &quot;port&quot;: &quot;22&quot;,
      &quot;fstab&quot;: &quot;/etc/fstab&quot;
    },
......
# 定义heketi数据库文件位置
    &quot;db&quot;: &quot;/var/lib/heketi/heketi.db&quot;
......
#调整日志输出级别
    &quot;loglevel&quot; : &quot;warning&quot;
</code></pre><p>Now start the heketi server via:</p><pre><code># heketi --config=/etc/heketi/heketi.json
</code></pre><p>Your server now is listening at <code>localhost:18080</code>, now use the cli for
creating the cluster and volume.</p><pre><code># heketi-cli --server http://localhost:18080 --user admin --secret &quot;adminkey&quot; cluster list
# heketi-cli --server http://localhost:18080 --user admin --secret &quot;adminkey&quot; cluster create
# alias heketi-cli-admin='heketi-cli --server http://localhost:18080 --user admin --secret &quot;adminkey&quot;'
# heketi-cli-admin node add --cluster &quot;360e5d504ff7cc18823d580bac55711a&quot; --management-host-name 10.48.129.101 --storage-host-name 10.48.129.101 --zone 1
# heketi-cli-admin node add --cluster &quot;360e5d504ff7cc18823d580bac55711a&quot; --management-host-name 10.48.129.102 --storage-host-name 10.48.129.102 --zone 1
# heketi-cli-admin node add --cluster &quot;360e5d504ff7cc18823d580bac55711a&quot; --management-host-name 10.48.129.103 --storage-host-name 10.48.129.103 --zone 1
# heketi-cli-admin node add --cluster &quot;360e5d504ff7cc18823d580bac55711a&quot; --management-host-name 10.48.129.104 --storage-host-name 10.48.129.104 --zone 1
# heketi-cli-admin --json device add --name=&quot;/dev/vdb&quot; --node &quot;f787daa4ff84461b11b7f7d00645dfdc&quot;
# heketi-cli-admin --json device add --name=&quot;/dev/vdb&quot; --node &quot;16f738ee9822fb79c7b321c0b7ed1792&quot;
# heketi-cli-admin --json device add --name=&quot;/dev/vdb&quot; --node &quot;c6b1f0ccfe1ff76a245012108c44043f&quot;
# heketi-cli-admin --json device add --name=&quot;/dev/vdb&quot; --node &quot;2e631bedcb6b58c4b82bba31082531f1&quot;
# heketi-cli-admin volume list
# heketi-cli-admin volume create --size=10
</code></pre><p>After it finishes, your volume will be created.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/01/21/migratingto1804/>MigratingTo1804</a></h1><span class=post-date>Jan 21, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=aim>AIM</h3><p>For upgrading the offline installation of kubespray from 1604 to 1804.</p><h3 id=ansible>Ansible</h3><p>Get the ansible debs via following command:</p><pre><code># apt-get update
# apt-get install  install software-properties-common
# apt-add-repository ppa:ansible/ansible
# apt-get install ansible
# mkdir /root/debs &amp;&amp; cd /var/cache
# find . | grep deb$ | xargs -I % cp % /root/debs
</code></pre><p>You should change the vagrant box&rsquo;s configuration:</p><pre><code># vim /etc/netplan/01-netcfg.yaml
    eth0:
      dhcp4: yes
+      dhcp-identifier: mac
</code></pre><p>The vagrant file(shell provision) should be like following:</p><pre><code>rm -f /etc/resolv.conf
ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf
echo '      nameservers:'&gt;&gt;/etc/netplan/50-vagrant.yaml
echo '        addresses: [10.148.129.101]'&gt;&gt;/etc/netplan/50-vagrant.yaml
netplan apply eth1
</code></pre><h3 id=task-changes>task Changes</h3><p>Judge for OS version changes:</p><pre><code># vim roles/kube-deploy/tasks/main.yml
- name: Set ubuntu_version
  set_fact:
    ubuntu_version: &gt;-
      {%- if 'bionic' in os_release.stdout -%}
      bionic
      {%- elif 'xenial' in os_release.stdout -%}
      xenial
      {%- endif -%}

# vim roles/kube-deploy/deploy-ubuntu.yml
  - name: &quot;upload debs.tar.xz files to kube-deploy(Xenial)&quot;
    copy:
      src: files/1604debs.tar.xz
      dest: /usr/local/
      owner: root
      group: root
      mode: 0777
    when: ubuntu_version == &quot;xenial&quot;

  - name: &quot;Install ansible and python-netaddr(Xenial)&quot;
    raw: cd /usr/local/ &amp;&amp; tar xJvf 1604debs.tar.xz -C /usr/local/ &amp;&amp; echo &quot;deb [trusted=yes] file:///usr/local/static ./&quot;&gt;/etc/apt/sources.list &amp;&amp; apt-get u
pdate -y &amp;&amp; apt-get install -y ansible python-netaddr &amp;&amp; rm -f /usr/local/debs.tar.xz
    when: ubuntu_version == &quot;xenial&quot;

  - name: &quot;upload debs.tar.xz files to kube-deploy(Bionic)&quot;
    copy:
      src: files/1804debs.tar.xz
      dest: /usr/local/
      owner: root
      group: root
      mode: 0777
    when: ubuntu_version == &quot;bionic&quot;

  - name: &quot;Install ansible and python-netaddr(Bionic)&quot;
    raw: cd /usr/local/ &amp;&amp; tar xJvf 1804debs.tar.xz -C /usr/local/ &amp;&amp; echo &quot;deb [trusted=yes] file:///usr/local/static ./&quot;&gt;/etc/apt/sources.list &amp;&amp; apt-get u
pdate -y &amp;&amp; apt-get install -y ansible python-netaddr &amp;&amp; rm -f /usr/local/debs.tar.xz
    when: ubuntu_version == &quot;bionic&quot;
</code></pre><p>Also you have to change the Vagrantfiles:</p><pre><code>File.open('./dns.sh' ,'w') do |f|
  f.write &quot;#!/bin/bash\n&quot;
  f.write &quot;sed -i '/^#VAGRANT-END/i dns-nameservers 10.148.129.101' /etc/network/interfaces\n&quot;
  f.write &quot;systemctl restart networking.service\n&quot;
  #f.write &quot;rm -f /etc/resolv.conf\n&quot;
  #f.write &quot;ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf\n&quot;
  #f.write &quot;echo '      nameservers:'&gt;&gt;/etc/netplan/50-vagrant.yaml\n&quot;
  #f.write &quot;echo '        addresses: [10.148.129.101]'&gt;&gt;/etc/netplan/50-vagrant.yaml\n&quot;
  #f.write &quot;netplan apply eth1\n&quot;
end
</code></pre><p>Uncomment the sed/systemctl for xenial, uncomment the later 5 lines for
bionic.</p><p>You should have xenial/bionic boxs under ~/.vagrant.d/boxes.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/01/20/ubuntu1804vagrantboxcreate/>Ubuntu1804VagrantBoxCreate</a></h1><span class=post-date>Jan 20, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>For creating Ubuntu 18.04 vagrant box, follow the following steps:</p><p>Change grub configuration for changing to <code>ethx</code> naming rules:</p><pre><code># vim /etc/default/grub
GRUB_CMDLINE_LINUX_DEFAULT=&quot;quite&quot;
GRUB_CMDLINE_LINUX=&quot;net.ifnames=0 biosdevname=0&quot;
# grub-mkconfig -o /boot/grub/grub.cfg
</code></pre><p>Change the netplan rules:</p><pre><code># vim /etc/netplan/01-netcfg.yaml 
# This file describes the network interfaces available on your system
# For more information, see netplan(5).
network:
  version: 2
  ethernets:
    eth0:
      dhcp4: yes
      dhcp-identifier: mac
</code></pre><p>Now reboot your machine, continue for later commands.</p><p>Create vagrant user and set the password, etc.</p><pre><code># useradd -m vagrant
# passwd vagrant
# visudo
vagrant ALL=(ALL)	NOPASSWD:ALL
Defaults:vagrant	!requiretty
# mkdir -p /home/vagrant/.ssh
# chmod 0700 /home/vagrant/.ssh/
# wget --no-check-certificate     https://raw.github.com/mitchellh/vagrant/master/keys/vagrant.pub     -O /home/vagrant/.ssh/authorized_keys
# cat /home/vagrant/.ssh/authorized_keys 
# chmod 0600 /home/vagrant/.ssh/authorized_keys
# chown -R vagrant /home/vagrant/.ssh
# cp /home/test/.bashrc /home/vagrant/.bashrc 
# cp /home/test/.bash_logout /home/vagrant/.bash_logout
# cp /home/test/.profile /home/vagrant/.profile
# vim /home/vagrant/.profile 
add
[ -z &quot;$BASH_VERSION&quot; ] &amp;&amp; exec /bin/bash -l
# chsh -s /bin/bash vagrant
</code></pre><p>Finally change the sshd configuration:</p><pre><code># vim /etc/ssh/sshd_config 
AuthorizedKeysFile .ssh/authorized_keys
</code></pre><p>Now you could halt you machine.</p><p>Package to a box via:</p><pre><code>$ vagrant package --base xxxx
</code></pre><p>Using the package.box , then you could mutate to libvirt or do some other
things.</p><h3 id=for-kubespray-offline>For kubespray offline</h3><p>Install ansible via:</p><pre><code># apt-get update &amp;&amp; apt-get install -y python-pip &amp;&amp; pip install ansible
</code></pre><p>Or use old ansible(from repository):</p><pre><code># apt-get update -y &amp;&amp; apt-get install -y ansible
</code></pre><p>Better you use the ppa repository for installing ansible:</p><pre><code># apt-add-repository ppa:ansible/ansible
# apt-get install ansible
</code></pre><p>Generate the ssh key and use ssh-copy-id for copying the key for passwordless
login.</p><p>Download kubespray source files:</p><pre><code># wget  https://github.com/kubernetes-sigs/kubespray/archive/v2.8.1.tar.gz
# tar xzvf v2.8.1.tar.gz
# vim inventory/sample/host.ini
[all]
node ansible_host=192.168.33.109 ip=192.168.33.109 etcd_member_name=etcd1

[kube-master]
node

[etcd]
node

[kube-node]
node

[k8s-cluster:children]
kube-master
kube-node
# vim inventory/sample/group_vars/k8s-cluster/k8s-cluster.yml
kube_version: v1.12.4
</code></pre><p>Deploy online for:</p><pre><code># ansible-playbook -i inventory/sample/hosts.ini cluster.yml
</code></pre><p>Now you could get all of the deb packages and docker images.</p><h3 id=1804-debs-preparation>1804 debs preparation</h3><p>Generate 1804debs.tar.xz files.</p><pre><code>### Install more necessary packages. 
# apt-get install -y bind9 bind9utils ntp nfs-common nfs-kernel-server python-netaddr
# mkdir /root/static
# cd /var/cache
# find . | grep deb$ | xargs -I % cp % /root/static
# cd /root/static
# dpkg-scanpackages . /dev/null | gzip -9c &gt; Packages.gz
# cd /root/
# tar cJvf 1804debs.tar.xz static/
</code></pre></div><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/71/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/71/>71</a></li><li class="page-item active"><a class=page-link href=/page/72/>72</a></li><li class=page-item><a class=page-link href=/page/73/>73</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/246/>246</a></li><li class=page-item><a href=/page/73/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/246/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div><script src=http://purplepalmdash.github.io/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>