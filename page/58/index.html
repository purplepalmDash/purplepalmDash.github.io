<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta name=generator content="Hugo 0.64.0"><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Dash &#183; Dash</title><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124"><link rel=stylesheet href=https://purplepalmdash.github.io/css/highlight/googlecode.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js></script><script type=text/javascript src=/js/html2canvas.js></script><script type=text/javascript>function genPostShot(){var rightNow=new Date();var imageName=rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");imageName+='.jpg'
html2canvas(document.getElementsByClassName('post'),{background:'#FFFFFF',onrendered:function(canvas){$('#test').attr('href',canvas.toDataURL("image/jpeg"));$('#test').attr('download',imageName);$('#test')[0].click();}});};</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124"><link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel=icon><link href=%7balternate%20%7bRSS%20application/rss+xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://purplepalmdash.github.io/index.xml%7d rel=alternate type=application/rss+xml title="Dash &#183; Dash"><meta name=description content><meta name=keywords content="unix,virtualization,embedded,linux"></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><img src=http://purplepalmdash.github.io/images/mylogo.jpeg alt=gravatar><h1><a href=http://purplepalmdash.github.io/>很惭愧，就做了一点微小的工作</a></h1><a href=http://purplepalmdash.github.io/><p>Dash</p></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/post/>All Posts</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/technology/>Technology</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/life/>Life</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/linuxtips/>LinuxTips</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/purplepalmdash><i class="fa fa-github-square fa-3x"></i></a><a href=https://cn.linkedin.com/in/yang-feipeng-1b909319><i class="fa fa-linkedin-square fa-3x"></i></a><a href=https://plus.google.com/u/0/106572959364703833986><i class="fa fa-google-plus-square fa-3x"></i></a><a href=https://www.facebook.com/yang.feipeng><i class="fa fa-facebook-square fa-3x"></i></a><a href=https://twitter.com/dashwillfly><i class="fa fa-twitter-square fa-3x"></i></a></li></ul></div></div><div class="content container"><div class=posts><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/02/19/troubleshootingonvmwarebasedvm/>TroubleShootingOnVMWareBasedVM</a></h1><span class=post-date>Feb 19, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>记录一下昨天遇到的一个问题：<br>同事的虚拟机(Ubuntu)自检失败，fsck总是不过，直接抛到initramfs的界面。</p><p>试图探索:<br>启动的时候按Shift键，到grub界面，以recovery模式启动，依然失败。看来是ext4的磁盘inode有错误。</p><p>解决方案：<br>用livecd启动系统，启动以后到系统里去fsck修复该文件系统。<br>gparted启动失败。<br>后面我们用的是一个CentOS7的livecd系统进入的系统,</p><pre><code># fsck -y /dev/mapper/lv_lv_root
</code></pre><p>扫描并自动修复(yes)磁盘上的inode错误。修复以后，用硬盘启动系统，可正常进入系统。避免了重新安装操作系统的开销。</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/01/31/kubesprayto282/>kubesprayto282</a></h1><span class=post-date>Jan 31, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=aim>AIM</h3><p>Since kubespray has released v2.8.2, I write this article for recording steps
of upgrding from v2.8.1 to v2.8.2 of my offline solution.</p><h3 id=prerequisites>Prerequisites</h3><p>Download the v2.8.2 release from:</p><p><a href=https://github.com/kubernetes-sigs/kubespray/releases>https://github.com/kubernetes-sigs/kubespray/releases</a></p><pre><code># wget https://github.com/kubernetes-sigs/kubespray/archive/v2.8.2.tar.gz
# tar xzvf v2.8.2.tar.gz
# cd kubespray-2.8.2
</code></pre><h3 id=modification>Modification</h3><h4 id=1-clusteryml>1. cluster.yml</h4><p>three changes made.</p><p>a. added <code>kube-deploy</code> role.<br>b. disable instal container-engine(cause we will directly install docker).<br>c. create role for k8s dashboard.</p><pre><code>    - { role: bastion-ssh-config, tags: [&quot;localhost&quot;, &quot;bastion&quot;]}

# Notice 1, we combine deploy.yml here. 
- hosts: kube-deploy
  gather_facts: false
  roles:
    - kube-deploy
# Notice 1 ends here. 

- hosts: k8s-cluster:etcd:calico-rr:!kube-deploy
  any_errors_fatal: &quot;{{ any_errors_fatal | default(true) }}&quot;

.......

    - { role: kubernetes/preinstall, tags: preinstall }
    #- { role: &quot;container-engine&quot;, tags: &quot;container-engine&quot;, when: deploy_container_engine|default(true) }
    - { role: download, tags: download, when: &quot;not skip_downloads&quot; }

.......


    - { role: kubernetes-apps, tags: apps }
- hosts: kube-master[0]
  tasks:
    - name: &quot;Create clusterrolebinding&quot;
      shell: kubectl create clusterrolebinding cluster-admin-fordashboard --clusterrole=cluster-admin --user=system:serviceaccount:kube-system:kubernetes-dashboard
</code></pre><h4 id=2-vagrantfile>2. Vagrantfile</h4><p>10 Changes made.</p><p>a. Added customized vagrant box(vagrant-libvirt).<br>b. Added kube-deploy instance, which will automatically added into vagrantfile.<br>c. Added disks, we will use glusterfs.<br>d. We specify the system, and enable calico instead of flannel.<br>e. Added dns.sh for customized the offline environment dns.<br>f. Added vagrant-libvirt items.<br>g. Disable sync folder.<br>h. Call dns.sh when every vm startup.<br>i. Disable some download items, and specify the ansible python interpreter.<br>j. Let ansible become root, and disable ask-pass prompt, also add kube-deploy into vagrant generated inventory file.</p><pre><code>  &quot;opensuse-tumbleweed&quot; =&gt; {box: &quot;opensuse/openSUSE-Tumbleweed-x86_64&quot;, user: &quot;vagrant&quot;},
  &quot;wukong&quot;              =&gt; {box: &quot;kubespray&quot;, user: &quot;vagrant&quot;},
  &quot;wukong1804&quot;          =&gt; {box: &quot;rong1804&quot;, user: &quot;vagrant&quot;},
}
......
$kube_master_instances = $num_instances == 1 ? $num_instances : ($num_instances - 1)
$kube_deploy_instances = 1
# All nodes are kube nodes
......
$kube_node_instances_with_disks = true
$kube_node_instances_with_disks_size = &quot;40G&quot;

......
$os = &quot;wukong&quot;
$network_plugin = &quot;calico&quot;
......
    FileUtils.ln_s($inventory, File.join($vagrant_ansible,&quot;inventory&quot;))
  end
end
File.open('./dns.sh' ,'w') do |f|
  f.write &quot;#!/bin/bash\n&quot;
  f.write &quot;sed -i '/^#VAGRANT-END/i dns-nameservers 10.148.129.101' /etc/network/interfaces\n&quot;
  f.write &quot;systemctl restart networking.service\n&quot;
  #f.write &quot;rm -f /etc/resolv.conf\n&quot;
  #f.write &quot;ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf\n&quot;
  #f.write &quot;echo '      nameservers:'&gt;&gt;/etc/netplan/50-vagrant.yaml\n&quot;
  #f.write &quot;echo '        addresses: [10.148.129.101]'&gt;&gt;/etc/netplan/50-vagrant.yaml\n&quot;
  #f.write &quot;netplan apply eth1\n&quot;
end

......

        lv.default_prefix = 'rong1804node'
	lv.cpu_mode = 'host-passthrough'
	lv.storage_pool_name = 'default'
	lv.disk_bus = 'virtio'
	lv.nic_model_type = 'virtio'
	lv.volume_cache = 'writeback'
        # Fix kernel panic on fedora 28
......

 node.vm.synced_folder &quot;.&quot;, &quot;/vagrant&quot;, disabled: true, type: &quot;rsync&quot;, rsync__args: ['--verbose', '--archive', '--delete', '-z'] , rsync__exclude: ['.git','venv']
      $shared_folders.each do |src, dst|
......
      node.vm.provision &quot;shell&quot;, inline: &quot;swapoff -a&quot;
      # Change the dns-nameservers
      node.vm.provision :shell, path: &quot;dns.sh&quot;

......
        #&quot;docker_keepcache&quot;: &quot;1&quot;,
        #&quot;download_run_once&quot;: &quot;True&quot;,
        #&quot;download_localhost&quot;: &quot;False&quot;,
	&quot;ansible_python_interpreter&quot;: &quot;/usr/bin/python3&quot;

......
	  ansible.become_user = &quot;root&quot;
          ansible.limit = &quot;all&quot;
          ansible.host_key_checking = false
          #ansible.raw_arguments = [&quot;--forks=#{$num_instances}&quot;, &quot;--flush-cache&quot;, &quot;--ask-become-pass&quot;]
          ansible.host_vars = host_vars
          #ansible.tags = ['download']
          ansible.groups = {
            &quot;kube-deploy&quot; =&gt; [&quot;#{$instance_name_prefix}-[1:#{$kube_deploy_instances}]&quot;],
</code></pre><h4 id=3-inventory-folder>3. inventory folder</h4><p>Remove the hosts.ini file under <code>inventory/sample/</code>.<br>Edit some content in sample folder:</p><p>inventory/sample/group_vars/k8s-cluster/k8s-cluster.yml:</p><pre><code>kubelet_deployment_type: host
helm_deployment_type: host
helm_stable_repo_url: &quot;http://portus.ddddd.com:5000/chartrepo/kubesprayns&quot;
</code></pre><p>helm/metric server :</p><pre><code>helm_enabled: true


# Metrics Server deployment
metrics_server_enabled: true
</code></pre><h4 id=4-role-bootstrap-os>4. role bootstrap-os</h4><ol><li>Copy <code>daemon.json</code>, <code>ntp.conf</code>, <code>portus.crt</code>, <code>server.crt</code> to kubespray-2.8.2.</li></ol><p><img src=/images/2019_01_31_10_12_33_444x187.jpg alt=/images/2019_01_31_10_12_33_444x187.jpg></p><ol start=2><li>bootstrap-ubuntu.yml changes<br>For disable auto-update, and configure intranet repository.</li></ol><pre><code># Bug-fix1, disable auto-update for releasing apt controlling
- name: &quot;Configure apt service status&quot;
  raw: systemctl stop apt-daily.timer;systemctl disable apt-daily.timer ; systemctl stop apt-daily-upgrade.timer ; systemctl disable apt-daily-upgrade.timer; systemctl stop apt-daily.service;  systemctl mask apt-daily.service; systemctl daemon-reload

- name: &quot;Sed configuration&quot;
  raw: sed -i 's/APT::Periodic::Update-Package-Lists &quot;1&quot;/APT::Periodic::Update-Package-Lists &quot;0&quot;/' /etc/apt/apt.conf.d/10periodic

- name: Configure intranet repository
  raw: echo &quot;deb [trusted=yes] http://portus.dddd.com:8888 ./&quot;&gt;/etc/apt/sources.list &amp;&amp; apt-get update -y &amp;&amp; mkdir -p /usr/local/static
- name: List ubuntu_packages
</code></pre><p>Install more packages:</p><pre><code>      - python-pip
      - ntp
      - dbus
      - docker-ce
      - build-essential
</code></pre><p>Upload the crt files, add configuration files for docker, docker login ,etc.</p><pre><code>- name: &quot;upload portus.crt and server.crt files to kube-deploy&quot;
  copy:
    src: &quot;{{ item }}&quot;
    dest: /usr/local/share/ca-certificates
    owner: root
    group: root
    mode: 0777
  with_items:
    - files/portus.crt
    - files/server.crt

- name: &quot;upload daemon.json files to kube-deploy&quot;
  copy:
    src: files/daemon.json
    dest: /etc/docker/daemon.json
    owner: root
    group: root
    mode: 0777

- name: &quot;update ca-certificates&quot;
  shell: update-ca-certificates

- name: ensure docker service is restarted and enabled
  service:
    name: &quot;{{ item }}&quot;
    enabled: yes
    state: restarted
  with_items:
    - docker

- name: Docker login portus.dddd.com
  shell: docker login -u kubespray -p Thinker@1 portus.dddd.com:5000

- name: Docker login docker.io
  shell: docker login -u kubespray -p thinker docker.io

- name: Docker login quay.io
  shell: docker login -u kubespray -p thinker quay.io

- name: Docker login gcr.io
  shell: docker login -u kubespray -p thinker gcr.io

- name: Docker login k8s.gcr.io
  shell: docker login -u kubespray -p thinker k8s.gcr.io

- name: Docker login docker.elastic.co
  shell: docker login -u kubespray -p thinker docker.elastic.co

- name: &quot;upload ntp.conf&quot;
  copy:
    src: files/ntp.conf
    dest: /etc/ntp.conf
    owner: root
    group: root
    mode: 0777

- name: &quot;Configure ntp service&quot;
  shell: systemctl enable ntp &amp;&amp; systemctl start ntp
- set_fact:
    ansible_python_interpreter: &quot;/usr/bin/python&quot;
  tags:
    - facts
</code></pre><h4 id=5-role-download>5. role download</h4><p>Using local server for downloading:</p><pre><code># Download URLs
#kubeadm_download_url: &quot;https://storage.googleapis.com/kubernetes-release/release/{{ kubeadm_version }}/bin/linux/{{ image_arch }}/kubeadm&quot;
#hyperkube_download_url: &quot;https://storage.googleapis.com/kubernetes-release/release/{{ kube_version }}/bin/linux/amd64/hyperkube&quot;
etcd_download_url: &quot;https://github.com/coreos/etcd/releases/download/{{ etcd_version }}/etcd-{{ etcd_version }}-linux-amd64.tar.gz&quot;
#cni_download_url: &quot;https://github.com/containernetworking/plugins/releases/download/{{ cni_version }}/cni-plugins-{{ image_arch }}-{{ cni_version }}.tgz&quot;
kubeadm_download_url: &quot;http://portus.dddd.com:8888/kubeadm&quot;
hyperkube_download_url: &quot;http://portus.dddd.com:8888/hyperkube&quot;
cni_download_url: &quot;http://portus.dddd.com:8888/cni-plugins-{{ image_arch }}-{{ cni_version }}.tgz&quot;
</code></pre><h3 id=offline-role>Offline role</h3><p>Create an offline folder under role:</p><pre><code># mkdir -p roles/kube-deploy
</code></pre><p>Under this folder we got following files:</p><pre><code># tree .
.
├── files
│   ├── 1604debs.tar.xz
│   ├── 1804debs.tar.xz
│   ├── autoindex.tar.xz
│   ├── ban_nouveau.run
│   ├── busybox1262.tar.xz
│   ├── cni-plugins-amd64-v0.6.0.tgz
│   ├── cni-plugins-amd64-v0.7.0.tgz
│   ├── compose.tar.gz
│   ├── daemon.json
│   ├── dashboard.tar.xz
│   ├── data.tar.gz
│   ├── db.docker.io
│   ├── db.elastic.co
│   ├── db.gcr.io
│   ├── db.quay.io
│   ├── db.dddd.com
│   ├── deploy.key
│   ├── deploy.key.pub
│   ├── deploy_local.repo
│   ├── dns.sh
│   ├── docker-compose
│   ├── docker-infra.service
│   ├── fileserver.tar.gz
│   ├── harbor.tar.xz
│   ├── hyperkube
│   ├── install.run
│   ├── kubeadm
│   ├── kubeadm.official
│   ├── mynginx.service
│   ├── named.conf.default-zones
│   ├── nginx19.tar.xz
│   ├── nginx.tar
│   ├── nginx.yaml
│   ├── ntp.conf
│   ├── pipcache.tar.gz
│   ├── portus.crt
│   ├── portus.tar.xz
│   ├── registry2.tar.xz
│   ├── secureregistryserver.service
│   ├── secureregistryserver.tar
│   ├── server.crt
│   └── tag_and_push.sh
└── tasks
    ├── deploy-centos.yml
    ├── deploy-ubuntu.yml
    └── main.yml

2 directories, 45 files
</code></pre><p>Now using old file, for setting up a cluster, now login to kube-deploy server. do following steps:</p><pre><code># systemctl stop secureregistryserver
# docker ps | grep secureregistryserver
# rm -rf /usr/local/secureregistryserver/data/
# mkdir /usr/local/secureregistryserver/data
# systemctl start secureregistryserver
# docker ps | grep secureregistry
7ce648336f22        nginx:1.9                                   &quot;nginx -g 'daemon of…&quot;   3 seconds ago       Up 1 second              80/tcp, 0.0.0.0:443-&gt;443/tcp                                          secureregistryserver_nginx_1_ce6348edc346
9258c0396001        registry:2                                  &quot;/entrypoint.sh /etc…&quot;   4 seconds ago       Up 2 seconds             127.0.0.1:5050-&gt;5000/tcp                                              secureregistryserver_registry_1_76f71f58f22d
</code></pre><p>Load the kubespray images and push to secureregistryserver:</p><pre><code># docker load&lt;combine.tar.xz
# docker push *****************
# docker push *****************
# docker push *****************
</code></pre><p>Load the kubeadm related images and push to secureregistryserver:</p><pre><code># docker load&lt;kubeadm.tar
# docker push *****************
# docker push *****************
# docker push *****************
</code></pre><p>Now stop the secureregistryserver service and make the tar file:</p><pre><code># systemctl stop secureregistryserver
# cd /usr/local/
# tar cvf secureregistryserver.tar secureregistryserver
</code></pre><p>The secureregistryserver.tar should be replaced into the <code>kube-deploy/files</code> folder</p><p>Replace the kubeadm( we add 100 years ssl signature):</p><pre><code># sha256sum kubeadm 
33ef160d8dd22e7ef6eb4527bea861218289636e6f1e3e54f2b19351c1048a07  kubeadm
# vim /var1/sync/kubespray282/kubespray282/kubespray-2.8.2/roles/download/defaults/main.yml
kubeadm_checksums:
  v1.12.5: 33ef160d8dd22e7ef6eb4527bea861218289636e6f1e3e54f2b19351c1048a07

</code></pre><p>Replaces the hyperkube:</p><pre><code># wget https://storage.googleapis.com/kubernetes-release/release/v1.12.5/bin/linux/amd64/hyperkube
</code></pre><h3 id=replace-new-items>Replace New Items</h3><p>!!!Rechecked here!!!</p><p>debs.tar.xz should be replaced(Currently we use the old ones)
secureregistryserver.tar contains all of the offline docker images, must be replaced.<br>kubeadm should be replaced, and checksums should be updated.
hyperkube should be replaced.</p><h3 id=using-vps-for-fetching-images>Using vps for fetching images</h3><p>Under a vps, download the kubespray-2.8.2.tar.gz, and made some modifications for syncing back the images:</p><p>Create a new sync.yml file:</p><pre><code>root@vpsserver:~/Code/kubespray-2.8.2# cat sync.yml 
---
- hosts: k8s-cluster:etcd:calico-rr
  any_errors_fatal: &quot;{{ any_errors_fatal | default(true) }}&quot;
  roles:
    - { role: kubespray-defaults}
    - { role: sync, tags: download, when: &quot;not skip_downloads&quot; }
  environment: &quot;{{proxy_env}}&quot;
</code></pre><p>Copy the sample folder under <code>inventory</code> to a new name <code>sync</code>:</p><pre><code>root@vpsserver:~/Code/kubespray-2.8.2# cat inventory/sync/hosts.ini 
# ## Configure 'ip' variable to bind kubernetes services on a
# ## different ip than the default iface
# ## We should set etcd_member_name for etcd cluster. The node that is not a etcd member do not need to set the value, or can set the empty string value.
[all]
vpsserver ansible_ssh_host=1xxx.xxx.xxx.xx ansible_user=root ansible_ssh_private_key_file=&quot;/root/.ssh/id_rsa&quot; ansible_port=22

[kube-master]
vpsserver

[etcd]
vpsserver

[kube-node]
vpsserver

[k8s-cluster:children]
kube-master
kube-node
</code></pre><p>Copy <code>roles/download</code> to <code>roles/sync</code>, and make some changes, notice all of the downloads docker images should changes to <code>enabled:true</code>:</p><pre><code>root@vpsserver:~/Code/kubespray-2.8.2/roles# ls
adduser             bootstrap-os      dnsmasq   etcd        kubernetes-apps     network_plugin  reset  upgrade
bastion-ssh-config  container-engine  download  kubernetes  kubespray-defaults  remove-node     sync   win_nodes
root@vpsserver:~/Code/kubespray-2.8.2/roles# cd sync/
root@vpsserver:~/Code/kubespray-2.8.2/roles/sync# ls
defaults  meta  tasks
root@vpsserver:~/Code/kubespray-2.8.2/roles/sync# cat defaults/main.yml 
downloads:
  netcheck_server:
    #enabled: &quot;{{ deploy_netchecker }}&quot;
    enabled: true
    container: true
    repo: &quot;{{ netcheck_server_image_repo }}&quot;
    tag: &quot;{{ netcheck_server_image_tag }}&quot;
    sha256: &quot;{{ netcheck_server_digest_checksum|default(None) }}&quot;
    groups:
      - k8s-cluster

  netcheck_agent:
    #enabled: &quot;{{ deploy_netchecker }}&quot;
    enabled:  true
    container: true
    repo: &quot;{{ netcheck_agent_image_repo }}&quot;
    tag: &quot;{{ netcheck_agent_image_tag }}&quot;
    sha256: &quot;{{ netcheck_agent_digest_checksum|default(None) }}&quot;
    groups:
      - k8s-cluster
</code></pre><p>Disalbe <code>Sync containers</code> tasks, so we will only download the docker images to local.</p><pre><code>root@vpsserver:~/Code/kubespray-2.8.2/roles/sync# cat tasks/main.yml 
---
- include_tasks: download_prep.yml
  when:
    - not skip_downloads|default(false)

- name: &quot;Download items&quot;
  include_tasks: &quot;download_{% if download.container %}container{% else %}file{% endif %}.yml&quot;
  vars:
    download: &quot;{{ download_defaults | combine(item.value) }}&quot;
  with_dict: &quot;{{ downloads }}&quot;
  when:
    - not skip_downloads|default(false)
    - item.value.enabled
    - (not (item.value.container|default(False))) or (item.value.container and download_container)

#- name: &quot;Sync container&quot;
#  include_tasks: sync_container.yml
#  vars:
#    download: &quot;{{ download_defaults | combine(item.value) }}&quot;
#  with_dict: &quot;{{ downloads }}&quot;
#  when:
#    - not skip_downloads|default(false)
#    - item.value.enabled
#    - item.value.container | default(false)
#    - download_run_once
#    - group_names | intersect(download.groups) | length
</code></pre><p>All of the kubespray pre-defined images will be downloaded to vps, check via:</p><p><img src=/images/2019_01_31_10_51_03_784x538.jpg alt=/images/2019_01_31_10_51_03_784x538.jpg></p><p>Save it using scripts:</p><pre><code># docker save -o combine.tar gcr.io/google-containers/hyperkube-amd64:v1.12.5 registry:2.6 busybox:latest quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0 quay.io/jetstack/cert-manager-controller:v0.5.2 weaveworks/weave-npc:2.5.0 weaveworks/weave-kube:2.5.0 coredns/coredns:1.2.6 cilium/cilium:v1.3.0 nfvpe/multus:v3.1.autoconf cloudnativelabs/kube-router:v0.2.1 gcr.io/google_containers/cluster-proportional-autoscaler-amd64:1.3.0 lachlanevenson/k8s-helm:v2.11.0 gcr.io/kubernetes-helm/tiller:v2.11.0 gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.13 gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.13 gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.13 k8s.gcr.io/metrics-server-amd64:v0.3.1 quay.io/external_storage/cephfs-provisioner:v2.1.0-k8s1.11 gcr.io/google_containers/kubernetes-dashboard-amd64:v1.10.0 busybox:1.29.2 quay.io/coreos/etcd:v3.2.24 k8s.gcr.io/addon-resizer:1.8.3 quay.io/calico/node:v3.1.3 quay.io/calico/ctl:v3.1.3 quay.io/calico/kube-controllers:v3.1.3 quay.io/calico/cni:v3.1.3 nginx:1.13 quay.io/external_storage/local-volume-provisioner:v2.1.0 quay.io/calico/routereflector:v0.6.1 quay.io/coreos/flannel:v0.10.0 contiv/netplugin:1.2.1 contiv/auth_proxy:1.2.1 gcr.io/google_containers/pause-amd64:3.1 ferest/etcd-initer:latest andyshinn/dnsmasq:2.78 quay.io/coreos/flannel-cni:v0.3.0 xueshanf/install-socat:latest quay.io/l23network/k8s-netchecker-agent:v1.0 quay.io/l23network/k8s-netchecker-server:v1.0 gcr.io/google_containers/kube-registry-proxy:0.4 
# xz combine.tar
</code></pre><h3 id=using-vagrant-for-fetching-kubeadm>Using vagrant for fetching kubeadm</h3><p>Your vagrant box should cross gfw, then you use a 1604 box for create an all-in-one k8s cluster.<br>After provision, use following command for saving all of the images:</p><pre><code># docker images
# docker save -o combine.tar xxxxx xxx xxx xxx xxx xxx
</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/01/29/multipleofflineregistryserver/>MultipleOfflineRegistryServer</a></h1><span class=post-date>Jan 29, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>Suppose your kubernetes clusters are in totally offline environment, you don&rsquo;t
have any connection to Internet, how to deal with the <code>docker pull</code> request
within the cluster? Following are steps for solving this problem.</p><h3 id=docker-composed-registry>Docker-Composed Registry</h3><p>This step is pretty simple, I mainly refers to:</p><p><a href=https://www.digitalocean.com/community/tutorials/how-to-set-up-a-private-docker-registry-on-ubuntu-14-04>https://www.digitalocean.com/community/tutorials/how-to-set-up-a-private-docker-registry-on-ubuntu-14-04</a></p><p>Following the steps in article, I got a running docker-compose triggered
docker registry server.</p><p>Following is my folder structure:</p><pre><code># tree .
├── data
├── docker-compose.yml
├── nginx
│   ├── openssl.cnf
│   ├── registry.conf
│   ├── registry.password
│   ├── server.crt
│   ├── server.csr
│   └── server.key
</code></pre><p>Docker-compose file is listed as:</p><pre><code># cat docker-compose.yml 
nginx:
  image: &quot;nginx:1.9&quot;
  ports:
    - 443:443
  links:
    - registry:registry
  volumes:
    - ./nginx/:/etc/nginx/conf.d:ro

registry:
  image: registry:2
  ports:
    - 127.0.0.1:5000:5000
  environment:
    REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY: /data
  volumes:
    - ./data:/data
</code></pre><h3 id=handle-ssl>Handle SSL</h3><p>The above steps will only create a single hostname ssl enabled docker registry
server, we want to enable multi-domain self-signed SSL certificate.</p><p>Create a file called openssl.cnf with the following details, notice that we
have set <code>docker.io</code>, <code>gcr.io</code>, <code>quay.io</code> and <code>elastic.co</code>.</p><pre><code># vim openssl.cnf
[req]
distinguished_name = req_distinguished_name
req_extensions = v3_req

[req_distinguished_name]
countryName = SL
countryName_default = SL
stateOrProvinceName = Western
stateOrProvinceName_default = Western
localityName = Colombo
localityName_default = Colombo
organizationalUnitName = ABC
organizationalUnitName_default = ABC
commonName = *.docker.io
commonName_max = 64

[ v3_req ]
# Extensions to add to a certificate request
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = *.docker.io
DNS.2 = *.gcr.io
DNS.3 = *.quay.io
DNS.4 = *.elastic.co
DNS.5 = docker.io
DNS.6 = gcr.io
DNS.7 = quay.io
DNS.8 = elastic.co
</code></pre><p>Create the Private key.</p><pre><code># rm -f server.key 
# sudo openssl genrsa -out server.key 2048
Generating RSA private key, 2048 bit long modulus (2 primes)
.....................+++++
......................................+++++
e is 65537 (0x010001)
</code></pre><p>Create Certificate Signing Request (CSR).</p><pre><code># rm -f server.csr 
# sudo openssl req -new -out server.csr -key server.key -config openssl.cnf
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
SL [SL]:
Western [Western]:
Colombo [Colombo]:
ABC [ABC]:
*.docker.io []:
</code></pre><p><code>*.docker.io</code> is from the above defined parts in <code>openssl.cnf</code>.</p><p>Sign the SSL Certificate.</p><pre><code># rm -f server.crt 
# sudo openssl x509 -req -days 36500 -in server.csr -signkey server.key -out server.crt -extensions v3_req -extfile openssl.cnf
Signature ok
subject=C = SL, ST = Western, L = Colombo, OU = ABC
Getting Private key
</code></pre><p>You can check the ssl certificated content via:</p><pre><code># openssl x509 -in server.crt -noout -text 
Certificate:
    Data:
....

        Validity
            Not Before: Jan 29 08:30:20 2019 GMT
            Not After : Jan  5 08:30:20 2119 GMT
....

            X509v3 Subject Alternative Name: 
                DNS:*.docker.io, DNS:*.gcr.io, DNS:*.quay.io, DNS:*.elastic.co, DNS:docker.io, DNS:gcr.io, DNS:quay.io, DNS:elastic.co
</code></pre><h3 id=nginx-conf>nginx conf</h3><p>Add the domain items into registry.conf:</p><pre><code>upstream docker-registry {
  server registry:5000;
}

server {
  listen 443;
  server_name docker.io gcr.io quay.io elastic.co;

  # SSL
  ssl on;
  ssl_certificate /etc/nginx/conf.d/server.crt;
  ssl_certificate_key /etc/nginx/conf.d/server.key;
</code></pre><h3 id=dns-configuration>dns configuration</h3><p>Install bind9 on ubuntu, or install dnsmasq on centos system. take bind9&rsquo;s
configuration for example, add following configurations:</p><pre><code># ls db*
db.docker.io  db.gcr.io  db.quay.io
</code></pre><p>The <code>named.conf.default-zones</code> is listed as following:</p><pre><code># cat named.conf.default-zones 
// prime the server with knowledge of the root servers
zone &quot;.&quot; {
	type hint;
	file &quot;/etc/bind/db.root&quot;;
};

// be authoritative for the localhost forward and reverse zones, and for
// broadcast zones as per RFC 1912

zone &quot;localhost&quot; {
	type master;
	file &quot;/etc/bind/db.local&quot;;
};

zone &quot;127.in-addr.arpa&quot; {
	type master;
	file &quot;/etc/bind/db.127&quot;;
};

zone &quot;0.in-addr.arpa&quot; {
	type master;
	file &quot;/etc/bind/db.0&quot;;
};

zone &quot;255.in-addr.arpa&quot; {
	type master;
	file &quot;/etc/bind/db.255&quot;;
};

zone &quot;docker.io&quot; {
        type master;
        file &quot;/etc/bind/db.docker.io&quot;; 
};

zone &quot;quay.io&quot; {
        type master;
        file &quot;/etc/bind/db.quay.io&quot;; 
};

zone &quot;gcr.io&quot; {
        type master;
        file &quot;/etc/bind/db.gcr.io&quot;; 
};

zone &quot;elastic.co&quot; {
        type master;
        file &quot;/etc/bind/db.elastic.co&quot;; 
};
</code></pre><p>Take <code>elastic.co</code> for example, show its content:</p><pre><code># cat db.elastic.co
$TTL    604800
@       IN      SOA     elastic.co. root.localhost. (
                              1         ; Serial
                         604800         ; Refresh
                          86400         ; Retry
                        2419200         ; Expire
                         604800 )       ; Negative Cache TTL
;
@       IN      NS      localhost.
@       IN      A      192.168.122.154
elastic.co IN      NS      192.168.122.154

docker     IN      A       192.168.122.154
</code></pre><h3 id=using-the-docker-registry>Using the docker registry</h3><p>Point the nodes&rsquo; dns server to your registry nodes, so if you ping elastic.co,
you got reply from <code>192.168.122.154</code>.</p><p>Then add the crt into your docker&rsquo;s trusted sites:</p><pre><code># cp server.crt /usr/local/share/ca-certificates
# update-ca-certificates
# systemctl restart docker
# docker login -u dddd -p gggg docker.io
# docker login -u dddd -p gggg quay.io
# docker login -u dddd -p gggg gcr.io
# docker login -u dddd -p gggg k8s.gcr.io
</code></pre><p>Now you can freely push/pull images from offline registry server, as if you
are on Internet.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/01/28/tipsonhelmchartsprometheus/>TipsOnHelmChartsPrometheus</a></h1><span class=post-date>Jan 28, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=upgrade-helm>Upgrade helm</h3><p>Download the helm from github, and extracted to your system PATH, then:</p><pre><code># helm init --upgrade
</code></pre><p>Notice in your k8s cluster your tiller will be upgraded:</p><pre><code>kube-system   tiller-deploy-68b77f4c57-dwd47                                  0/1       ContainerCreating   0          13s
</code></pre><p>Examine the upgraded version:</p><pre><code># helm version
Client: &amp;version.Version{SemVer:&quot;v2.12.2&quot;, GitCommit:&quot;7d2b0c73d734f6586ed222a567c5d103fed435be&quot;, GitTreeState:&quot;clean&quot;}
Server: &amp;version.Version{SemVer:&quot;v2.12.2&quot;, GitCommit:&quot;7d2b0c73d734f6586ed222a567c5d103fed435be&quot;, GitTreeState:&quot;clean&quot;}
</code></pre><h3 id=working-issue>Working Issue</h3><p>Using stable/prometheus-operator.</p><p>Fetch the helm/charts package via:</p><pre><code># helm fetch stable/prometheus-operator
# ls
prometheus-operator-1.9.0.tgz
</code></pre><p>Deploy it on the minikube and you will get all of the images, then save it
via:</p><pre><code>eval $(minikube docker-env)
docker save -o prometheus-operator.tar grafana/grafana:5.4.3 quay.io/prometheus/node-exporter:v0.17.0 quay.io/coreos/prometheus-config-reloader:v0.26.0 quay.io/coreos/prometheus-operator:v0.26.0 quay.io/prometheus/alertmanager:v0.15.3 quay.io/prometheus/prometheus:v2.5.0 kiwigrid/k8s-sidecar:0.0.6 quay.io/coreos/kube-state-metrics:v1.4.0 quay.io/coreos/configmap-reload:v0.0.1
xz prometheus-operator.tar
</code></pre><p>Then we could write the offline scripts for deploying this helm/charts.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/01/22/workingtipsonubuntuglusterfs/>WorkingTipsOnUbuntuGlusterFS</a></h1><span class=post-date>Jan 22, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=install>Install</h3><p>In 4 nodes, install the latest glusterfs via:</p><pre><code>sudo add-apt-repository -y ppa:gluster/glusterfs-5
sudo apt-get -y update
sudo apt-get -y install glusterfs-server
sudo apt-get -y install thin-provisioning-tools
</code></pre><h3 id=configure>Configure</h3><p>See following steps for configurating the gluster in 4
nodes(10.48.129.101~104):</p><pre><code>root@gluster-1:/home/vagrant# gluster peer status
Number of Peers: 0
root@gluster-1:/home/vagrant# gluster pool list
UUID					Hostname 	State
628ce8f2-622c-4be3-92b0-d8e5241d01b8	localhost	Connected 
root@gluster-1:/home/vagrant# ifconfig eth1
eth1      Link encap:Ethernet  HWaddr 52:54:00:c8:74:01  
          inet addr:10.48.129.101  Bcast:10.48.129.255  Mask:255.255.255.0
          inet6 addr: fe80::5054:ff:fec8:7401/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:2456 errors:0 dropped:17 overruns:0 frame:0
          TX packets:103 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:126151 (126.1 KB)  TX bytes:10351 (10.3 KB)

root@gluster-1:/home/vagrant# gluster peer probe 10.48.129.102
peer probe: success. 
root@gluster-1:/home/vagrant# gluster peer probe 10.48.129.103
peer probe: success. 
root@gluster-1:/home/vagrant# gluster peer probe 10.48.129.104
peer probe: success. 
root@gluster-1:/home/vagrant# gluster peer status
Number of Peers: 3

Hostname: 10.48.129.102
Uuid: 52472b8f-b835-4038-b061-89229d825c42
State: Peer in Cluster (Connected)

Hostname: 10.48.129.103
Uuid: 46161901-954a-4ffe-9aec-a8c14ceded03
State: Peer in Cluster (Connected)

Hostname: 10.48.129.104
Uuid: a597a3ff-8c32-4a5a-b83d-1d25187afe31
State: Peer in Cluster (Connected)
</code></pre><p>After peer probe, the cluster is listed as following:</p><pre><code>root@gluster-1:/home/vagrant# gluster peer status
Number of Peers: 3

Hostname: 10.48.129.102
Uuid: 52472b8f-b835-4038-b061-89229d825c42
State: Peer in Cluster (Connected)

Hostname: 10.48.129.103
Uuid: 46161901-954a-4ffe-9aec-a8c14ceded03
State: Peer in Cluster (Connected)

Hostname: 10.48.129.104
Uuid: a597a3ff-8c32-4a5a-b83d-1d25187afe31
State: Peer in Cluster (Connected)
root@gluster-1:/home/vagrant# gluster pool list
UUID					Hostname     	State
52472b8f-b835-4038-b061-89229d825c42	10.48.129.102	Connected 
46161901-954a-4ffe-9aec-a8c14ceded03	10.48.129.103	Connected 
a597a3ff-8c32-4a5a-b83d-1d25187afe31	10.48.129.104	Connected 
628ce8f2-622c-4be3-92b0-d8e5241d01b8	localhost    	Connected 
</code></pre><h3 id=heketi>heketi</h3><p>Download the heketi from:</p><pre><code># wget https://github.com/heketi/heketi/releases/download/v8.0.0/heketi-v8.0.0.linux.amd64.tar.gz
# tar xzvf heketi-v8.0.0.linux.amd64.tar.gz
# cd heketi
# cp heketi heketi-cli /usr/bin
</code></pre><p>Configuration:</p><pre><code># cd heketi
# cp heketi.json heketi.json.back
# vim heketi.json
# mkdir -p /etc/heketi
# cp heketi.json /etc/heketi
</code></pre><p>Your content is listed as following:</p><pre><code>......
#修改端口，防止端口冲突
  &quot;port&quot;: &quot;18080&quot;,
......
#允许认证
  &quot;use_auth&quot;: true,
......
#admin用户的key改为adminkey
      &quot;key&quot;: &quot;adminkey&quot;
......
#修改执行插件为ssh，并配置ssh的所需证书，注意要能对集群中的机器免密ssh登陆，使用ssh-copy-id把pub key拷到每台glusterfs服务器上
    &quot;executor&quot;: &quot;ssh&quot;,
    &quot;sshexec&quot;: {
      &quot;keyfile&quot;: &quot;/root/.ssh/id_rsa&quot;,
      &quot;user&quot;: &quot;root&quot;,
      &quot;port&quot;: &quot;22&quot;,
      &quot;fstab&quot;: &quot;/etc/fstab&quot;
    },
......
# 定义heketi数据库文件位置
    &quot;db&quot;: &quot;/var/lib/heketi/heketi.db&quot;
......
#调整日志输出级别
    &quot;loglevel&quot; : &quot;warning&quot;
</code></pre><p>Now start the heketi server via:</p><pre><code># heketi --config=/etc/heketi/heketi.json
</code></pre><p>Your server now is listening at <code>localhost:18080</code>, now use the cli for
creating the cluster and volume.</p><pre><code># heketi-cli --server http://localhost:18080 --user admin --secret &quot;adminkey&quot; cluster list
# heketi-cli --server http://localhost:18080 --user admin --secret &quot;adminkey&quot; cluster create
# alias heketi-cli-admin='heketi-cli --server http://localhost:18080 --user admin --secret &quot;adminkey&quot;'
# heketi-cli-admin node add --cluster &quot;360e5d504ff7cc18823d580bac55711a&quot; --management-host-name 10.48.129.101 --storage-host-name 10.48.129.101 --zone 1
# heketi-cli-admin node add --cluster &quot;360e5d504ff7cc18823d580bac55711a&quot; --management-host-name 10.48.129.102 --storage-host-name 10.48.129.102 --zone 1
# heketi-cli-admin node add --cluster &quot;360e5d504ff7cc18823d580bac55711a&quot; --management-host-name 10.48.129.103 --storage-host-name 10.48.129.103 --zone 1
# heketi-cli-admin node add --cluster &quot;360e5d504ff7cc18823d580bac55711a&quot; --management-host-name 10.48.129.104 --storage-host-name 10.48.129.104 --zone 1
# heketi-cli-admin --json device add --name=&quot;/dev/vdb&quot; --node &quot;f787daa4ff84461b11b7f7d00645dfdc&quot;
# heketi-cli-admin --json device add --name=&quot;/dev/vdb&quot; --node &quot;16f738ee9822fb79c7b321c0b7ed1792&quot;
# heketi-cli-admin --json device add --name=&quot;/dev/vdb&quot; --node &quot;c6b1f0ccfe1ff76a245012108c44043f&quot;
# heketi-cli-admin --json device add --name=&quot;/dev/vdb&quot; --node &quot;2e631bedcb6b58c4b82bba31082531f1&quot;
# heketi-cli-admin volume list
# heketi-cli-admin volume create --size=10
</code></pre><p>After it finishes, your volume will be created.</p></div><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/57/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/57/>57</a></li><li class="page-item active"><a class=page-link href=/page/58/>58</a></li><li class=page-item><a class=page-link href=/page/59/>59</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/233/>233</a></li><li class=page-item><a href=/page/59/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/233/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div><script src=http://purplepalmdash.github.io/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>