<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta name=generator content="Hugo 0.64.0"><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Dash &#183; Dash</title><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124"><link rel=stylesheet href=https://purplepalmdash.github.io/css/highlight/googlecode.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js></script><script type=text/javascript src=/js/html2canvas.js></script><script type=text/javascript>function genPostShot(){var rightNow=new Date();var imageName=rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");imageName+='.jpg'
html2canvas(document.getElementsByClassName('post'),{background:'#FFFFFF',onrendered:function(canvas){$('#test').attr('href',canvas.toDataURL("image/jpeg"));$('#test').attr('download',imageName);$('#test')[0].click();}});};</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124"><link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel=icon><link href=%7balternate%20%7bRSS%20application/rss+xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://purplepalmdash.github.io/index.xml%7d rel=alternate type=application/rss+xml title="Dash &#183; Dash"><meta name=description content><meta name=keywords content="unix,virtualization,embedded,linux"></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><img src=http://purplepalmdash.github.io/images/mylogo.jpeg alt=gravatar><h1><a href=http://purplepalmdash.github.io/>很惭愧，就做了一点微小的工作</a></h1><a href=http://purplepalmdash.github.io/><p>Dash</p></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/post/>All Posts</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/technology/>Technology</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/life/>Life</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/linuxtips/>LinuxTips</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/purplepalmdash><i class="fa fa-github-square fa-3x"></i></a><a href=https://cn.linkedin.com/in/yang-feipeng-1b909319><i class="fa fa-linkedin-square fa-3x"></i></a><a href=https://plus.google.com/u/0/106572959364703833986><i class="fa fa-google-plus-square fa-3x"></i></a><a href=https://www.facebook.com/yang.feipeng><i class="fa fa-facebook-square fa-3x"></i></a><a href=https://twitter.com/dashwillfly><i class="fa fa-twitter-square fa-3x"></i></a></li></ul></div></div><div class="content container"><div class=posts><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2021/06/05/workingtipsongpupassthrough/>WorkingTipsOnGpuPassthrough</a></h1><span class=post-date>Jun 5, 2021<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>近来要调研一些虚拟化的东西，主要包括轻量级虚拟化、边缘池算力等，记下一些关键的点以便后续整理。</p><p>手头的两台GPU服务器，每台上面有7块GPU卡，需要将它们透传到虚拟机中以便调研相关系统的性能。</p><h3 id=环境>环境</h3><p>环境信息列举如下:</p><pre><code>CPU: model name      : Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz
Memory: 
free -m
              total        used        free      shared  buff/cache   available
Mem:         385679       22356      284811        1042       78512      358158
Swap:             0           0           0
GPU: 
# lspci -nn | grep -i nvidia
3d:00.0 3D controller [0302]: NVIDIA Corporation GV100GL [Tesla V100 PCIe 32GB] [10de:xxxx] (rev a1)
3e:00.0 3D controller [0302]: NVIDIA Corporation GV100GL [Tesla V100 PCIe 32GB] [10de:xxxx] (rev a1)
40:00.0 3D controller [0302]: NVIDIA Corporation GV100GL [Tesla V100 PCIe 32GB] [10de:xxxx] (rev a1)
41:00.0 3D controller [0302]: NVIDIA Corporation GV100GL [Tesla V100 PCIe 32GB] [10de:xxxx] (rev a1)
b1:00.0 3D controller [0302]: NVIDIA Corporation GV100GL [Tesla V100 PCIe 32GB] [10de:xxxx] (rev a1)
b2:00.0 3D controller [0302]: NVIDIA Corporation GV100GL [Tesla V100 PCIe 32GB] [10de:xxxx] (rev a1)
b4:00.0 3D controller [0302]: NVIDIA Corporation GV100GL [Tesla V100 PCIe 32GB] [10de:xxxx] (rev a1)
b5:00.0 3D controller [0302]: NVIDIA Corporation GV100GL [Tesla V100 PCIe 32GB] [10de:xxxx] (rev a1)
</code></pre><p>操作系统：</p><pre><code># cat /etc/redhat-release 
CentOS Linux release 7.6.1810 (Core) 
# /usr/libexec/qemu-kvm --version
QEMU emulator version 2.12.0 (qemu-kvm-ev-2.12.0-44.1.el7_8.1)
Copyright (c) 2003-2017 Fabrice Bellard and the QEMU Project developers
# uname -r
4.19.12-1.el7.elrepo.x86_64
</code></pre><h3 id=系统配置>系统配置</h3><p>激活IOMMU， 通过编辑<code>/etc/default/grub</code>中的<code>GRUB_CMDLINE_LINUX</code>行，增加<code>intel_iommu=on</code>后，重新生成grub引导文件：</p><pre><code># vim /etc/default/grub
GRUB_CMDLINE_LINUX=&quot;crashkernel=auto rhgb quiet intel_iommu=on rd.driver.pre=vfio-pci&quot;
# grub-mkconfig -o /boot/grub2/grub.cfg
# reboot
重启后，检查：   
dmesg | grep -E &quot;DMAR|IOMMU&quot;

</code></pre><p>激活<code>vfio-pci</code>内核模块， 注意填入的数值是<code>lspci</code>取得的:</p><pre><code># options vfio-pci ids=10de:xxxx
</code></pre><p>激活<code>vfio-pci</code>的自动加载:</p><pre><code># echo 'vfio-pci' &gt; /etc/modules-load.d/vfio-pci.conf
# reboot
# dmesg | grep -i vfio
</code></pre><p>qemu的更新步骤如下:</p><pre><code># /usr/libexec/qemu-kvm -version
QEMU emulator version 1.5.3 (qemu-kvm-1.5.3-141.el7_4.6), Copyright (c) 2003-2008 Fabrice Bellard
# yum -y install centos-release-qemu-ev
# sed -i -e &quot;s/enabled=1/enabled=0/g&quot; /etc/yum.repos.d/CentOS-QEMU-EV.repo
# yum --enablerepo=centos-qemu-ev -y install qemu-kvm-ev
# systemctl restart libvirtd
# /usr/libexec/qemu-kvm -version
QEMU emulator version 2.12.0 (qemu-kvm-ev-2.12.0-44.1.el7_8.1)
</code></pre><p>现在在virt-manager中是可以指定下放GPU的。</p><p><img src=/images/2021_06_05_08_17_16_785x569.jpg alt=/images/2021_06_05_08_17_16_785x569.jpg></p><p>登录到虚拟机以后：</p><pre><code>root@localhost:~# lspci -nn | grep -i nvidia
00:0a.0 3D controller [0302]: NVIDIA Corporation GV100GL [Tesla V100 PCIe 32GB] [10de:xxxx] (rev a1)
</code></pre><p>接下来我会调研如何在lxd下及在k3s+kubevirt的场景下透传GPU.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2021/06/03/enablehostnetworkinlxd/>EnableHostNetworkInLXD</a></h1><span class=post-date>Jun 3, 2021<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>lxc/lxd lacks of using host network like <code>docker run -it --net host</code>, following are the steps on how to enable this feature:</p><p>注：使用host网络模式会使得主机层面的网络增强在容器层面被绕过，等同于完全放开了网络安全策略，被攻击面增大。</p><p>建议谨慎使用。</p><h3 id=lxd开启hostnetwork>lxd开启hostNetwork</h3><p>将默认额lxd profile导入到文件中:</p><pre><code># lxc profile show default&gt;host
</code></pre><p>编辑文件，去掉关于网卡的配置(前置<code>-</code>的行需要被删除):</p><pre><code># vim host
config:
  security.secureboot: &quot;false&quot;
description: Default LXD profile
devices:
-  eth0:
-    name: eth0
-    network: lxdbr0
-    type: nic
  root:
    path: /
    pool: default
    type: disk
name: default
used_by:
- /1.0/instances/centos
</code></pre><p>删除后保存文件，创建一个名称为<code>hostnetwork</code>的<code>profile</code>，用修改过的配置文件定义之:</p><pre><code>lxc profile create hostnetwork
lxc profile edit hostnetwork&lt;host
</code></pre><p>创建一个新实例:</p><pre><code># lxc init adcfe657303d ubuntu1
</code></pre><p>指定<code>hostnetwork</code>的profile为其启动所需的profile:</p><pre><code># lxc profile assign ubuntu1 hostnetwork
</code></pre><p>创建一个<code>raw.lxc</code>的外部配置文件:</p><pre><code># vim /root/lxc_host_network_config
lxc.net.0.type = none
</code></pre><p>配置该lxc实例的属性(<code>raw.lxc</code>及特权容器), 在某些系统上可能不需要特权容器, 设置完毕后启动:</p><pre><code>lxc config set ubuntu1 raw.lxc=&quot;lxc.include=/root/lxc_host_network_config&quot;
lxc config set ubuntu1 security.privileged=true
lxc start ubuntu1
</code></pre><p>检查lxc实例所拥有的网络地址空间:</p><pre><code># lxc ls
lxc ls
+---------+---------+---------------------------+--------------------------------------------------+-----------------+-----------+
|  NAME   |  STATE  |           IPV4            |                       IPV6                       |      TYPE       | SNAPSHOTS |
+---------+---------+---------------------------+--------------------------------------------------+-----------------+-----------+
| centos  | RUNNING | 10.225.0.168 (enp5s0)     | fd42:1cac:64d0:f018:547d:b31b:251b:381e (enp5s0) | VIRTUAL-MACHINE | 0         |
+---------+---------+---------------------------+--------------------------------------------------+-----------------+-----------+
| ubuntu1 | RUNNING | 192.192.189.1 (virbr1)    | fd42:1cac:64d0:f018::1 (lxdbr0)                  | CONTAINER       | 0         |
|         |         | 192.168.122.1 (virbr0)    | 240e:3b5:cb5:abf0:36d4:30d3:285b:862e (enp3s0)   |                 |           |
|         |         | 192.168.1.222 (enp3s0)    |                                                  |                 |           |
|         |         | 172.23.8.165 (ztwdjmv5j3) |                                                  |                 |           |
|         |         | 172.17.0.1 (docker0)      |                                                  |                 |           |
|         |         | 10.33.34.1 (virbr2)       |                                                  |                 |           |
|         |         | 10.225.0.1 (lxdbr0)       |                                                  |                 |           |
+---------+---------+---------------------------+--------------------------------------------------+-----------------+-----------+
</code></pre><p>进入到网络中检查所有网卡:</p><pre><code># lxc exec ubuntu1 bash
root@ubuntu1:~# ip link
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: enp3s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000
    link/ether 74:d4:35:6a:84:19 brd ff:ff:ff:ff:ff:ff
3: ztwdjmv5j3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 2800 qdisc fq_codel state UNKNOWN mode DEFAULT group default qlen 1000
    link/ether 0e:d0:a9:d8:58:2a brd ff:ff:ff:ff:ff:ff
4: wlp2s0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc mq state DOWN mode DORMANT group default qlen 1000
    link/ether 40:e2:30:30:1e:ee brd ff:ff:ff:ff:ff:ff
5: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default qlen 1000
    link/ether 52:54:00:ea:66:bb brd ff:ff:ff:ff:ff:ff
6: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default 
    link/ether 02:42:d0:f9:2f:53 brd ff:ff:ff:ff:ff:ff
7: lxdbr0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
    link/ether 00:16:3e:b7:04:a9 brd ff:ff:ff:ff:ff:ff
8: virbr1: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default qlen 1000
    link/ether 52:54:00:88:da:f3 brd ff:ff:ff:ff:ff:ff
9: virbr2: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default qlen 1000
    link/ether 52:54:00:7a:73:8b brd ff:ff:ff:ff:ff:ff
10: tapeac054b8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master lxdbr0 state UP mode DEFAULT group default qlen 1000
    link/ether fe:25:d8:30:90:f6 brd ff:ff:ff:ff:ff:ff
11: veth43eb373c@veth5f86c132: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 00:16:3e:cb:fe:fc brd ff:ff:ff:ff:ff:ff
12: veth5f86c132@veth43eb373c: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP,M-DOWN&gt; mtu 1500 qdisc noqueue master lxdbr0 state LOWERLAYERDOWN mode DEFAULT group default qlen 1000
    link/ether 9e:b0:74:e7:fa:86 brd ff:ff:ff:ff:ff:ff
19: veth8d89fdf9@vethb3a2bfad: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 00:16:3e:b3:06:c7 brd ff:ff:ff:ff:ff:ff
20: vethb3a2bfad@veth8d89fdf9: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP,M-DOWN&gt; mtu 1500 qdisc noqueue master lxdbr0 state LOWERLAYERDOWN mode DEFAULT group default qlen 1000
    link/ether da:c7:ab:6b:90:7f brd ff:ff:ff:ff:ff:ff
</code></pre><h3 id=lxc-开启hostnetwork>lxc 开启hostNetwork</h3><p>使用lxc创建一个容器:</p><pre><code>lxc-create -n test -t download -- --dist ubuntu --release focal --arch amd64
</code></pre><p>编辑 /var/lib/lxc/test/config. 删除所有关于network的行, 添加 &ldquo;lxc.network.type = none&rdquo;</p><p>启动容器:</p><pre><code>lxc-start -n test
</code></pre><p>检查lxc网络设置:</p><pre><code>lxc-ls -f


NAME STATE   AUTOSTART GROUPS IPV4                                                                                          IPV6                                                          UNPRIVILEGED 
test RUNNING 0         -      10.225.0.1, 10.33.34.1, 172.17.0.1, 172.23.8.165, 192.168.1.222, 192.168.122.1, 192.192.189.1 240e:3b5:cb5:abf0:36d4:30d3:285b:862e, fd42:1cac:64d0:f018::1 false   
</code></pre><p>进入到容器后检查网卡:</p><pre><code>
➜  ~ lxc-attach test  
# /bin/bash
root@test:~# ip addr
这里可以看到主机上所有的接口地址

</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2021/05/28/workingtipsonk3skubevirt/>WorkingTipsOnK3sKubevirt</a></h1><span class=post-date>May 28, 2021<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=1-目的>1. 目的</h3><p>k3s+kubevirt，运行虚拟机工作负载. 落地平台为AllInOne节点。主要针对边缘侧算力平台落地场景。</p><h3 id=2-环境>2. 环境</h3><p>嵌套虚拟化环境用于承载k3s算力管控平台。运行操作系统为Ubuntu20.04, 40Core, 274G内存。<br>更新： 嵌套虚拟化会产生诸多问题，导致qemu无法启动，因而后面我采用在物理机上直接启动k3s的方式。</p><p>物理机环境：</p><pre><code>Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz
376G memory
2T nvme ssd
CentOS 7.6.1810
kernel: 4.19.12-1.el7.elrepo.x86_64
</code></pre><h3 id=3-步骤>3. 步骤</h3><p>注：以下为虚拟化环境，未安装成功（因后续有嵌套虚拟化导致qemu无法启动的问题）<br>安装步骤:</p><pre><code># apt-get update -y &amp;&amp; apt-get upgrade -y
# export http_proxy export https_proxy
# curl -sfL https://get.k3s.io | sh -
# vim /etc/resolv.conf
nameserver 223.5.5.5
# systemctl stop systemd-resolved
# systemctl disable systemd-resolved
</code></pre><p>以下为正常过程:</p><p>安装k3s:</p><pre><code># yum update -y &amp;&amp; yum install -y git
# curl -sfL https://get.k3s.io | sh -
</code></pre><p>安装KubeVirt:</p><pre><code># export VERSION=v0.41.0
# kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/${VERSION}/kubevirt-operator.yaml
# kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/${VERSION}/kubevirt-cr.yaml
</code></pre><p>安装<code>virtctl</code>用于控制KubeVirt虚拟机，这里使用Kubernetes的插件管理器<code>Krew</code>来安装<code>virtctl</code>:</p><pre><code># (   set -x; cd &quot;$(mktemp -d)&quot; &amp;&amp;   curl -fsSLO &quot;https://github.com/kubernetes-sigs/krew/releases/latest/download/krew.tar.gz&quot; &amp;&amp;   tar zxvf krew.tar.gz &amp;&amp;   KREW=./krew-&quot;$(uname | tr '[:upper:]' '[:lower:]')_$(uname -m | sed -e 's/x86_64/amd64/' -e 's/arm.*$/arm/')&quot; &amp;&amp;   &quot;$KREW&quot; install krew; )

# export PATH=&quot;${KREW_ROOT:-$HOME/.krew}/bin:$PATH&quot;
# kubectl krew install virt
</code></pre><p>安装<code>Containerized-Data-Importer(CDI)</code>用于管理虚拟机的磁盘，安装步骤如下:</p><pre><code># export VERSION=$(curl -s https://github.com/kubevirt/containerized-data-importer/releases/latest | grep -o &quot;v[0-9]\.[0-9]*\.[0-9]*&quot;)
# kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$VERSION/cdi-operator.yaml
# kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$VERSION/cdi-cr.yaml
</code></pre><p>从<code>https://www.microsoft.com/en-us/evalcenter/evaluate-windows-server-2019</code> 下载Windows ISO 180天试用版, 而后上传该ISO:</p><pre><code># Get the CDI upload proxy service IP:
 kubectl get svc -n cdi
    
 # Upload 
 kubectl virt image-upload --image-path &lt;/path/to/iso&gt; \
     --pvc-name iso-win2k19 --access-mode ReadWriteOnce \
     --pvc-size 10G --uploadproxy-url &lt;upload-proxy service:443&gt; \
     --insecure --wait-secs=240
</code></pre><p>上传前需要确保<code>coredns</code>正常运行，否则会出现上传不成功的情况。</p><p>PVC被设置成<code>ReadWriteOnce</code>, 因为默认的local-path storageclass 不支持更多的模式。因为我们只使用一个节点，所以这点没所谓，但是在大型的K3s集群里，需要注意PVC的属性配置。</p><p><code>virtio-container-disk </code>容器镜像需要被实现拉回，因为在安装的时候我们需要使用其中包含的驱动程序。</p><pre><code># crictl pull kubevirt/virtio-container-disk
</code></pre><p>现在我们创建一个yaml文件用于定义需要创建的KubeVirt虚拟机(<code>win.yaml</code>):</p><pre><code>---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: winhd
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 15Gi
  storageClassName: manual
---
apiVersion: kubevirt.io/v1alpha3
kind: VirtualMachine
metadata:
  name: win2k19-iso
spec:
  running: false
  template:
    metadata:
      labels:
        kubevirt.io/domain: win2k19-iso
    spec:
      domain:
        cpu:
          cores: 4
        devices:
          disks:
          - bootOrder: 1
            cdrom:
              bus: sata
            name: cdromiso
          - disk:
              bus: sata
            name: harddrive
          - cdrom:
              bus: sata
            name: virtiocontainerdisk
        machine:
          type: q35
        resources:
          requests:
            memory: 8G
      volumes:
      - name: cdromiso
        persistentVolumeClaim:
          claimName: iso-win2k19
      - name: harddrive
        persistentVolumeClaim:
          claimName: winhd
      - containerDisk:
          image: kubevirt/virtio-container-disk
        name: virtiocontainerdisk
</code></pre><p>我们再创建一个PV用于承载该虚拟机的磁盘(<code>pv.yaml</code>):</p><pre><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: &quot;/media/sda/win&quot;
</code></pre><p>创建虚拟机:</p><pre><code> kubectl apply -f pv.yaml
 kubectl apply -f win.yaml
 kubectl virt start win2k19-iso
 # If you're running this on a remote machine, use X-forwarding and
 # apt-get install virt-viewer
 kubectl virt vnc win2k19-iso
</code></pre><p>值得注意的是，<code>win.yaml</code>中我们只能选择<code>sata</code>作为主磁盘的格式，<code>kubectl virt vnc</code>在我的机器上无法使用，所以我用了<code>kubectl virt --proxy-only=true win2k19-iso</code>用于获取一个动态端口，而后<code>vncviewer</code>到该动态端口上去。</p><p>完成安装后，进入<code>设备管理器</code>安装完未安装好的驱动程序。</p><p>使用NodePort暴露安装好以后的虚拟机的RDP端口:</p><pre><code>apiVersion: v1
kind: Service
metadata:
  name: windows-nodeport
spec:
  externalTrafficPolicy: Cluster
  ports:
  - name: nodeport
    nodePort: 30000
    port: 27017
    protocol: TCP
    targetPort: 3389
  selector:
    kubevirt.io/domain: win2k19-iso
  type: NodePort
</code></pre><p>创建完该服务后，则可通过<code>xxx.xxx.xxx.xxx:30000</code>用于访问该虚拟机的RDP远程桌面端口了。</p><p>创建完以后的虚拟机如图所示:</p><p><img src=/images/2021_05_31_21_30_56_746x693.jpg alt=/images/2021_05_31_21_30_56_746x693.jpg></p><p>如果是virtctl，则直接访问的方式是通过proxy:</p><pre><code>kubectl proxy --address=0.0.0.0 --accept-hosts='^*$' --port 8080
</code></pre><p>testvm:</p><pre><code>apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: testvm
spec:
  running: false
  template:
    metadata:
      labels:
        kubevirt.io/size: small
        kubevirt.io/domain: testvm
    spec:
      domain:
        devices:
          disks:
            - name: containerdisk
              disk:
                bus: virtio
            - name: cloudinitdisk
              disk:
                bus: virtio
          interfaces:
          - name: default
            bridge: {}
        resources:
          requests:
            memory: 64M
      networks:
      - name: default
        pod: {}
      volumes:
        - name: containerdisk
          containerDisk:
            image: quay.io/kubevirt/cirros-container-disk-demo
        - name: cloudinitdisk
          cloudInitNoCloud:
            userDataBase64: SGkuXG4=
</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2021/05/09/workingtipsonlxdccse/>WorkingTipsOnLXDccse</a></h1><span class=post-date>May 9, 2021<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=目标>目标</h3><p>在LXD上运行ccse</p><h3 id=准备>准备</h3><p>服务器上安装lxd, 初始化镜像centos7, ccse安装介质。</p><h3 id=步骤>步骤</h3><p>创建一个profile， 用于创建lxd用于部署验证:</p><pre><code>lxc profile show default&gt;ccse
vim ccse 
lxc profile create ccse
lxc profile edit ccse&lt;ccse
</code></pre><p>文件的内容如下：</p><pre><code>config:
  linux.kernel_modules: ip_tables,ip6_tables,netlink_diag,nf_nat,overlay,br_netfilter,xt_conntrack
  raw.lxc: &quot;lxc.apparmor.profile=unconfined\nlxc.cap.drop= \nlxc.cgroup.devices.allow=a\nlxc.mount.auto=p
roc:rw sys:rw&quot;
  security.nesting: &quot;true&quot;
  security.privileged: &quot;true&quot;
description: CCSE Running profile
devices:
  eth0:
    name: eth0
    network: lxdbr0
    type: nic
  hashsize:
    path: /sys/module/nf_conntrack/parameters/hashsize
    source: /dev/null
    type: disk
  kmsg:
    path: /dev/kmsg
    source: /dev/kmsg
    type: unix-char
  root:
    path: /
    pool: ssd
    type: disk
name: ccse
</code></pre><p>验证此profile是否可正常工作:</p><pre><code># lxc launch centos7 kkk --profile ccse
Creating kkk
Starting kkk                              
# lxc exec kkk bash
[root@kkk ~]# cat /etc/redhat-release 
CentOS Linux release 7.9.2009 (Core)
</code></pre><p>注意：</p><ol><li>版本略高于推荐的centos 7.6.</li><li>使用上述的权限文件加载，可以解决teledb组碰到的获取磁盘权限问题。</li></ol><h3 id=部署介质准备>部署介质准备</h3><p>初始化容器:</p><pre><code>cd /etc/yum.repos.d/
mkdir back
mv * back
vi ccse.repo
yum makecache
vi /etc/yum.conf 
yum install -y which vim net-tools lsof sudo
</code></pre><p>因为需要将lxd当成物理机来使用，安装openssh-server后重启：</p><pre><code> yum install -y openssh-server
 systemctl enable sshd
 systemctl start sshd
 passwd
 reboot
</code></pre><p>再次进入容器后，下载安装文件:</p><pre><code>scp docker@xxx.xxx.xxx.xx:/home/docker/shrink280/ccse-installer-2.8.0-rc-linux-amd64-offline-20210409204619-shrink.tar.xz .
tar xJf ccse-installer-2.8.0-rc-linux-amd64-offline-20210409204619-shrink.tar.xz
</code></pre><h3 id=部署console节点>部署console节点</h3><p>记录ip 地址<code> 10.222.125.68</code>， 配置完正确的IP地址后，按原有步骤安装console节点，安装完毕后上传镜像。</p><h3 id=制作基础节点>制作基础节点</h3><p>需打包节点所需要的依赖：</p><pre><code># lxc launch centos7 base
# lxc exec base bash
     yum install -y which lsof vim net-tools sudo selinux-policy libseccomp libselinux-python selinux-policy-targeted openssh-server ebtables ethtool
     systemctl enable sshd
     passwd
     shutdown -h now
#  lxc publish base --alias ccsenode

</code></pre><p>hashsize:</p><pre><code>sudo su
echo &quot;262144&quot; &gt; /sys/module/nf_conntrack/parameters/hashsize
cat /sys/module/nf_conntrack/parameters/hashsize
</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2021/05/08/workingtipsondnscryption/>WorkingTipsOndnscryption</a></h1><span class=post-date>May 8, 2021<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=working-environment>Working Environment</h3><p>Centos 7.9, vm , 8core, 16G.</p><h3 id=installation>Installation</h3><p>Install dnsmasq:</p><pre><code># sudo yum install -y dnsmasq
</code></pre><p>Install dnscrypt-proxy:</p><pre><code># sudo yum install -y dnscrypt-proxy
</code></pre><p>Wget the chinadns configuration file:</p><pre><code># wget https://raw.githubusercontent.com/felixonmars/dnsmasq-china-list/master/accelerated-domains.china.conf
# mv accelerated-domains.china.conf /etc/dnsmasq.d/accelerated-domains.china.conf
</code></pre><p>you can replace the <code>114.114.114.114</code> via your own dns(china intranet dns).</p><h3 id=configuration>Configuration</h3><p>Configure dnsmasq:</p><pre><code># vim /etc/dnsmasq.conf
listen-address=127.0.0.1
no-resolv
conf-dir=/usr/local/etc/dnsmasq.d
server=127.0.0.1#5300
interface=lo
bind-interfaces
</code></pre><p>Configure dnscrypt-proxy:</p><pre><code># vim /etc/dnscrypt-proxy/dnscrypt-proxy.toml
     # 监听5300端口
     listen_addresses = ['127.0.0.1:5300', '[::1]:5300']
     # 使用下面3个公开的DNS服务
     server_names = ['google', 'cloudflare', 'cloudflare-ipv6']
     # 如果找不到合适的公开DNS服务，则使用下面的DNS服务
     fallback_resolvers = ['9.9.9.9:53', '8.8.8.8:53']
     # 担心这些DNS请求被墙，设置使用代理发送DNS请求
     force_tcp = true
     proxy = 'socks5://127.0.0.1:1086'
</code></pre><p>Configure <code>/etc/resolv.conf</code> for using <code>127.0.0.1</code>:</p><pre><code>nameserver 127.0.0.1
</code></pre><h3 id=privoxy>privoxy</h3><p>In centos 7.9. don&rsquo;t install this package from epel, download the source code from internet and compile it:</p><pre><code>$ privoxy  --version
Privoxy version 3.0.28 (https://www.privoxy.org/)
</code></pre><p>make sure you have specify the gfwlist.</p></div><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/27/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/27/>27</a></li><li class="page-item active"><a class=page-link href=/page/28/>28</a></li><li class=page-item><a class=page-link href=/page/29/>29</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/231/>231</a></li><li class=page-item><a href=/page/29/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/231/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div><script src=http://purplepalmdash.github.io/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>