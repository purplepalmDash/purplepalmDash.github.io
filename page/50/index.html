<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta name=generator content="Hugo 0.64.0"><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Dash &#183; Dash</title><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124"><link rel=stylesheet href=https://purplepalmdash.github.io/css/highlight/googlecode.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js></script><script type=text/javascript src=/js/html2canvas.js></script><script type=text/javascript>function genPostShot(){var rightNow=new Date();var imageName=rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");imageName+='.jpg'
html2canvas(document.getElementsByClassName('post'),{background:'#FFFFFF',onrendered:function(canvas){$('#test').attr('href',canvas.toDataURL("image/jpeg"));$('#test').attr('download',imageName);$('#test')[0].click();}});};</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124"><link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel=icon><link href=%7balternate%20%7bRSS%20application/rss+xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://purplepalmdash.github.io/index.xml%7d rel=alternate type=application/rss+xml title="Dash &#183; Dash"><meta name=description content><meta name=keywords content="unix,virtualization,embedded,linux"></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><img src=http://purplepalmdash.github.io/images/mylogo.jpeg alt=gravatar><h1><a href=http://purplepalmdash.github.io/>很惭愧，就做了一点微小的工作</a></h1><a href=http://purplepalmdash.github.io/><p>Dash</p></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/post/>All Posts</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/technology/>Technology</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/life/>Life</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/linuxtips/>LinuxTips</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/purplepalmdash><i class="fa fa-github-square fa-3x"></i></a><a href=https://cn.linkedin.com/in/yang-feipeng-1b909319><i class="fa fa-linkedin-square fa-3x"></i></a><a href=https://plus.google.com/u/0/106572959364703833986><i class="fa fa-google-plus-square fa-3x"></i></a><a href=https://www.facebook.com/yang.feipeng><i class="fa fa-facebook-square fa-3x"></i></a><a href=https://twitter.com/dashwillfly><i class="fa fa-twitter-square fa-3x"></i></a></li></ul></div></div><div class="content container"><div class=posts><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/09/09/workingtipsonlxcko/>WorkingTipsOnLXCKO</a></h1><span class=post-date>Sep 9, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=environment>Environment</h3><p>Ubuntu 18.04.3 LTS, Kernel version:<br><code>Linux build 5.3.0-62-generic</code>.</p><p>vagrant box image: <code>centos76</code>.</p><p>lxc images:</p><pre><code># apt-get install -y kpartx
# cp ~/.vagrant.d/boxes/centos76/0/libvirt/box.img  /media/sdb/
# cd /media/sdb

root@build:/media/sdb# qemu-img convert box.img box1.img
root@build:/media/sdb# qemu-img info box.img
image: box.img
file format: qcow2
virtual size: 200G (214748364800 bytes)
disk size: 655M
cluster_size: 65536
Format specific information:
    compat: 1.1
    lazy refcounts: false
    refcount bits: 16
    corrupt: false
root@build:/media/sdb# qemu-img info box1.img
image: box1.img
file format: raw
virtual size: 200G (214748364800 bytes)
disk size: 1.3G

# kpartx -av box1.img 
add map loop2p1 (253:2): 0 419428352 linear 7:2 2048
# mount /dev/mapper/loop2p1 /mnt8/
# ls /mnt8/
bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
# tar -cvzf rootfs.tar.gz -C /mnt8 .
</code></pre><p>Create metadata and import lxc images:</p><pre><code># vim metadata.yaml
architecture: &quot;x86_64&quot;
creation_date: 1599622122 # To get current date in Unix time, use `date +%s` command
properties:
architecture: &quot;x86_64&quot;
description: &quot;CentOS 7.6 for lxc&quot;
os: &quot;redhat&quot;
release: &quot;7.6&quot;
# tar czvf metadata.tar.gz metadata.yaml
# lxc image import metadata.tar.gz rootfs.tar.gz --alias &quot;centos76&quot;
Image imported with fingerprint: 9f53f37e869c643049933dccf8cac9c76107856b1f66955cc2a9d3a55329a060
# lxc image ls
+----------+--------------+--------+-------------+--------+----------+-----------------------------+
|  ALIAS   | FINGERPRINT  | PUBLIC | DESCRIPTION |  ARCH  |   SIZE   |         UPLOAD DATE         |
+----------+--------------+--------+-------------+--------+----------+-----------------------------+
| centos76 | 9f53f37e869c | no     |             | x86_64 | 473.97MB | Sep 9, 2020 at 3:29am (UTC) |
+----------+--------------+--------+-------------+--------+----------+-----------------------------+
</code></pre><p>lxd init using following configuration:</p><pre><code>root@build:/media/sdb# lxd init
Would you like to use LXD clustering? (yes/no) [default=no]: 
Do you want to configure a new storage pool? (yes/no) [default=yes]: 
Name of the new storage pool [default=default]: 
Name of the storage backend to use (btrfs, dir, lvm) [default=btrfs]: 
Create a new BTRFS pool? (yes/no) [default=yes]: 
Would you like to use an existing block device? (yes/no) [default=no]: ^C
root@build:/media/sdb# lxd init
Would you like to use LXD clustering? (yes/no) [default=no]: 
Do you want to configure a new storage pool? (yes/no) [default=yes]: 
Name of the new storage pool [default=default]: 
Name of the storage backend to use (btrfs, dir, lvm) [default=btrfs]: dir
Would you like to connect to a MAAS server? (yes/no) [default=no]: 
Would you like to create a new local network bridge? (yes/no) [default=yes]: 
What should the new bridge be called? [default=lxdbr0]: 
What IPv4 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]: 
What IPv6 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]: none
Would you like LXD to be available over the network? (yes/no) [default=no]: 
Would you like stale cached images to be updated automatically? (yes/no) [default=yes] 
Would you like a YAML &quot;lxd init&quot; preseed to be printed? (yes/no) [default=no]: 
</code></pre><p>Create a bridge profile:</p><pre><code># lxc profile create bridge
root@build:/media/sdb# cat bridge.profile 
config:
  linux.kernel_modules: ip_tables,ip6_tables,netlink_diag,nf_nat,overlay,br_netfilter
  raw.lxc: &quot;lxc.apparmor.profile=unconfined\nlxc.cap.drop= \nlxc.cgroup.devices.allow=a\nlxc.mount.auto=proc:rw sys:rw&quot;
  security.nesting: &quot;true&quot;
  security.privileged: &quot;true&quot;
description: Bridge LXD profile
devices:
  eth0:
    name: eth0
    nictype: bridged
    parent: br0
    type: nic
  root:
    path: /
    pool: default
    type: disk
name: bridge
root@build:/media/sdb# lxc  profile edit bridge &lt; bridge.profile 
root@build:/media/sdb# lxc profile list
+---------+---------+
|  NAME   | USED BY |
+---------+---------+
| bridge  | 0       |
+---------+---------+
| default | 0       |
+---------+---------+
</code></pre><p>Create a instance:</p><pre><code># lxc launch centos76 ko1 --profile bridge
Creating ko1
# lxc ls
+------+---------+-----------------------+------+------------+-----------+
| NAME |  STATE  |         IPV4          | IPV6 |    TYPE    | SNAPSHOTS |
+------+---------+-----------------------+------+------------+-----------+
| ko1  | RUNNING | 10.137.149.190 (eth0) |      | PERSISTENT | 0         |
+------+---------+-----------------------+------+------------+-----------+
</code></pre><h3 id=how-ko-works>How KO Works</h3><p>Bug fix1: conf/my.cnf mapping.<br>Bug fix2: could not running on lxc&rsquo;s docker-compose.</p><h3 id=on-adding-ubuntu>On adding Ubuntu</h3><p>Via following commands, we could use lxc for ubuntu20.04.</p><pre><code># qemu-img convert box.img box1.img
# kpartx -av box1.img 
# lvscan
# mount /dev/vgubuntu/root /mnt7/
# tar -czvf rootfs.tar.gz -C /mnt7 .
# vim metadata.yaml
architecture: &quot;x86_64&quot;
creation_date: 1600908919 # To get current date in Unix time, use `date +%s` command
properties:
architecture: &quot;x86_64&quot;
description: &quot;ubuntu20.04 for lxc&quot;
os: &quot;ubuntu&quot;
release: &quot;20.04&quot;
# tar czvf metadata.tar.gz metadata.yaml
# lxc image import metadata.tar.gz  rootfs.tar.gz --alias &quot;ubuntu20.04&quot;
</code></pre><h3 id=on-adding-storage-in-cluster>On adding storage in cluster</h3><p>Via following commands:</p><pre><code>lxc cluster list
# for getting the member name. 

lxc storage create fastPool dir source=/media/md0/lxd --target arm-a1
lxc storage create fastPool dir source=/media/md0/lxd --target arm-a2
lxc storage create fastPool dir source=/media/md0/lxd --target arm-a3
lxc storage create fastPool dir
</code></pre><h3 id=x86-working-tips>x86 working tips</h3><p>working tips for creating lxd cluster:</p><pre><code>sudo apt install linux-image-extra-virtual linux-generic
cat /proc/sys/net/bridge/bridge-nf-call-iptables
1
lxc profile copy default k8s
lxc profile edit k8s 
config:
  environment.TZ: Asia/Bangkok
  raw.lxc: |
    lxc.apparmor.profile=unconfined
    lxc.cgroup.devices.allow = a
    lxc.mount.auto=proc:rw sys:rw
    lxc.cap.drop=
  security.nesting: &quot;true&quot;
  security.privileged: &quot;true&quot;
description: Default LXD profile
devices:
  apparmor:
    path: /sys/module/apparmor/parameters/enabled
    source: /dev/null
    type: disk
  eth0:
    name: eth0
    nictype: bridged
    parent: br10
    type: nic
  hashsize:
    path: /sys/module/nf_conntrack/parameters/hashsize
    source: /dev/null
    type: disk
  kmsg:
    path: /dev/kmsg
    source: /dev/kmsg
    type: unix-char
  root:
    path: /
    pool: default
    type: disk
name: k8s
used_by:
- /1.0/containers/k1
- /1.0/containers/k2
- /1.0/containers/k3
lxc launch ubuntu:18.04 k1 -p k8s
</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/09/07/workingtiponaarchftp/>Workingtiponaarchftp</a></h1><span class=post-date>Sep 7, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=ftpd服务端>ftpd服务端</h3><p>加载镜像:</p><pre><code># docker load&lt;ftpd.tar
Loaded image: gists/pure-ftpd:arm64
# docker images | grep ftpd
gists/pure-ftpd                                                            arm64               1b3e76d8756b        3 months ago        5.77MB
</code></pre><p>运行以下命令, 创建一个pure-ftpd实例, 当前目录下的ftpd含有ftpd的配置文件(pureftpd)及存储目录(data):</p><pre><code># mkdir ftpd
# cd ftpd
# mkdir pureftpd data
# docker run -d --restart unless-stopped --name pure-ftpd  -e MIN_PASV_PORT=40000 -e MAX_PASV_PORT=40009 -p 21:21  -p 40000-40009:40000-40009  -v $(pwd)/pureftpd:/etc/pureftpd  -v $(pwd)/data:/home/ftpuser gists/pure-ftpd:arm64
</code></pre><p>运行以下命令配置pure-ftpd的权限，以及添加test用户，并刷新pure-ftpd本地配置文件:</p><pre><code> docker exec -it pure-ftpd chown ftpuser:ftpuser -R /home/ftpuser
 docker exec -it pure-ftpd pure-pw useradd test -m -u ftpuser -d /home/ftpuser/test
 docker exec -it pure-ftpd pure-pw mkdb
</code></pre><h3 id=客户端>客户端</h3><p>举winscp ftp连接为例, 新建一个ftp连接:</p><p><img src=./images/2020_09_07_10_25_02_615x286.jpg alt=./images/2020_09_07_10_25_02_615x286.jpg></p><p>直接在winscp里拖拉实现上传下载:</p><p><img src=./images/2020_09_07_10_26_36_990x327.jpg alt=./images/2020_09_07_10_26_36_990x327.jpg></p><p>进度:</p><p><img src=./images/2020_09_07_10_29_46_649x286.jpg alt=./images/2020_09_07_10_29_46_649x286.jpg></p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/08/27/offlinearm64desktopenvsetup/>OfflineArm64DesktopEnvSetup</a></h1><span class=post-date>Aug 27, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=download-via-docker>Download via docker</h3><p>Run a docker instance via:</p><pre><code>$ sudo docker run -v /mnt:/mnt -it ubuntu:focal-20200115 /bin/bash
</code></pre><p>In docker instance, do following:</p><pre><code>rm -f /etc/apt/apt.conf.d/docker-clean
sed -i 's/ports.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list
cd /mnt
apt-get -d -o dir::cache=`pwd` -o Debug::NoLocking=1 install xubuntu-desktop xubuntu-core chromium-browser  firefox xrdp virt-manager ubuntu-wallpapers xubuntu-community-wallpapers xubuntu-community-wallpapers-focal xubuntu-wallpapers lxd lxc
apt-get install -y snapd
snap download lxd
snap download chromium

</code></pre><h3 id=transfer>Transfer</h3><p>Transfer these packages into the offline environments, and do following:</p><pre><code># cd ~/pkgs
# dpkg-scanpackages . /dev/null | gzip -9c &gt; Packages.gz
</code></pre><h3 id=install>Install</h3><p>In a ubuntu base environment, do following:</p><pre><code># vim /etc/apt/sources.list
deb [trusted=yes] file:///home/test/focal/ ./
# apt-get update -y
# apt-get install -y  xubuntu-desktop xubuntu-core  firefox xrdp virt-manager ubuntu-wallpapers xubuntu-community-wallpapers xubuntu-community-wallpapers-focal xubuntu-wallpapers
</code></pre><h3 id=configure-xrdp>Configure xrdp</h3><p>Configure xrdp via:</p><pre><code>$ sudo systemctl status xrdp
$ sudo adduser xrdp ssl-cert  
$ sudo systemctl restart xrdp
$ sudo ufw disable
$ echo xfce4-session &gt;~/.xsession
$ sudo vim /etc/xrdp/startwm.sh
#!/bin/sh

if [ -r /etc/default/locale ]; then
  . /etc/default/locale
  export LANG LANGUAGE
fi

startxfce4
$ sudo systemctl restart xrdp
</code></pre><p>So now you could use xfce4 as your remote desktop to linux.</p><h3 id=snapd-installation>snapd installation</h3><p>In docker run:</p><pre><code># sudo apt-get install -y snapd
# snap download lxd
# snap download chromium
# snap download gtk-common-themes
# snap download core
# snap download core18
</code></pre><p>Install sequence:</p><pre><code># snap install  (core/core18/gtk-common-themes/lxd/chromium)
chromium_1253.snap  core18_1888.snap  core_9806.snap                 gtk-common-themes_1506.snap  lxd_16946.snap
chromium_1253.assert  core18_1888.assert  core_9806.assert  gtk-common-themes_1506.assert  lxd_16946.assert   
</code></pre><h3 id=updated>Updated</h3><p>xrdp configuration:</p><pre><code>#!/bin/sh -e

# Install XRDP.
sudo apt install -y xrdp
sudo sed -e 's/^new_cursors=true/new_cursors=false/g' \
     -i /etc/xrdp/xrdp.ini
sudo systemctl enable xrdp
sudo systemctl restart xrdp

# Load Ubuntu config.
echo &quot;xfce4-session&quot; &gt; ~/.xsession
D=/usr/share/xfce4:/usr/share/xubuntu:/usr/local/share
D=${D}:/usr/share:/var/lib/snapd/desktop:/usr/share
cat &lt;&lt;EOF &gt; ~/.xsessionrc
export XDG_SESSION_DESKTOP=xubuntu
export XDG_DATA_DIRS=${D}
export XDG_CONFIG_DIRS=/etc/xdg/xdg-xubuntu:/etc/xdg:/etc/xdg
EOF

# Disable light-locker for avoiding error.
sudo cp /usr/bin/light-locker /usr/bin/light-locker.orig
cat &lt;&lt;EOF | sudo tee /usr/bin/light-locker
#!/bin/sh

# The light-locker uses XDG_SESSION_PATH provided by lightdm.
if [ ! -z &quot;\${XDG_SESSION_PATH}&quot; ]; then
  /usr/bin/light-locker.orig
else
  # Disable light-locker in XRDP.
  true
fi
EOF
sudo chmod a+x /usr/bin/light-locker
</code></pre><h3 id=final-script>Final script</h3><p>Final script is listed as following:</p><pre><code>#!/bin/bash
sudo cp -r focal /home/test/
echo 'deb [trusted=yes] file:///home/test/focal/ ./'|sudo tee -a /etc/apt/sources.list
sudo apt-get update -y
sudo apt-get install -y xubuntu-desktop xubuntu-core  firefox xrdp virt-manager ubuntu-wallpapers xubuntu-community-wallpapers xubuntu-community-wallpapers-focal xubuntu-wallpapers 
sudo adduser xrdp ssl-cert  
sudo systemctl restart xrdp
sudo ufw disable

sudo sed -e 's/^new_cursors=true/new_cursors=false/g' \
	     -i /etc/xrdp/xrdp.ini
sudo systemctl enable xrdp
sudo systemctl restart xrdp

# Load Ubuntu config.
echo &quot;xfce4-session&quot; &gt; ~/.xsession
D=/usr/share/xfce4:/usr/share/xubuntu:/usr/local/share
D=${D}:/usr/share:/var/lib/snapd/desktop:/usr/share
cat &lt;&lt;EOF &gt; ~/.xsessionrc
export XDG_SESSION_DESKTOP=xubuntu
export XDG_DATA_DIRS=${D}
export XDG_CONFIG_DIRS=/etc/xdg/xdg-xubuntu:/etc/xdg:/etc/xdg
EOF

# Disable light-locker for avoiding error.
sudo cp /usr/bin/light-locker /usr/bin/light-locker.orig
cat &lt;&lt;EOF | sudo tee /usr/bin/light-locker
#!/bin/sh

# The light-locker uses XDG_SESSION_PATH provided by lightdm.
if [ ! -z &quot;\${XDG_SESSION_PATH}&quot; ]; then
  /usr/bin/light-locker.orig
else
  # Disable light-locker in XRDP.
  true
fi
EOF
sudo chmod a+x /usr/bin/light-locker

# Install snapd
sudo snap ack snap/chromium_1253.assert
sudo snap ack snap/core18_1888.assert
sudo snap ack snap/core_9806.assert
sudo snap ack snap/gtk-common-themes_1506.assert
sudo snap ack snap/lxd_16946.assert

sudo snap install snap/core_9806.snap
sudo snap install snap/core18_1888.snap
sudo snap install snap/gtk-common-themes_1506.snap
sudo snap install snap/chromium_1253.snap
#sudo snap install snap/lxd_16946.snap

</code></pre><p>New user will be added if you want to create new session.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/07/29/bbbhidworkingtips/>BBBHIDWorkingTips</a></h1><span class=post-date>Jul 29, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=preparation>Preparation</h3><p>The system is:</p><pre><code>debian@beaglebone:~$ cat /etc/issue
Debian GNU/Linux 9 \n \l
BeagleBoard.org Debian Stretch imgtec Image 2020-04-06
Support: http://elinux.org/Beagleboard:BeagleBoneBlack_Debian
default username:password is [debian:temppwd]
debian@beaglebone:~$ uname -a
Linux beaglebone 4.14.108-ti-r131 #1stretch SMP PREEMPT Tue Mar 24 19:18:37 UTC 2020 armv7l GNU/Linux
</code></pre><p>Change password via <code>passwd</code>.<br>Enlarge the partition from 4GB to 32GB(32 GB disk)</p><pre><code>root@beaglebone:/home/debian# /opt/scripts/tools/grow_partition.sh 
# vim /etc/network/interfaces
auto eth0
iface eth0 inet dhcp
# reboot
##### Checking #############
debian@beaglebone:~$ df -h
Filesystem      Size  Used Avail Use% Mounted on
udev            217M     0  217M   0% /dev
tmpfs            49M  5.2M   44M  11% /run
/dev/mmcblk0p1   30G  2.5G   26G   9% /
</code></pre><p>Edit the sources.list configuration:</p><pre><code>$ cat /etc/apt/sources.list
deb http://mirrors.ustc.edu.cn/debian stretch main contrib non-free
#deb-src http://mirrors.ustc.edu.cn/debian stretch main contrib non-free

deb http://mirrors.ustc.edu.cn/debian stretch-updates main contrib non-free
#deb-src http://mirrors.ustc.edu.cn/debian stretch-updates main contrib non-free

deb http://mirrors.ustc.edu.cn/debian-security stretch/updates main contrib non-free
#deb-src http://mirrors.ustc.edu.cn/debian-security stretch/updates main contrib non-free
#  apt-get update -y 
# apt-get install -y libudev-dev libusb-dev awesome iotop
</code></pre><h3 id=usb-hid>USB HID</h3></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/07/27/quicktipsonterraformandlibvirtdonmultiple2/>QuickTipsOnTerraformAndLibvirtdOnMultiple2</a></h1><span class=post-date>Jul 27, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>紧接上一篇，这一篇引入<code>Cluster Mesh</code>用于暴露全局服务。</p><h3 id=cluster-mesh>Cluster Mesh</h3><p>因为我们上一节搭建的集群中已经设定了<code>cluster-name</code>及<code>cluster-id</code>， 故不需要对其进行更改，如果没有设置，则需要手动更改:</p><pre><code># kubectl -n kube-system edit cm cilium-config
[ ... add/edit ... ]
cluster-name: cluster1
cluster-id: &quot;1&quot;
</code></pre><p>两个集群上分别创建etcd服务的NodePort服务暴露:</p><pre><code>root@mouse-1:/mnt/Rong_cilium# kubectl apply -f etcdNodePort.yaml  -n kube-system
cservice/cilium-etcd-external created
root@mouse-1:/mnt/Rong_cilium# cat etcdNodePort.yaml 
apiVersion: v1
kind: Service
metadata:
  name: cilium-etcd-external
spec:
  type: NodePort
  ports:
  - port: 2379
  selector:
    app: etcd
    etcd_cluster: cilium-etcd
    io.cilium/app: etcd-operator
</code></pre><p>克隆<code> cilium/clustermesh-tools</code> 仓库，它含有用于取出密码及生成Kubernetes secrets配置的脚本文件，</p><pre><code># git clone https://github.com/cilium/clustermesh-tools.git
# cd clustermesh-tools
# cd clustermesh-tools/
# ls
clustermesh.yaml  config  ds.patch  extract-etcd-secrets.sh  generate-name-mapping.sh  generate-secret-yaml.sh
# rm -rf config/
# rm -f clustermesh.yaml 
# rm -f ds.patch 
</code></pre><p>生成<code>etcd-secrets</code>:</p><pre><code>root@mouse-1:/mnt/Rong_cilium/clustermesh-tools# ./extract-etcd-secrets.sh 
Derived cluster-name c1 from present ConfigMap
====================================================
 WARNING: The directory config contains private keys.
          Delete after use.
====================================================
</code></pre><p>在另一集群上重复以上操作。</p><p>从上面解压出来的keys/certificates生成单个Kubernetes secret， 这个secret将含有service IP/etcd节点名等用于访问的键/值。</p><pre><code># ./generate-secret-yaml.sh &gt; clustermesh.yaml
</code></pre><p>两个节点上分别运行以下命令，用于生成用于插入到<code>cilium</code> daemonset中的所需字段：</p><pre><code># # ./generate-name-mapping.sh &gt; ds.patch
root@cilium-1:/mnt/Rong_cilium_2/clustermesh-tools# ./generate-name-mapping.sh &gt; ds.patch
root@cilium-1:/mnt/Rong_cilium_2/clustermesh-tools# cat ds.patch 
spec:
  template:
    spec:
      hostAliases:
      - ip: &quot;10.137.149.72&quot;
        hostnames:
        - c2.mesh.cilium.io
      - ip: &quot;10.137.149.73&quot;
        hostnames:
        - c2.mesh.cilium.io
root@mouse-1:/mnt/Rong_cilium/clustermesh-tools# ./generate-name-mapping.sh &gt; ds.patch
root@mouse-1:/mnt/Rong_cilium/clustermesh-tools# cat ds.patch 
spec:
  template:
    spec:
      hostAliases:
      - ip: &quot;10.137.149.61&quot;
        hostnames:
        - c1.mesh.cilium.io
      - ip: &quot;10.137.149.62&quot;
        hostnames:
        - c1.mesh.cilium.io
</code></pre><p>组合出一个新的文件:</p><pre><code># cat combine_ds.patch 
spec:
  template:
    spec:
      hostAliases:
      - ip: &quot;10.137.149.61&quot;
        hostnames:
        - c1.mesh.cilium.io
      - ip: &quot;10.137.149.62&quot;
        hostnames:
        - c1.mesh.cilium.io
      - ip: &quot;10.137.149.72&quot;
        hostnames:
        - c2.mesh.cilium.io
      - ip: &quot;10.137.149.73&quot;
        hostnames:
        - c2.mesh.cilium.io
</code></pre><p>两个集群上分别apply：</p><pre><code># kubectl -n kube-system patch ds cilium -p &quot;$(cat combine_ds.patch)&quot;
daemonset.apps/cilium patched
</code></pre><p>两个集群上分别apply <code>cluster-mesh</code>:</p><pre><code>root@mouse-1:/mnt/Rong_cilium/clustermesh-tools# kubectl -n kube-system apply -f clustermesh.yaml
secret/cilium-clustermesh created
root@mouse-1:/mnt/Rong_cilium/clustermesh-tools# kubectl -n kube-system apply -f /mnt/Rong_cilium_2/clustermesh-tools/clustermesh.yaml 

root@cilium-1:/mnt/Rong_cilium_2/clustermesh-tools# kubectl -n kube-system apply -f clustermesh.yaml
secret/cilium-clustermesh created
root@cilium-1:/mnt/Rong_cilium_2/clustermesh-tools# kubectl -n kube-system apply -f /mnt/Rong_cilium/clustermesh-tools/clustermesh.yaml 
secret/cilium-clustermesh configured
</code></pre><p>重新启动所有节点上的<code>cilium-agent</code>，以便使用新的<code>cilium-clustermesh</code>密码文件中标识的集群名称、cluster id等，</p><pre><code>root@cilium-1:/mnt/Rong_cilium_2/clustermesh-tools# kubectl -n kube-system delete pod -l k8s-app=cilium
pod &quot;cilium-pb7rd&quot; deleted
pod &quot;cilium-wqdrv&quot; deleted
root@cilium-1:/mnt/Rong_cilium_2/clustermesh-tools# kubectl -n kube-system delete pod -l name=cilium-operator
pod &quot;cilium-operator-7cd598bdf6-42lwl&quot; deleted
root@mouse-1:/mnt/Rong_cilium/clustermesh-tools# kubectl -n kube-system delete pod -l k8s-app=cilium
pod &quot;cilium-5k5rj&quot; deleted
pod &quot;cilium-7m4gv&quot; deleted
root@mouse-1:/mnt/Rong_cilium/clustermesh-tools# kubectl -n kube-system delete pod -l name=cilium-operator
pod &quot;cilium-operator-7cd598bdf6-5s4q9&quot; deleted
</code></pre><p>测试集群pod的连接性:</p><pre><code>root@cilium-1:/home/test# kubectl exec -ti cilium-sqjm9 -n kube-system cilium node list
Name          IPv4 Address    Endpoint CIDR    IPv6 Address   Endpoint CIDR
c1/mouse-1    10.137.149.61   10.233.64.0/24                  
c1/mouse-2    10.137.149.62   10.233.65.0/24                  
c2/cilium-1   10.137.149.72   10.234.65.0/24                  
c2/cilium-2   10.137.149.73   10.234.64.0/24  
</code></pre><pre><code># kubectl run foo1 -it --rm --image  byrnedo/alpine-curl:latest  --command -- sh -c &quot;curl nginx&quot;
kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
# root@mouse-1:/home/test# kubectl logs foo1-78cf747c46-xmk9f
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre><p>如何证明其互通性？可以看到我们创建的curl pod是位于集群1中的，而所有的nginx负载则来自于受控集群(集群2）:</p><pre><code>root@mouse-1:/home/test# kubectl get pods -o wide
NAME                     READY   STATUS             RESTARTS   AGE    IP              NODE           NOMINATED NODE   READINESS GATES
foo-76db7d689b-vzlkg     0/1     CrashLoopBackOff   3          95s    10.233.64.84    mouse-1        &lt;none&gt;           &lt;none&gt;
foo1-78cf747c46-xmk9f    0/1     CrashLoopBackOff   3          118s   10.233.64.31    mouse-1        &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-8cw76   1/1     Running            0          50m    10.234.64.236   admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-c6rkm   1/1     Running            0          50m    10.234.65.98    admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-l2jmj   1/1     Running            0          49m    10.234.64.116   admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-r4p6p   1/1     Running            0          50m    10.234.65.54    admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-sc2jh   1/1     Running            0          49m    10.234.65.139   admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-t5vtx   1/1     Running            0          49m    10.234.64.177   admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-v4lz4   1/1     Running            0          50m    10.234.64.159   admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-wj4xf   1/1     Running            0          50m    10.234.65.82    admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-x4tpv   1/1     Running            0          49m    10.234.65.69    admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-xzfmx   1/1     Running            0          49m    10.234.64.18    admiralty-c2   &lt;none&gt;           &lt;none&gt;
</code></pre></div><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/49/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/49/>49</a></li><li class="page-item active"><a class=page-link href=/page/50/>50</a></li><li class=page-item><a class=page-link href=/page/51/>51</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/244/>244</a></li><li class=page-item><a href=/page/51/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/244/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div><script src=http://purplepalmdash.github.io/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>