<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta name=generator content="Hugo 0.64.0"><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Dash &#183; Dash</title><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124"><link rel=stylesheet href=https://purplepalmdash.github.io/css/highlight/googlecode.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js></script><script type=text/javascript src=/js/html2canvas.js></script><script type=text/javascript>function genPostShot(){var rightNow=new Date();var imageName=rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");imageName+='.jpg'
html2canvas(document.getElementsByClassName('post'),{background:'#FFFFFF',onrendered:function(canvas){$('#test').attr('href',canvas.toDataURL("image/jpeg"));$('#test').attr('download',imageName);$('#test')[0].click();}});};</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124"><link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel=icon><link href=%7balternate%20%7bRSS%20application/rss+xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://purplepalmdash.github.io/index.xml%7d rel=alternate type=application/rss+xml title="Dash &#183; Dash"><meta name=description content><meta name=keywords content="unix,virtualization,embedded,linux"></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><img src=http://purplepalmdash.github.io/images/mylogo.jpeg alt=gravatar><h1><a href=http://purplepalmdash.github.io/>很惭愧，就做了一点微小的工作</a></h1><a href=http://purplepalmdash.github.io/><p>Dash</p></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/post/>All Posts</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/technology/>Technology</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/life/>Life</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/linuxtips/>LinuxTips</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/purplepalmdash><i class="fa fa-github-square fa-3x"></i></a><a href=https://cn.linkedin.com/in/yang-feipeng-1b909319><i class="fa fa-linkedin-square fa-3x"></i></a><a href=https://plus.google.com/u/0/106572959364703833986><i class="fa fa-google-plus-square fa-3x"></i></a><a href=https://www.facebook.com/yang.feipeng><i class="fa fa-facebook-square fa-3x"></i></a><a href=https://twitter.com/dashwillfly><i class="fa fa-twitter-square fa-3x"></i></a></li></ul></div></div><div class="content container"><div class=posts><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/03/08/bootupsequence/>BootUpSequence</a></h1><span class=post-date>Mar 8, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>服务器配置:</p><p><img src=/images/2019_03_08_10_46_27_657x334.jpg alt=/images/2019_03_08_10_46_27_657x334.jpg></p><p>按del键进入BIOS，更改启动顺序:</p><p><img src=/images/2019_03_08_10_46_52_404x161.jpg alt=/images/2019_03_08_10_46_52_404x161.jpg></p><p>插入ISO盘:</p><p><img src=/images/2019_03_08_10_47_11_600x196.jpg alt=/images/2019_03_08_10_47_11_600x196.jpg></p><p>选择English，进入安装界面:</p><p><img src=/images/2019_03_08_10_47_37_437x464.jpg alt=/images/2019_03_08_10_47_37_437x464.jpg></p><p>手动分区:</p><p><img src=/images/2019_03_08_10_48_04_651x401.jpg alt=/images/2019_03_08_10_48_04_651x401.jpg></p><p>原有分区, sda:</p><p><img src=/images/2019_03_08_10_48_24_498x336.jpg alt=/images/2019_03_08_10_48_24_498x336.jpg></p><p>删除后创建逻辑卷:</p><p><img src=/images/2019_03_08_10_48_52_527x249.jpg alt=/images/2019_03_08_10_48_52_527x249.jpg></p><p><img src=/images/2019_03_08_10_48_04_651x401.jpg alt=/images/2019_03_08_10_48_04_651x401.jpg></p><p>原有分区, sda:</p><p><img src=/images/2019_03_08_10_48_24_498x336.jpg alt=/images/2019_03_08_10_48_24_498x336.jpg></p><p>删除后创建逻辑卷:</p><p><img src=/images/2019_03_08_10_48_52_527x249.jpg alt=/images/2019_03_08_10_48_52_527x249.jpg></p><p>挂载到根分区:</p><p><img src=/images/2019_03_08_10_49_53_625x256.jpg alt=/images/2019_03_08_10_49_53_625x256.jpg></p><p>磁盘布局最后检查：</p><p><img src=/images/2019_03_08_10_50_08_528x210.jpg alt=/images/2019_03_08_10_50_08_528x210.jpg></p><p>Install continue。。。。</p><p>Grub配置，手动选择/dev/sda:</p><p><img src=/images/2019_03_08_10_50_41_565x297.jpg alt=/images/2019_03_08_10_50_41_565x297.jpg></p><p>重新启动，弹出光盘：</p><p>![/images/2019_03_08_10_51_09_450x162.jpg](/images/2019_03_08_10_51_09
Install continue。。。。</p><p>Grub配置，手动选择/dev/sda:</p><p><img src=/images/2019_03_08_10_50_41_565x297.jpg alt=/images/2019_03_08_10_50_41_565x297.jpg></p><p>重新启动，弹出光盘：</p><p><img src=/images/2019_03_08_10_51_09_450x162.jpg alt=/images/2019_03_08_10_51_09_450x162.jpg></p><p>System boot:</p><p><img src=/images/2019_03_08_10_51_36_583x410.jpg alt=/images/2019_03_08_10_51_36_583x410.jpg></p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/02/20/buildingpwkcd/>BuildingPWKCD</a></h1><span class=post-date>Feb 20, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=makeiso-server>MakeISO Server</h3><p>Configure via:</p><pre><code># apt-get update -y 
# apt-get install -y vim openssh-server
</code></pre><p>Install cubic via:</p><pre><code># apt-add-repository ppa:cubic-wizard/release
# apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 6494C6D6997C215E
# apt-get update -y &amp;&amp; apt-get install -y cubic
</code></pre><h3 id=cubic-make-iso>cubic make iso</h3><p>Start cubic via:</p><p><img src=/images/2019_02_20_16_20_41_429x333.jpg alt=/images/2019_02_20_16_20_41_429x333.jpg></p><p>Create the iso project folder:</p><pre><code># mkdir ~/isoproject
</code></pre><p>Slect the original disk image to customize:</p><p><img src=/images/2019_02_20_16_28_24_516x413.jpg alt=/images/2019_02_20_16_28_24_516x413.jpg></p><p>Cubic will copy the content from the origin folder to remote folder, this will
takes for some time:</p><p><img src=/images/2019_02_20_16_29_11_495x357.jpg alt=/images/2019_02_20_16_29_11_495x357.jpg></p><p>In chroot terminal you could custom the cd:</p><p><img src=/images/2019_02_20_16_32_14_516x207.jpg alt=/images/2019_02_20_16_32_14_516x207.jpg></p><h3 id=cd-customize>CD customize</h3><p>Install docker/docker-compose</p><pre><code># vim /etc/apt/sources.list(Changes to 163.com)
# apt-get install -y python-pip &amp;&amp; pip install docker-compose
# ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
# apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg-agent \
    software-properties-common

# curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

# apt-key fingerprint 0EBFCD88
# add-apt-repository \
   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable&quot;
# apt-get install docker-ce docker-ce-cli containerd.io
# docker version
</code></pre><p>The current Version is :</p><p><img src=/images/2019_02_20_16_52_47_505x133.jpg alt=/images/2019_02_20_16_52_47_505x133.jpg></p><p>Cause we are under chroot, we don&rsquo;t have server running.</p><p>We fetch the pwk ready machine&rsquo;s <code>/var/lib/docker</code>, transfer them into our
chroot env:</p><p><img src=/images/2019_02_20_17_06_27_549x122.jpg alt=/images/2019_02_20_17_06_27_549x122.jpg></p><p>We also need golang for running the pwk environment:</p><pre><code># apt-get install -y golang
# vim /root/.bashrc

</code></pre><p>Transfer the golang environment from the pwk ready machine:</p><pre><code># ls /root
 go/ Code/
</code></pre><p>systemd file:</p><pre><code>root@test-Standard-PC-Q35-ICH9-2009:/etc/systemd/system# cat mynginx.service 
[Unit]
Description=mynginx
Requires=docker.service
After=docker.service

[Service]
Restart=always
ExecStart=/usr/bin/docker start -a docker-nginx
ExecStop=/usr/bin/docker stop -t 2 docker-nginx

[Install]
WantedBy=multi-user.target

root@test-Standard-PC-Q35-ICH9-2009:/etc/systemd/system# cat playwithdockerblog.service 
[Unit]
Description=playwithdockerblog
After=docker.service
Requires=docker.service

[Service]
ExecStart=/usr/bin/docker-compose -f /root/Code/play-with-kubernetes.github.io/docker-compose.yml up -d

[Install]
WantedBy=multi-user.target
root@test-Standard-PC-Q35-ICH9-2009:/etc/systemd/system# cat playwithdocker.service 
[Unit]
Description=playwithdocker
After=docker.service
Requires=docker.service

[Service]
Environment=GOPATH=/root/go/
WorkingDirectory=/root/go/src/github.com/play-with-docker/play-with-docker
Type=idle
# Remove old container items
ExecStartPre=/usr/bin/docker-compose -f /root/go/src/github.com/play-with-docker/play-with-docker/docker-compose.yml down
# Compose up
ExecStart=/usr/bin/docker-compose -f /root/go/src/github.com/play-with-docker/play-with-docker/docker-compose.yml up -d

[Install]
WantedBy=multi-user.target
</code></pre><p>ToBeContinue, later I will change the static website content, and k8s image
offline, then do the iso-build.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/02/19/notesonplaywithk8s/>NotesOnPlayWithK8s</a></h1><span class=post-date>Feb 19, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>早先在2018年做了一个离线版本的playwithk8s, 主要参考了:</p><p><a href=https://labs.play-with-k8s.com/>https://labs.play-with-k8s.com/</a></p><p>以及</p><p><a href=https://training.play-with-kubernetes.com/kubernetes-workshop/>https://training.play-with-kubernetes.com/kubernetes-workshop/</a></p><p>当时还写了一系列教程并形成了一个离线部署的ISO。那个ISO前几天有同事用，反映跑不起来。看了下问题，总结如下.</p><h3 id=dns问题>dns问题</h3><p>安装完的系统中，dnsmasq不能使用，需要通过以下步骤来修正:</p><pre><code># vim /etc/systemd/resolved.conf
DNSStubListener=no
# systemctl disable systemd-resolved.service
# systemctl stop systemd-resolved.service
# echo nameserver 192.168.0.15&gt;/etc/resolv.conf
# apt-get install -y dnsmasq
# systemctl enable dnsmasq
# vim /etc/dnsmasq.conf
address=/192.168.122.151/192.168.122.151
address=/localhost/127.0.0.1
# chattr +i /etc/resolv.conf
# chattr -e /etc/resolv.conf
# ufw disable
# docker swarm leave
# docker swarm init
</code></pre><p>这样下来可以访问到界面，</p><p><img src=/images/2019_02_19_15_05_45_1065x630.jpg alt=/images/2019_02_19_15_05_45_1065x630.jpg></p><h3 id=kubeadm卡顿>kubeadm卡顿</h3><p>卡在init的时候，现象见上图。</p><p>问题分析，在docker版本为17.12.1-ce的系统上可正常运行。</p><p>ISO安装出来的docker版本为18.06.0-ce.</p><p>是否是docker版本与kubeadm不兼容?</p><p>kubeadm生成的文件(/etc/kubernetes):</p><p><img src=/images/2019_02_19_15_08_49_565x229.jpg alt=/images/2019_02_19_15_08_49_565x229.jpg></p><p>对比kube-apiserver.yaml, 发现imagePullPolicy的不同:</p><p><img src=/images/2019_02_19_15_10_09_1106x395.jpg alt=/images/2019_02_19_15_10_09_1106x395.jpg></p><p>这点也是很让人奇怪的，为什么同样的容器镜像版本,
franela/k8s:latest会有如此的不同呢？
一样的容器镜像派生出来的实例，应该说其内置的kubeadm生成的yaml编排文件是一样的，但是在我们的系统上，更新版本的docker下生成的yaml文件未带IfNotPresent的选项，导致kubeadm认为镜像不存在，报了超时错误。</p><h3 id=解决途径>解决途径</h3><p>最近没有时间来做这个事情，所以只能先写下自己的思路。</p><ol><li><p>用一个registry缓存所有的包(去掉docker
load的环节），直接从cache里取回gcr.io的包。docker在启动的时候从cache里取东西回来。但是我不是很确定gcr.io是否可以像registry-1.docker.io一样被缓存下来。</p></li><li></li></ol><p>伪造gcr.io的签名，指向内部的registry仓库。这个可以仿照我的kubespray框架中的做法，骗过dind。</p><p>离线情况下玩docker，真是没事找事，一波三折啊！！！</p><h3 id=更新>更新</h3><p>最近没心思做别的事情，还是把这个PWK的离线给做了，记录一下步骤：</p><p>更新到新的play-with-kubernetes:</p><pre><code># git clone https://github.com/play-with-docker/play-with-kubernetes.github.io
</code></pre><p>当然在这里我们要做适配，以允许其离线化 。</p><p>采用的franela/k8s版本大约是2018年年底的版本，</p><pre><code># franela/k8s              latest              c7038cbdbc5d        2 months ago        733MB
</code></pre><p>因为这个容器镜像中的kubeadm版本已经升级到比较新的版本，需要重新下载镜像:</p><pre><code>k8s.gcr.io/kube-proxy-amd64                v1.11.7             e9a1134ab5aa        4 weeks ago         98.1MB
k8s.gcr.io/kube-apiserver-amd64            v1.11.7             d82b2643a56a        4 weeks ago         187MB
k8s.gcr.io/kube-controller-manager-amd64   v1.11.7             93fb4304c50c        4 weeks ago         155MB
k8s.gcr.io/kube-scheduler-amd64            v1.11.7             52ea1e0a3e60        4 weeks ago         56.9MB
weaveworks/weave-npc                       2.5.1               789b7f496034        4 weeks ago         49.6MB
weaveworks/weave-kube                      2.5.1               1f394ae9e226        4 weeks ago         148MB
k8s.gcr.io/coredns                         1.1.3               b3b94275d97c        9 months ago        45.6MB
k8s.gcr.io/etcd-amd64                      3.2.18              b8df3b177be2        10 months ago       219MB
k8s.gcr.io/pause                           3.1                 da86e6ba6ca1        14 months ago       742kB
nginx                                      latest              05a60462f8ba        2 years ago         181MB
</code></pre><p>同时需要更改kubeadm init的参数为:</p><pre><code># kubeadm init --apiserver-advertise-address $(hostname -i) --kubernetes-version=v1.11.7
</code></pre><p>在多节点章节中，kubeaadm
1.11.7与原先的calico3.1冲突，因而我们更新到了更新的3.5版本,
因为docker-in-docker配备了两个网络，我们在calico.yaml中也需要指定IP范围，以确保BGP隧道建立在正确的网络接口上:</p><pre><code>            - name: CALICO_IPV4POOL_CIDR
              value: &quot;192.168.0.0/16&quot;
            - name: IP_AUTODETECTION_METHOD
              value: &quot;interface=eth0.*&quot;

</code></pre><p>到此，则新版本的playwithk8s更新完毕，总共花了5天时间。虽然有点磨人，但想起来这5天还是值得的。</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/02/19/troubleshootingonvmwarebasedvm/>TroubleShootingOnVMWareBasedVM</a></h1><span class=post-date>Feb 19, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>记录一下昨天遇到的一个问题：<br>同事的虚拟机(Ubuntu)自检失败，fsck总是不过，直接抛到initramfs的界面。</p><p>试图探索:<br>启动的时候按Shift键，到grub界面，以recovery模式启动，依然失败。看来是ext4的磁盘inode有错误。</p><p>解决方案：<br>用livecd启动系统，启动以后到系统里去fsck修复该文件系统。<br>gparted启动失败。<br>后面我们用的是一个CentOS7的livecd系统进入的系统,</p><pre><code># fsck -y /dev/mapper/lv_lv_root
</code></pre><p>扫描并自动修复(yes)磁盘上的inode错误。修复以后，用硬盘启动系统，可正常进入系统。避免了重新安装操作系统的开销。</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/01/31/kubesprayto282/>kubesprayto282</a></h1><span class=post-date>Jan 31, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=aim>AIM</h3><p>Since kubespray has released v2.8.2, I write this article for recording steps
of upgrding from v2.8.1 to v2.8.2 of my offline solution.</p><h3 id=prerequisites>Prerequisites</h3><p>Download the v2.8.2 release from:</p><p><a href=https://github.com/kubernetes-sigs/kubespray/releases>https://github.com/kubernetes-sigs/kubespray/releases</a></p><pre><code># wget https://github.com/kubernetes-sigs/kubespray/archive/v2.8.2.tar.gz
# tar xzvf v2.8.2.tar.gz
# cd kubespray-2.8.2
</code></pre><h3 id=modification>Modification</h3><h4 id=1-clusteryml>1. cluster.yml</h4><p>three changes made.</p><p>a. added <code>kube-deploy</code> role.<br>b. disable instal container-engine(cause we will directly install docker).<br>c. create role for k8s dashboard.</p><pre><code>    - { role: bastion-ssh-config, tags: [&quot;localhost&quot;, &quot;bastion&quot;]}

# Notice 1, we combine deploy.yml here. 
- hosts: kube-deploy
  gather_facts: false
  roles:
    - kube-deploy
# Notice 1 ends here. 

- hosts: k8s-cluster:etcd:calico-rr:!kube-deploy
  any_errors_fatal: &quot;{{ any_errors_fatal | default(true) }}&quot;

.......

    - { role: kubernetes/preinstall, tags: preinstall }
    #- { role: &quot;container-engine&quot;, tags: &quot;container-engine&quot;, when: deploy_container_engine|default(true) }
    - { role: download, tags: download, when: &quot;not skip_downloads&quot; }

.......


    - { role: kubernetes-apps, tags: apps }
- hosts: kube-master[0]
  tasks:
    - name: &quot;Create clusterrolebinding&quot;
      shell: kubectl create clusterrolebinding cluster-admin-fordashboard --clusterrole=cluster-admin --user=system:serviceaccount:kube-system:kubernetes-dashboard
</code></pre><h4 id=2-vagrantfile>2. Vagrantfile</h4><p>10 Changes made.</p><p>a. Added customized vagrant box(vagrant-libvirt).<br>b. Added kube-deploy instance, which will automatically added into vagrantfile.<br>c. Added disks, we will use glusterfs.<br>d. We specify the system, and enable calico instead of flannel.<br>e. Added dns.sh for customized the offline environment dns.<br>f. Added vagrant-libvirt items.<br>g. Disable sync folder.<br>h. Call dns.sh when every vm startup.<br>i. Disable some download items, and specify the ansible python interpreter.<br>j. Let ansible become root, and disable ask-pass prompt, also add kube-deploy into vagrant generated inventory file.</p><pre><code>  &quot;opensuse-tumbleweed&quot; =&gt; {box: &quot;opensuse/openSUSE-Tumbleweed-x86_64&quot;, user: &quot;vagrant&quot;},
  &quot;wukong&quot;              =&gt; {box: &quot;kubespray&quot;, user: &quot;vagrant&quot;},
  &quot;wukong1804&quot;          =&gt; {box: &quot;rong1804&quot;, user: &quot;vagrant&quot;},
}
......
$kube_master_instances = $num_instances == 1 ? $num_instances : ($num_instances - 1)
$kube_deploy_instances = 1
# All nodes are kube nodes
......
$kube_node_instances_with_disks = true
$kube_node_instances_with_disks_size = &quot;40G&quot;

......
$os = &quot;wukong&quot;
$network_plugin = &quot;calico&quot;
......
    FileUtils.ln_s($inventory, File.join($vagrant_ansible,&quot;inventory&quot;))
  end
end
File.open('./dns.sh' ,'w') do |f|
  f.write &quot;#!/bin/bash\n&quot;
  f.write &quot;sed -i '/^#VAGRANT-END/i dns-nameservers 10.148.129.101' /etc/network/interfaces\n&quot;
  f.write &quot;systemctl restart networking.service\n&quot;
  #f.write &quot;rm -f /etc/resolv.conf\n&quot;
  #f.write &quot;ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf\n&quot;
  #f.write &quot;echo '      nameservers:'&gt;&gt;/etc/netplan/50-vagrant.yaml\n&quot;
  #f.write &quot;echo '        addresses: [10.148.129.101]'&gt;&gt;/etc/netplan/50-vagrant.yaml\n&quot;
  #f.write &quot;netplan apply eth1\n&quot;
end

......

        lv.default_prefix = 'rong1804node'
	lv.cpu_mode = 'host-passthrough'
	lv.storage_pool_name = 'default'
	lv.disk_bus = 'virtio'
	lv.nic_model_type = 'virtio'
	lv.volume_cache = 'writeback'
        # Fix kernel panic on fedora 28
......

 node.vm.synced_folder &quot;.&quot;, &quot;/vagrant&quot;, disabled: true, type: &quot;rsync&quot;, rsync__args: ['--verbose', '--archive', '--delete', '-z'] , rsync__exclude: ['.git','venv']
      $shared_folders.each do |src, dst|
......
      node.vm.provision &quot;shell&quot;, inline: &quot;swapoff -a&quot;
      # Change the dns-nameservers
      node.vm.provision :shell, path: &quot;dns.sh&quot;

......
        #&quot;docker_keepcache&quot;: &quot;1&quot;,
        #&quot;download_run_once&quot;: &quot;True&quot;,
        #&quot;download_localhost&quot;: &quot;False&quot;,
	&quot;ansible_python_interpreter&quot;: &quot;/usr/bin/python3&quot;

......
	  ansible.become_user = &quot;root&quot;
          ansible.limit = &quot;all&quot;
          ansible.host_key_checking = false
          #ansible.raw_arguments = [&quot;--forks=#{$num_instances}&quot;, &quot;--flush-cache&quot;, &quot;--ask-become-pass&quot;]
          ansible.host_vars = host_vars
          #ansible.tags = ['download']
          ansible.groups = {
            &quot;kube-deploy&quot; =&gt; [&quot;#{$instance_name_prefix}-[1:#{$kube_deploy_instances}]&quot;],
</code></pre><h4 id=3-inventory-folder>3. inventory folder</h4><p>Remove the hosts.ini file under <code>inventory/sample/</code>.<br>Edit some content in sample folder:</p><p>inventory/sample/group_vars/k8s-cluster/k8s-cluster.yml:</p><pre><code>kubelet_deployment_type: host
helm_deployment_type: host
helm_stable_repo_url: &quot;http://portus.ddddd.com:5000/chartrepo/kubesprayns&quot;
</code></pre><p>helm/metric server :</p><pre><code>helm_enabled: true


# Metrics Server deployment
metrics_server_enabled: true
</code></pre><h4 id=4-role-bootstrap-os>4. role bootstrap-os</h4><ol><li>Copy <code>daemon.json</code>, <code>ntp.conf</code>, <code>portus.crt</code>, <code>server.crt</code> to kubespray-2.8.2.</li></ol><p><img src=/images/2019_01_31_10_12_33_444x187.jpg alt=/images/2019_01_31_10_12_33_444x187.jpg></p><ol start=2><li>bootstrap-ubuntu.yml changes<br>For disable auto-update, and configure intranet repository.</li></ol><pre><code># Bug-fix1, disable auto-update for releasing apt controlling
- name: &quot;Configure apt service status&quot;
  raw: systemctl stop apt-daily.timer;systemctl disable apt-daily.timer ; systemctl stop apt-daily-upgrade.timer ; systemctl disable apt-daily-upgrade.timer; systemctl stop apt-daily.service;  systemctl mask apt-daily.service; systemctl daemon-reload

- name: &quot;Sed configuration&quot;
  raw: sed -i 's/APT::Periodic::Update-Package-Lists &quot;1&quot;/APT::Periodic::Update-Package-Lists &quot;0&quot;/' /etc/apt/apt.conf.d/10periodic

- name: Configure intranet repository
  raw: echo &quot;deb [trusted=yes] http://portus.dddd.com:8888 ./&quot;&gt;/etc/apt/sources.list &amp;&amp; apt-get update -y &amp;&amp; mkdir -p /usr/local/static
- name: List ubuntu_packages
</code></pre><p>Install more packages:</p><pre><code>      - python-pip
      - ntp
      - dbus
      - docker-ce
      - build-essential
</code></pre><p>Upload the crt files, add configuration files for docker, docker login ,etc.</p><pre><code>- name: &quot;upload portus.crt and server.crt files to kube-deploy&quot;
  copy:
    src: &quot;{{ item }}&quot;
    dest: /usr/local/share/ca-certificates
    owner: root
    group: root
    mode: 0777
  with_items:
    - files/portus.crt
    - files/server.crt

- name: &quot;upload daemon.json files to kube-deploy&quot;
  copy:
    src: files/daemon.json
    dest: /etc/docker/daemon.json
    owner: root
    group: root
    mode: 0777

- name: &quot;update ca-certificates&quot;
  shell: update-ca-certificates

- name: ensure docker service is restarted and enabled
  service:
    name: &quot;{{ item }}&quot;
    enabled: yes
    state: restarted
  with_items:
    - docker

- name: Docker login portus.dddd.com
  shell: docker login -u kubespray -p Thinker@1 portus.dddd.com:5000

- name: Docker login docker.io
  shell: docker login -u kubespray -p thinker docker.io

- name: Docker login quay.io
  shell: docker login -u kubespray -p thinker quay.io

- name: Docker login gcr.io
  shell: docker login -u kubespray -p thinker gcr.io

- name: Docker login k8s.gcr.io
  shell: docker login -u kubespray -p thinker k8s.gcr.io

- name: Docker login docker.elastic.co
  shell: docker login -u kubespray -p thinker docker.elastic.co

- name: &quot;upload ntp.conf&quot;
  copy:
    src: files/ntp.conf
    dest: /etc/ntp.conf
    owner: root
    group: root
    mode: 0777

- name: &quot;Configure ntp service&quot;
  shell: systemctl enable ntp &amp;&amp; systemctl start ntp
- set_fact:
    ansible_python_interpreter: &quot;/usr/bin/python&quot;
  tags:
    - facts
</code></pre><h4 id=5-role-download>5. role download</h4><p>Using local server for downloading:</p><pre><code># Download URLs
#kubeadm_download_url: &quot;https://storage.googleapis.com/kubernetes-release/release/{{ kubeadm_version }}/bin/linux/{{ image_arch }}/kubeadm&quot;
#hyperkube_download_url: &quot;https://storage.googleapis.com/kubernetes-release/release/{{ kube_version }}/bin/linux/amd64/hyperkube&quot;
etcd_download_url: &quot;https://github.com/coreos/etcd/releases/download/{{ etcd_version }}/etcd-{{ etcd_version }}-linux-amd64.tar.gz&quot;
#cni_download_url: &quot;https://github.com/containernetworking/plugins/releases/download/{{ cni_version }}/cni-plugins-{{ image_arch }}-{{ cni_version }}.tgz&quot;
kubeadm_download_url: &quot;http://portus.dddd.com:8888/kubeadm&quot;
hyperkube_download_url: &quot;http://portus.dddd.com:8888/hyperkube&quot;
cni_download_url: &quot;http://portus.dddd.com:8888/cni-plugins-{{ image_arch }}-{{ cni_version }}.tgz&quot;
</code></pre><h3 id=offline-role>Offline role</h3><p>Create an offline folder under role:</p><pre><code># mkdir -p roles/kube-deploy
</code></pre><p>Under this folder we got following files:</p><pre><code># tree .
.
├── files
│   ├── 1604debs.tar.xz
│   ├── 1804debs.tar.xz
│   ├── autoindex.tar.xz
│   ├── ban_nouveau.run
│   ├── busybox1262.tar.xz
│   ├── cni-plugins-amd64-v0.6.0.tgz
│   ├── cni-plugins-amd64-v0.7.0.tgz
│   ├── compose.tar.gz
│   ├── daemon.json
│   ├── dashboard.tar.xz
│   ├── data.tar.gz
│   ├── db.docker.io
│   ├── db.elastic.co
│   ├── db.gcr.io
│   ├── db.quay.io
│   ├── db.dddd.com
│   ├── deploy.key
│   ├── deploy.key.pub
│   ├── deploy_local.repo
│   ├── dns.sh
│   ├── docker-compose
│   ├── docker-infra.service
│   ├── fileserver.tar.gz
│   ├── harbor.tar.xz
│   ├── hyperkube
│   ├── install.run
│   ├── kubeadm
│   ├── kubeadm.official
│   ├── mynginx.service
│   ├── named.conf.default-zones
│   ├── nginx19.tar.xz
│   ├── nginx.tar
│   ├── nginx.yaml
│   ├── ntp.conf
│   ├── pipcache.tar.gz
│   ├── portus.crt
│   ├── portus.tar.xz
│   ├── registry2.tar.xz
│   ├── secureregistryserver.service
│   ├── secureregistryserver.tar
│   ├── server.crt
│   └── tag_and_push.sh
└── tasks
    ├── deploy-centos.yml
    ├── deploy-ubuntu.yml
    └── main.yml

2 directories, 45 files
</code></pre><p>Now using old file, for setting up a cluster, now login to kube-deploy server. do following steps:</p><pre><code># systemctl stop secureregistryserver
# docker ps | grep secureregistryserver
# rm -rf /usr/local/secureregistryserver/data/
# mkdir /usr/local/secureregistryserver/data
# systemctl start secureregistryserver
# docker ps | grep secureregistry
7ce648336f22        nginx:1.9                                   &quot;nginx -g 'daemon of…&quot;   3 seconds ago       Up 1 second              80/tcp, 0.0.0.0:443-&gt;443/tcp                                          secureregistryserver_nginx_1_ce6348edc346
9258c0396001        registry:2                                  &quot;/entrypoint.sh /etc…&quot;   4 seconds ago       Up 2 seconds             127.0.0.1:5050-&gt;5000/tcp                                              secureregistryserver_registry_1_76f71f58f22d
</code></pre><p>Load the kubespray images and push to secureregistryserver:</p><pre><code># docker load&lt;combine.tar.xz
# docker push *****************
# docker push *****************
# docker push *****************
</code></pre><p>Load the kubeadm related images and push to secureregistryserver:</p><pre><code># docker load&lt;kubeadm.tar
# docker push *****************
# docker push *****************
# docker push *****************
</code></pre><p>Now stop the secureregistryserver service and make the tar file:</p><pre><code># systemctl stop secureregistryserver
# cd /usr/local/
# tar cvf secureregistryserver.tar secureregistryserver
</code></pre><p>The secureregistryserver.tar should be replaced into the <code>kube-deploy/files</code> folder</p><p>Replace the kubeadm( we add 100 years ssl signature):</p><pre><code># sha256sum kubeadm 
33ef160d8dd22e7ef6eb4527bea861218289636e6f1e3e54f2b19351c1048a07  kubeadm
# vim /var1/sync/kubespray282/kubespray282/kubespray-2.8.2/roles/download/defaults/main.yml
kubeadm_checksums:
  v1.12.5: 33ef160d8dd22e7ef6eb4527bea861218289636e6f1e3e54f2b19351c1048a07

</code></pre><p>Replaces the hyperkube:</p><pre><code># wget https://storage.googleapis.com/kubernetes-release/release/v1.12.5/bin/linux/amd64/hyperkube
</code></pre><h3 id=replace-new-items>Replace New Items</h3><p>!!!Rechecked here!!!</p><p>debs.tar.xz should be replaced(Currently we use the old ones)
secureregistryserver.tar contains all of the offline docker images, must be replaced.<br>kubeadm should be replaced, and checksums should be updated.
hyperkube should be replaced.</p><h3 id=using-vps-for-fetching-images>Using vps for fetching images</h3><p>Under a vps, download the kubespray-2.8.2.tar.gz, and made some modifications for syncing back the images:</p><p>Create a new sync.yml file:</p><pre><code>root@vpsserver:~/Code/kubespray-2.8.2# cat sync.yml 
---
- hosts: k8s-cluster:etcd:calico-rr
  any_errors_fatal: &quot;{{ any_errors_fatal | default(true) }}&quot;
  roles:
    - { role: kubespray-defaults}
    - { role: sync, tags: download, when: &quot;not skip_downloads&quot; }
  environment: &quot;{{proxy_env}}&quot;
</code></pre><p>Copy the sample folder under <code>inventory</code> to a new name <code>sync</code>:</p><pre><code>root@vpsserver:~/Code/kubespray-2.8.2# cat inventory/sync/hosts.ini 
# ## Configure 'ip' variable to bind kubernetes services on a
# ## different ip than the default iface
# ## We should set etcd_member_name for etcd cluster. The node that is not a etcd member do not need to set the value, or can set the empty string value.
[all]
vpsserver ansible_ssh_host=1xxx.xxx.xxx.xx ansible_user=root ansible_ssh_private_key_file=&quot;/root/.ssh/id_rsa&quot; ansible_port=22

[kube-master]
vpsserver

[etcd]
vpsserver

[kube-node]
vpsserver

[k8s-cluster:children]
kube-master
kube-node
</code></pre><p>Copy <code>roles/download</code> to <code>roles/sync</code>, and make some changes, notice all of the downloads docker images should changes to <code>enabled:true</code>:</p><pre><code>root@vpsserver:~/Code/kubespray-2.8.2/roles# ls
adduser             bootstrap-os      dnsmasq   etcd        kubernetes-apps     network_plugin  reset  upgrade
bastion-ssh-config  container-engine  download  kubernetes  kubespray-defaults  remove-node     sync   win_nodes
root@vpsserver:~/Code/kubespray-2.8.2/roles# cd sync/
root@vpsserver:~/Code/kubespray-2.8.2/roles/sync# ls
defaults  meta  tasks
root@vpsserver:~/Code/kubespray-2.8.2/roles/sync# cat defaults/main.yml 
downloads:
  netcheck_server:
    #enabled: &quot;{{ deploy_netchecker }}&quot;
    enabled: true
    container: true
    repo: &quot;{{ netcheck_server_image_repo }}&quot;
    tag: &quot;{{ netcheck_server_image_tag }}&quot;
    sha256: &quot;{{ netcheck_server_digest_checksum|default(None) }}&quot;
    groups:
      - k8s-cluster

  netcheck_agent:
    #enabled: &quot;{{ deploy_netchecker }}&quot;
    enabled:  true
    container: true
    repo: &quot;{{ netcheck_agent_image_repo }}&quot;
    tag: &quot;{{ netcheck_agent_image_tag }}&quot;
    sha256: &quot;{{ netcheck_agent_digest_checksum|default(None) }}&quot;
    groups:
      - k8s-cluster
</code></pre><p>Disalbe <code>Sync containers</code> tasks, so we will only download the docker images to local.</p><pre><code>root@vpsserver:~/Code/kubespray-2.8.2/roles/sync# cat tasks/main.yml 
---
- include_tasks: download_prep.yml
  when:
    - not skip_downloads|default(false)

- name: &quot;Download items&quot;
  include_tasks: &quot;download_{% if download.container %}container{% else %}file{% endif %}.yml&quot;
  vars:
    download: &quot;{{ download_defaults | combine(item.value) }}&quot;
  with_dict: &quot;{{ downloads }}&quot;
  when:
    - not skip_downloads|default(false)
    - item.value.enabled
    - (not (item.value.container|default(False))) or (item.value.container and download_container)

#- name: &quot;Sync container&quot;
#  include_tasks: sync_container.yml
#  vars:
#    download: &quot;{{ download_defaults | combine(item.value) }}&quot;
#  with_dict: &quot;{{ downloads }}&quot;
#  when:
#    - not skip_downloads|default(false)
#    - item.value.enabled
#    - item.value.container | default(false)
#    - download_run_once
#    - group_names | intersect(download.groups) | length
</code></pre><p>All of the kubespray pre-defined images will be downloaded to vps, check via:</p><p><img src=/images/2019_01_31_10_51_03_784x538.jpg alt=/images/2019_01_31_10_51_03_784x538.jpg></p><p>Save it using scripts:</p><pre><code># docker save -o combine.tar gcr.io/google-containers/hyperkube-amd64:v1.12.5 registry:2.6 busybox:latest quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0 quay.io/jetstack/cert-manager-controller:v0.5.2 weaveworks/weave-npc:2.5.0 weaveworks/weave-kube:2.5.0 coredns/coredns:1.2.6 cilium/cilium:v1.3.0 nfvpe/multus:v3.1.autoconf cloudnativelabs/kube-router:v0.2.1 gcr.io/google_containers/cluster-proportional-autoscaler-amd64:1.3.0 lachlanevenson/k8s-helm:v2.11.0 gcr.io/kubernetes-helm/tiller:v2.11.0 gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.13 gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.13 gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.13 k8s.gcr.io/metrics-server-amd64:v0.3.1 quay.io/external_storage/cephfs-provisioner:v2.1.0-k8s1.11 gcr.io/google_containers/kubernetes-dashboard-amd64:v1.10.0 busybox:1.29.2 quay.io/coreos/etcd:v3.2.24 k8s.gcr.io/addon-resizer:1.8.3 quay.io/calico/node:v3.1.3 quay.io/calico/ctl:v3.1.3 quay.io/calico/kube-controllers:v3.1.3 quay.io/calico/cni:v3.1.3 nginx:1.13 quay.io/external_storage/local-volume-provisioner:v2.1.0 quay.io/calico/routereflector:v0.6.1 quay.io/coreos/flannel:v0.10.0 contiv/netplugin:1.2.1 contiv/auth_proxy:1.2.1 gcr.io/google_containers/pause-amd64:3.1 ferest/etcd-initer:latest andyshinn/dnsmasq:2.78 quay.io/coreos/flannel-cni:v0.3.0 xueshanf/install-socat:latest quay.io/l23network/k8s-netchecker-agent:v1.0 quay.io/l23network/k8s-netchecker-server:v1.0 gcr.io/google_containers/kube-registry-proxy:0.4 
# xz combine.tar
</code></pre><h3 id=using-vagrant-for-fetching-kubeadm>Using vagrant for fetching kubeadm</h3><p>Your vagrant box should cross gfw, then you use a 1604 box for create an all-in-one k8s cluster.<br>After provision, use following command for saving all of the images:</p><pre><code># docker images
# docker save -o combine.tar xxxxx xxx xxx xxx xxx xxx
</code></pre></div><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/50/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/50/>50</a></li><li class="page-item active"><a class=page-link href=/page/51/>51</a></li><li class=page-item><a class=page-link href=/page/52/>52</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/226/>226</a></li><li class=page-item><a href=/page/52/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/226/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div><script src=http://purplepalmdash.github.io/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>