<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta name=generator content="Hugo 0.64.0"><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Dash &#183; Dash</title><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124"><link rel=stylesheet href=https://purplepalmdash.github.io/css/highlight/googlecode.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js></script><script type=text/javascript src=/js/html2canvas.js></script><script type=text/javascript>function genPostShot(){var rightNow=new Date();var imageName=rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");imageName+='.jpg'
html2canvas(document.getElementsByClassName('post'),{background:'#FFFFFF',onrendered:function(canvas){$('#test').attr('href',canvas.toDataURL("image/jpeg"));$('#test').attr('download',imageName);$('#test')[0].click();}});};</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124"><link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel=icon><link href=%7balternate%20%7bRSS%20application/rss+xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://purplepalmdash.github.io/index.xml%7d rel=alternate type=application/rss+xml title="Dash &#183; Dash"><meta name=description content><meta name=keywords content="unix,virtualization,embedded,linux"></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><img src=http://purplepalmdash.github.io/images/mylogo.jpeg alt=gravatar><h1><a href=http://purplepalmdash.github.io/>很惭愧，就做了一点微小的工作</a></h1><a href=http://purplepalmdash.github.io/><p>Dash</p></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/post/>All Posts</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/technology/>Technology</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/life/>Life</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/linuxtips/>LinuxTips</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/purplepalmdash><i class="fa fa-github-square fa-3x"></i></a><a href=https://cn.linkedin.com/in/yang-feipeng-1b909319><i class="fa fa-linkedin-square fa-3x"></i></a><a href=https://plus.google.com/u/0/106572959364703833986><i class="fa fa-google-plus-square fa-3x"></i></a><a href=https://www.facebook.com/yang.feipeng><i class="fa fa-facebook-square fa-3x"></i></a><a href=https://twitter.com/dashwillfly><i class="fa fa-twitter-square fa-3x"></i></a></li></ul></div></div><div class="content container"><div class=posts><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/08/12/workingtipsonofflineopenshift/>WorkingTipsOnOfflineOpenshift</a></h1><span class=post-date>Aug 12, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=os-preparation>OS Preparation</h3><p>Centos 7.6 OS, installed via:</p><pre><code>CentOS-7-x86_64-Everything-1810.iso
</code></pre><p>Download the source code from:</p><pre><code>https://gitee.com/xhua/OpenshiftOneClick
</code></pre><p>Corresponding docker images:</p><pre><code>docker.io/redis:5
docker.io/openshift/origin-node:v3.11.0
docker.io/openshift/origin-control-plane:v3.11.0
docker.io/openshift/origin-haproxy-router:v3.11.0
docker.io/openshift/origin-deployer:v3.11.0
docker.io/openshift/origin-pod:v3.11.0
docker.io/rabbitmq:3.7-management
docker.io/mongo:4.1
docker.io/memcached:1.5
quay.io/kubevirt/kubevirt-web-ui-operator:latest
docker.io/xhuaustc/openldap-2441-centos7:latest
quay.io/kubevirt/kubevirt-web-ui:v2.0.0
docker.io/perconalab/pxc-openshift:latest
docker.io/tomcat:8.5-alpine
docker.io/centos/postgresql-95-centos7:latest
docker.io/centos/mysql-57-centos7:latest
docker.io/centos/nginx-112-centos7:latest
docker.io/curiouser/dubbo_zookeeper:v1
docker.io/xhuaustc/logstash:6.6.1
docker.io/xhuaustc/kibana:6.6.1
docker.io/xhuaustc/elasticsearch:6.6.1
docker.io/openshift/jenkins-2-centos7:latest
docker.io/openshift/origin-docker-registry:v3.11.0
docker.io/openshift/jenkins-agent-maven-35-centos7:v4.0
docker.io/openshift/origin-console:v3.11.0
docker.io/sonatype/nexus3:3.14.0
docker.io/gitlab/gitlab-ce:11.4.0-ce.0
docker.io/openshift/origin-web-console:v3.11.0
docker.io/cockpit/kubernetes:latest
docker.io/xhuaustc/apolloportal:latest
docker.io/xhuaustc/apolloconfigadmin:latest
docker.io/xhuaustc/nfs-client-provisioner:latest
docker.io/blackcater/easy-mock:1.6.0
docker.io/perconalab/proxysql-openshift:0.5
docker.io/xhuaustc/selenium:3
docker.io/xhuaustc/zalenium:3
docker.io/xhuaustc/etcd:v3.2.22
docker.io/openshiftdemos/gogs:0.11.34
docker.io/openshiftdemos/sonarqube:6.7
docker.io/xhuaustc/openshift-kafka:latest
docker.io/redis:3.2.3-alpine
docker.io/kubevirt/virt-api:v0.19.0
docker.io/kubevirt/virt-controller:v0.19.0
docker.io/kubevirt/virt-handler:v0.19.0
docker.io/kubevirt/virt-operator:v0.19.0
</code></pre><h3 id=servers>Servers</h3><h3 id=rpm-server>rpm server</h3><p>ISO as a rpm server.</p><p>offline iso rpm server.</p><pre><code># vim files/all.repo
[openshift]
name=openshift
baseurl=http://192.192.189.1/ocrpmpkgs/
enabled=1
gpgcheck=0

[openshift1]
name=openshift1
baseurl=http://192.192.189.1:8080
enabled=1
gpgcheck=0
</code></pre><h4 id=simple-https-server>Simple https server</h4><p>Create a new folder and generate pem files under this folder:</p><pre><code># openssl req -new -x509 -keyout server.pem -out server.pem -days 365 -nodes
Common name: ssl.xxxx.com
</code></pre><p>If you set <code>ssl.xxxx.com</code>, then you visit this website via <code>https://ssl.xxxx.com:4443/index.html</code>.</p><p>Write a simple python file for serving https:</p><pre><code># vi simple-https-server.py
import BaseHTTPServer, SimpleHTTPServer
import ssl
httpd = BaseHTTPServer.HTTPServer(('localhost', 4443), SimpleHTTPServer.SimpleHTTPRequestHandler)
httpd.socket = ssl.wrap_socket (httpd.socket, certfile='./server.pem', server_side=True)
httpd.serve_forever()
# sudo python simple-https-server.py
</code></pre><p>Server folder content:</p><pre><code># ls
allinone-webconsole.css  apollo.png  easymock.png  kafka.png  nexus3.png    pxc.png     simple-https-server.py  zalenium.png
allinone-webconsole.js   dubbo.png   gogs.png      kelk.png   openldap.png  server.pem  sonarqube.svg
# cat allinone-webconsole.css 
.icon-gogs{
  background-image: url(https://ssl.xxxx.com:4443/gogs.png);
  width: 50px;
  height: 50px;
  background-size: 100% 100%;
}
.icon-sonarqube{
  background-image: url(https://ssl.xxxx.com:4443/sonarqube.svg);
  width: 80px;
  height: 50px;
  background-size: 100% 100%;
}

</code></pre><h4 id=using-simple-https-server>Using Simple https server</h4><p>Add customized domain name into <code>/etc/hosts</code>:</p><pre><code># vim /etc/hosts
192.192.189.1	ssl.xxxx.com
</code></pre><p>Add <code>server.pem</code> into the client system:</p><pre><code># yum install -y ca-certificates
# update-ca-trust force-enable
# cp server.pem /etc/pki/ca-trust/source/anchors/ssl.xxxx.com.pem
# update-ca-trust
</code></pre><h3 id=deployment>Deployment</h3><p>Two nodes, run following scripts first:</p><pre><code>#!/bin/bash

setenforce 1
selinux=$(getenforce)
if [ &quot;$selinux&quot; != Enforcing ]
then
	echo &quot;Please setlinux Enforcing&quot;
	exit 10
fi

cat &gt;/etc/sysctl.d/99-elasticsearch.conf &lt;&lt;EOF
vm.max_map_count = 262144
EOF
sysctl vm.max_map_count=262144

export CHANGEREPO=true
if [ $CHANGEREPO == true -a ! -d /etc/yum.repos.d/back ]
then
    cd /etc/yum.repos.d/; mkdir -p back; mv -f *.repo back/; cd -
    cp files/all.repo /etc/yum.repos.d/
    yum clean all
fi


current_path=`pwd`
yum localinstall tools/ansible-2.6.5-1.el7.ans.noarch.rpm -y
ansible-playbook playbook.yml --skip-tags after_task
cd $current_path/openshift-ansible-playbook
ansible-playbook playbooks/prerequisites.yml
</code></pre><p>Configuration of Master node&rsquo;s <code>config.yml</code>:</p><pre><code>---
CHANGEREPO: true
HOSTNAME: os311.test.it.example.com

</code></pre><p>Configuration of Worker node&rsquo;s <code>config.yml</code>:</p><pre><code>---
CHANGEREPO: true
HOSTNAME: os312.test.it.example.com
</code></pre><p>Then add following lines into <code>/etc/hosts</code>:</p><pre><code>192.192.189.128 os311.test.it.example.com
192.192.189.129 os312.test.it.example.com
192.192.189.1	ssl.xxxx.com
</code></pre><p>Then on master node, replace the <code>/etc/ansible/hosts</code> with our pre-defined one:</p><pre><code>.....
openshift_web_console_extension_script_urls=[&quot;https://ssl.xxxx.com:4443/allinone-webconsole.js&quot;]
openshift_web_console_extension_stylesheet_urls=[&quot;https://ssl.xxxx.com:4443/allinone-webconsole.css&quot;]

......
openshift_disable_check=memory_availability,disk_availability,package_availability,package_update,docker_image_availability,docker_storage_driver,docker_storage,package_version

.......
openshift_node_groups=[{'name': 'node-config-all-in-one', 'labels': ['node-role.kubernetes.io/master=true', 'node-role.kubernetes.io/infra=true']}, {'name': 'node-config-compute', 'labels': ['node-role.kubernetes.io/compute=true']}]
.......
[masters]
os311.test.it.example.com
[etcd]
os311.test.it.example.com

[nfs]
os311.test.it.example.com

[nodes]
os311.test.it.example.com openshift_node_group_name=&quot;node-config-all-in-one&quot;
os312.test.it.example.com openshift_node_group_name='node-config-compute'
</code></pre><p>Now run deployment:</p><pre><code>current_path=`pwd`
cd $current_path/openshift-ansible-playbook
ansible-playbook playbooks/prerequisites.yml

ansible-playbook -vvvv playbooks/deploy_cluster.yml
oc adm policy add-cluster-role-to-user cluster-admin admin

cd $current_path

ansible-playbook playbook.yml --tags install_nfs
ansible-playbook playbook.yml --tags after_task
</code></pre><p>After deployment, check the status via:</p><pre><code>[root@os311 OpenshiftOneClick]# oc get nodes
NAME                        STATUS    ROLES          AGE       VERSION
os311.test.it.example.com   Ready     infra,master   3d        v1.11.0+d4cacc0
os312.test.it.example.com   Ready     compute        3d        v1.11.0+d4cacc0
</code></pre><h3 id=kube-virt>kube-virt</h3><p>via following steps, deploy kubevirt:</p><pre><code># kubectl apply -f kubevirt-operator.yaml
# kubectl apply -f kubevirt-cr.yaml
</code></pre><p>deploy ui:</p><pre><code># cd web-ui-operator-master
# oc new-project kubevirt-web-ui
# cd deploy
# oc apply -f service_account.yaml
# oc apply -f role.yaml
# oc apply -f role_binding.yaml
# oc create -f crds/kubevirt_v1alpha1_kwebui_crd.yaml
# oc apply -f operator.yaml
# oc apply -f deploy/crds/kubevirt_v1alpha1_kwebui_cr.yaml
</code></pre><h3 id=dns-setting>DNS setting</h3><p>By following steps:</p><pre><code># vim /etc/dnsmasq.d/origin-dns.conf
address=/os311.test.it.example.com/192.192.189.128
# systemctl daemon-reload
# systemctl restart dnsmasq
</code></pre><h3 id=create-vm>Create vm</h3><p>The definition files should be modified into:</p><pre><code>apiVersion: kubevirt.io/v1alpha3
kind: VirtualMachine
metadata:
  name: testvm
spec:
  running: false
  template:
    metadata:
      labels: 
        kubevirt.io/size: small
        kubevirt.io/domain: testvm
    spec:
      domain:
        devices:
          disks:
            - name: containerdisk
              disk:
                bus: virtio
            - name: cloudinitdisk
              disk:
                bus: virtio
          interfaces:
          - name: default
            bridge: {}
        resources:
          requests:
            memory: 64M
      networks:
      - name: default
        pod: {}
      volumes:
        - name: containerdisk
          containerDisk:
            image: kubevirt/cirros-registry-disk-demo
            imagePullPolicy: IfNotPresent
        - name: cloudinitdisk
          cloudInitNoCloud:
            userDataBase64: SGkuXG4=
</code></pre><p>Thus we could launch the vms, notice we have to pull the images manually:</p><pre><code># sudo docker pull kubevirt/cirros-registry-disk-demo
# sudo docker pull index.docker.io/kubevirt/virt-launcher:v0.19.0
# sudo docker pull kubevirt/virt-launcher:v0.19.0
</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/07/05/workingtipsonarm64centos7/>WorkingTipsOnArm64centos7</a></h1><span class=post-date>Jul 5, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=仓库配置>仓库配置</h3><p>使用epel-7及centos7的源为默认源:</p><pre><code>[root@arm64 ~]# cat /etc/yum.repos.d/epel7.repo 
[epel]
name=Extra Packages for Enterprise Linux 7 - $basearch
baseurl=http://mirrors.aliyun.com/epel/7/$basearch
failovermethod=priority
enabled=1
gpgcheck=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7
 
[epel-debuginfo]
name=Extra Packages for Enterprise Linux 7 - $basearch - Debug
baseurl=http://mirrors.aliyun.com/epel/7/$basearch/debug
failovermethod=priority
enabled=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7
gpgcheck=0
 
[epel-source]
name=Extra Packages for Enterprise Linux 7 - $basearch - Source
baseurl=http://mirrors.aliyun.com/epel/7/SRPMS
failovermethod=priority
enabled=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7
gpgcheck=0

[root@arm64 ~]# cat /etc/yum.repos.d/centos7.repo 


[base]
name=CentOS-7 - Base
baseurl=https://mirrors.aliyun.com/centos-altarch/7/os/$basearch/
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7

#released updates 
[updates]
name=CentOS-7 - Updates
baseurl=https://mirrors.aliyun.com/centos-altarch/7/updates/$basearch/
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7

#additional packages that may be useful
[extras]
name=CentOS-7 - Extras
baseurl=https://mirrors.aliyun.com/centos-altarch/7/extras/$basearch/
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7
enabled=1

#additional packages that extend functionality of existing packages
[centosplus]
name=CentOS-7 - Plus
baseurl=https://mirrors.aliyun.com/centos-altarch/7/centosplus/$basearch/
gpgcheck=1
enabled=0
gpgkey=https://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7

</code></pre><p>基本情况:</p><pre><code>[root@arm64 ~]# uname -a
Linux arm64 4.14.0-115.el7a.0.1.aarch64 #1 SMP Sun Nov 25 20:54:21 UTC 2018 aarch64 aarch64 aarch64 GNU/Linux
[root@arm64 ~]# cat /etc/redhat-release 
CentOS Linux release 7.6.1810 (AltArch) 
</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/07/03/building-k8s-dns-node-cache-arm64/>Building k8s-dns-node-cache-arm64</a></h1><span class=post-date>Jul 3, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=steps>Steps</h3><p>Clone the source code from github:</p><pre><code># git clone https://github.com/kubernetes/dns.git
# git checkout tags/1.15.1 -b local_1.15.1
</code></pre><p>Modify the source code:</p><pre><code># vim Makefile
BINARIES := \
    node-cache
#    e2e \
#    ginkgo \
#    sidecar-e2e


CONTAINER_BINARIES := \
    node-cache

ARCH ?= arm64
</code></pre><p>Now building via:</p><pre><code># make build
# make containers
</code></pre><p>Will get error, now copy the generated node-cache file into destination directory:</p><pre><code># cp ./.go/bin/node-cache bin/arm64/
</code></pre><p>Edit the dnsmasq&rsquo;s Makefile via:</p><pre><code># vim images/dnsmasq/Makefile
</code></pre><p><img src=/images/2019_07_03_14_47_54_635x612.jpg alt=/images/2019_07_03_14_47_54_635x612.jpg></p><p>make will also get error, manually compile the dnsmasq:</p><pre><code># cd ./images/dnsmasq/_output/arm64/dnsmasq-2.78
# make
# cp src/dnsmasq ../docker
</code></pre><p>Now go the project root directory and make containers:</p><pre><code># make containers
# docker images  | grep dns
staging-k8s.gcr.io/k8s-dns-dnsmasq-arm64      1.15.1-dirty         fbb04ccb60e6        About an hour ago   3.63MB
staging-k8s.gcr.io/k8s-dns-node-cache-arm64   1.15.1-dirty         bf6131745b5e        About an hour ago   71.9MB
</code></pre><p>Now replace the dns-node-cache default to our own build-out version we could enable node-cache working on arm64.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/07/02/arm64isocustomization/>Arm64ISOCustomization</a></h1><span class=post-date>Jul 2, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>Make working directory:</p><pre><code>#  mkdir Rong1907iso
# cd Rong1907iso/
# cp ../ubuntu-18.04.2-server-arm64.iso .
# cp -r ./iso/* ./newISO
# cp -r ./iso/.disk ./newISO
# umount ./iso
# rm -f ubuntu-18.04.2-server-arm64.iso 
# rm -rf iso/
</code></pre><p>Add seed files under <code>preseed</code> directory, then edit the grub files:</p><pre><code>root@arm02:/home/test/Rong1907iso/newISO# ls preseed/
cli.seed      hwe-ubuntu-server-minimal.seed    hwe-ubuntu-server.seed  fuck_auto.seed        ubuntu-server-minimal.seed    ubuntu-server.seed
hwe-cli.seed  hwe-ubuntu-server-minimalvm.seed  fuck.seed            fuck_auto_multi.seed  ubuntu-server-minimalvm.seed
root@arm02:/home/test/Rong1907iso/newISO# ls boot/grub/grub.cfg 
boot/grub/grub.cfg
</code></pre><p>Edit the grub file like following:</p><pre><code>set menu_color_normal=white/black
set menu_color_highlight=black/yellow

insmod gzio

set timeout=10
menuentry &quot;Auto Install Ubuntu Server(Manual-Partition)&quot; {
        set gfxpayload=keep
        linux   /install/vmlinuz auto-install/enable=true file=/cdrom/preseed/fuck.seed quiet ---
        initrd  /install/initrd.gz
}
menuentry &quot;Auto Install Ubuntu Server(Auto-Partition-AllInOne)&quot; {
        set gfxpayload=keep
        linux   /install/vmlinuz auto-install/enable=true file=/cdrom/preseed/fuck_auto.seed quiet ---
        initrd  /install/initrd.gz
}
menuentry &quot;Auto Install Ubuntu Server(Auto-Partition-Seperate)&quot; {
        set gfxpayload=keep
        linux   /install/vmlinuz auto-install/enable=true file=/cdrom/preseed/fuck_auto_multi.seed quiet ---
        initrd  /install/initrd.gz
}
menuentry &quot;Install Ubuntu Server&quot; {
        set gfxpayload=keep
        linux   /install/vmlinuz  file=/cdrom/preseed/ubuntu-server.seed quiet ---
        initrd  /install/initrd.gz
}

</code></pre><p>Make the iso via following command:</p><pre><code># xorriso -as mkisofs -r -checksum_algorithm_iso md5,sha1 -V 'Server 18.04.2 LTS arm64' -o ./fuck_ubuntu180402_arm64.iso -J -joliet-long -cache-inodes -e boot/grub/efi.img  -no-emul-boot -append_partition 2 0xef newISO/boot/grub/efi.img  -partition_cyl_align all newISO/
root@arm02:/home/test/Rong1907iso# ls
newISO  fuck_ubuntu180402_arm64.iso
</code></pre><p>Using the <code>fuck_ubuntu180402_arm64.iso</code> you could install systme on arm64 based server.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2019/06/28/arm64kubesprayofflinetips/>arm64KubesprayOfflineTips</a></h1><span class=post-date>Jun 28, 2019<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=folder-structure>Folder structure</h3><p>Compare the downloaded source code to our offlined edition, make some changes.</p><pre><code>cluster.yml should added kube-deploy related items.   
ansible.cfg should be modified.
Added role/kube-deploy folder. 
scale.yml/upgrade-cluster.yml should be modified. 
Added deploy.key for easy deployment. 
roles/kubernetes-apps/ansible/defaults/main.yml, modified dashboard_skip_login condition
roles/kubernetes-apps/ansible/templates/dashboard.yml.j2: NodePort modification
roles/kubespray-defaults/defaults/main.yaml: enable_nodelocaldns:false(TBD)
roles/download/defaults/main.yml: download position, for example hyperkube/kubeadm/cni/calicoctl etc. 
/roles/kubernetes/master/templates/kubeadm-config.v1alpha3.yaml.j2: controllerManager listening port to 0.0.0.0
roles/kubernetes/master/tasks/kubeadm-upgrade.yml: upgrade items to --force
</code></pre><h3 id=todo-bootstrapsh>(Todo) bootstrap.sh</h3><p>Change the installation of ansible from apt-get to pip-cache</p><pre><code>#!/bin/sh
## 
OS_ID=`cat /etc/os-release | grep VERSION_CODENAME | awk -F '=' {'print $2'}`
echo $OS_ID
# xenial use 1604, bionic use 1804
if [ &quot;$OS_ID&quot; = &quot;xenial&quot; ]; then
	sudo tar xJvf ./roles/kube-deploy/files/1604debs.tar.xz -C /usr/local/
else
	sudo tar xJvf ./roles/kube-deploy/files/1804debs.tar.xz -C /usr/local/
	sudo tar xJvf ./roles/kube-deploy/files/pip_ansible.tar.xz -C /usr/local/
fi
sudo echo &quot;deb [trusted=yes] file:///usr/local/static ./&quot;&gt;/etc/apt/sources.list
sudo apt-get update -y
# Install pip so we could use pip for installing ansible
sudo DEBIAN_FRONTEND=noninteractive apt-get install -y python-pip
# Install ansible via ansible(version 2.8.1)
sudo pip install --no-index --find-links /usr/local/pip_ansible ansible
sudo DEBIAN_FRONTEND=noninteractive apt-get install -y python-netaddr
</code></pre><h3 id=kube-deploy-role>kube-deploy role</h3><p>Added the offline role, and replace the files.</p><p>Replace:</p><pre><code>nginx-autoindex.tar.xz 
kubeadm(arm version)
hyperkube(arm version)
cni-plugins-arm64-v0.6.0.tgz(arm version)
calicoctl(arm version)

</code></pre><h3 id=nginx-autoindex>nginx-autoindex</h3><p>Find the Dockerfile, and build the arm64 based docker images via following commands:</p><pre><code># mkdir -p ~/code/autoindex
# vim Dockerfile
FROM nginx
MAINTAINER Jason Kingsbury

RUN sed -i 'N; s/root   \/usr\/share\/nginx\/html;\n        index  index.html index.htm;/root   \/usr\/share\/nginx\/html;\n        autoindex on;/' /etc/nginx/conf.d/default.conf
# sudo docker build -t jrelva/nginx-autoindex:latest .
# sudo docker  --name docker-nginx -p 7888:80 -d --restart=always -v `pwd`:/usr/share/nginx/html jrelva/nginx-autoindex
</code></pre><p>Save and xz the docker images:</p><pre><code># sudo docker save jrelva/nginx-autoindex:latest&gt;autoindex.tar; sudo xz autoindex.tar
</code></pre><p>Transfer the autoindex.tar.xz to folder.</p><pre><code>➜  files ls -l -h | grep autoindex
-rwxr-xr-x  1 dash dash  26M 5月   7 16:40 autoindex.tar.xz
➜  files pwd
/home/dash/Code/kubsprayarm64/roles/kube-deploy/files
➜  files ls -l -h | grep autoindex
-rwxr-xr-x  1 dash dash  23M 6月  28 10:59 autoindex.tar.xz
</code></pre><h3 id=secureregistryserver>secureregistryserver</h3><p>Change the docker-compose file, also pull the arm based docker images:</p><pre><code># docker pull registry:2
# docker pull nginx:latest
# docker save registry:2&gt;regsitry.tar; xz registry.tar
# docker save nginx:latest&gt;nginx19.tar; xz nginx19.tar

</code></pre><h3 id=verification>Verification</h3><p>Create a new virtual machine(18.04.2):</p><pre><code>$ qemu-img create -f qcow2 pure1804.qcow2 200G
```'
Install the system:    

![/images/2019_06_28_12_10_54_469x237.jpg](/images/2019_06_28_12_10_54_469x237.jpg)


### harbor offline
With the harbor-offline-installer-1.7.0-arm64.tgz we could quickly setup the offline harbor environment:    

</code></pre><h1 id=ls>ls</h1><p>common docker-compose.clair.yml docker-compose.yml harbor.cfg LICENSE prepare<br>docker-compose.chartmuseum.yml docker-compose.notary.yml harbor.1.7.0-arm64.tar.gz install.sh open_source_license</p><h1 id=vim-harborcfg>vim harbor.cfg</h1><h1 id=installsh---with-chartmuseum>./install.sh &ndash;with-chartmuseum</h1><h1 id=docker-ps>docker ps</h1><p>&mldr;..</p><pre><code>
Login:    

![/images/2019_06_28_12_19_39_475x380.jpg](/images/2019_06_28_12_19_39_475x380.jpg)

Create user kubespray:    

![/images/2019_06_28_12_19_59_447x368.jpg](/images/2019_06_28_12_19_59_447x368.jpg)

Fill in user info:    

![/images/2019_06_28_12_20_28_555x469.jpg](/images/2019_06_28_12_20_28_555x469.jpg)

Create project:    

![/images/2019_06_28_12_21_45_549x348.jpg](/images/2019_06_28_12_21_45_549x348.jpg)

Projects:    

![/images/2019_06_28_12_22_02_667x296.jpg](/images/2019_06_28_12_22_02_667x296.jpg)

Add kubespray to kubesprayns as administrator:    

![/images/2019_06_28_12_22_42_718x295.jpg](/images/2019_06_28_12_22_42_718x295.jpg)

Now you could login with kubespray user:    

![/images/2019_06_28_12_23_52_936x419.jpg](/images/2019_06_28_12_23_52_936x419.jpg)

Now in docker-compose folder we just `docker-compose down` all of the service and backup our environment:    

</code></pre><h1 id=docker-compose-down>docker-compose down</h1><p>Stopping nginx &mldr; done
Stopping harbor-jobservice &mldr; done
Stopping harbor-portal &mldr; done
Stopping harbor-core &mldr; done
Stopping redis &mldr; done
Stopping harbor-adminserver &mldr; done
Stopping harbor-db &mldr; done
Stopping registry &mldr; done
Stopping registryctl &mldr; done
Stopping harbor-log &mldr;</p><h1 id=docker-save--o-harbortar-goharborchartmuseum-photonv071-170-arm64-goharborredis-photon170-arm64-goharborclair-photonv207-170-arm64-goharbornotary-server-photonv061-170-arm64-goharbornotary-signer-photonv061-170-arm64-goharborharbor-registryctl170-arm64-goharborregistry-photonv262-170-arm64-goharbornginx-photon170-arm64-goharborharbor-log170-arm64-goharborharbor-jobservice170-arm64-goharborharbor-core170-arm64-goharborharbor-portal170-arm64-goharborharbor-adminserver170-arm64-goharborharbor-db170-arm64>docker save -o harbor.tar goharbor/chartmuseum-photon:v0.7.1-1.7.0-arm64 goharbor/redis-photon:1.7.0-arm64 goharbor/clair-photon:v2.0.7-1.7.0-arm64 goharbor/notary-server-photon:v0.6.1-1.7.0-arm64 goharbor/notary-signer-photon:v0.6.1-1.7.0-arm64 goharbor/harbor-registryctl:1.7.0-arm64 goharbor/registry-photon:v2.6.2-1.7.0-arm64 goharbor/nginx-photon:1.7.0-arm64 goharbor/harbor-log:1.7.0-arm64 goharbor/harbor-jobservice:1.7.0-arm64 goharbor/harbor-core:1.7.0-arm64 goharbor/harbor-portal:1.7.0-arm64 goharbor/harbor-adminserver:1.7.0-arm64 goharbor/harbor-db:1.7.0-arm64</h1><h1 id=xz-harbortar>xz harbor.tar</h1><pre><code></code></pre></div><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/56/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/56/>56</a></li><li class="page-item active"><a class=page-link href=/page/57/>57</a></li><li class=page-item><a class=page-link href=/page/58/>58</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/239/>239</a></li><li class=page-item><a href=/page/58/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/239/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div><script src=http://purplepalmdash.github.io/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>