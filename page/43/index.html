<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta name=generator content="Hugo 0.64.0"><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Dash &#183; Dash</title><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124"><link rel=stylesheet href=https://purplepalmdash.github.io/css/highlight/googlecode.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js></script><script type=text/javascript src=/js/html2canvas.js></script><script type=text/javascript>function genPostShot(){var rightNow=new Date();var imageName=rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");imageName+='.jpg'
html2canvas(document.getElementsByClassName('post'),{background:'#FFFFFF',onrendered:function(canvas){$('#test').attr('href',canvas.toDataURL("image/jpeg"));$('#test').attr('download',imageName);$('#test')[0].click();}});};</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124"><link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel=icon><link href=%7balternate%20%7bRSS%20application/rss+xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://purplepalmdash.github.io/index.xml%7d rel=alternate type=application/rss+xml title="Dash &#183; Dash"><meta name=description content><meta name=keywords content="unix,virtualization,embedded,linux"></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><img src=http://purplepalmdash.github.io/images/mylogo.jpeg alt=gravatar><h1><a href=http://purplepalmdash.github.io/>很惭愧，就做了一点微小的工作</a></h1><a href=http://purplepalmdash.github.io/><p>Dash</p></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/post/>All Posts</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/technology/>Technology</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/life/>Life</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/linuxtips/>LinuxTips</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/purplepalmdash><i class="fa fa-github-square fa-3x"></i></a><a href=https://cn.linkedin.com/in/yang-feipeng-1b909319><i class="fa fa-linkedin-square fa-3x"></i></a><a href=https://plus.google.com/u/0/106572959364703833986><i class="fa fa-google-plus-square fa-3x"></i></a><a href=https://www.facebook.com/yang.feipeng><i class="fa fa-facebook-square fa-3x"></i></a><a href=https://twitter.com/dashwillfly><i class="fa fa-twitter-square fa-3x"></i></a></li></ul></div></div><div class="content container"><div class=posts><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2021/07/06/workingtipsonmultiseat/>WorkingTipsOnMultiSeat</a></h1><span class=post-date>Jul 6, 2021<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=steps>steps</h3><p>default seat:</p><pre><code># loginctl seat-status seat0&gt;seat0.txt
# cat seat0.txt
seat0
	Sessions: *1
	 Devices:
		  ├─/sys/devices/LNXSYSTM:00/LNXPWRBN:00/input/input1
		  │ input:input1 &quot;Power Button&quot;
		  ├─/sys/devices/LNXSYSTM:00/LNXSYBUS:00/PNP0A08:00/LNXVIDEO:00/input/input14
		  │ input:input14 &quot;Video Bus&quot;
		  ├─/sys/devices/LNXSYSTM:00/LNXSYBUS:00/PNP0C0C:00/input/input0
		  │ input:input0 &quot;Power Button&quot;
		  ├─/sys/devices/pci0000:00/0000:00:02.0/drm/card0
		  │ [MASTER] drm:card0
		  │ ├─/sys/devices/pci0000:00/0000:00:02.0/drm/card0/card0-DP-1
		  │ │ [MASTER] drm:card0-DP-1
		  │ ├─/sys/devices/pci0000:00/0000:00:02.0/drm/card0/card0-HDMI-A-1
		  │ │ [MASTER] drm:card0-HDMI-A-1
		  │ ├─/sys/devices/pci0000:00/0000:00:02.0/drm/card0/card0-HDMI-A-2
		  │ │ [MASTER] drm:card0-HDMI-A-2
		  │ └─/sys/devices/pci0000:00/0000:00:02.0/drm/card0/card0-eDP-1
		  │   [MASTER] drm:card0-eDP-1
		  │   └─/sys/devices/pci0000:00/0000:00:02.0/drm/card0/card0-eDP-1/intel_backlight
		  │     backlight:intel_backlight
		  ├─/sys/devices/pci0000:00/0000:00:02.0/graphics/fb0
		  │ graphics:fb0 &quot;i915drmfb&quot;
		  ├─/sys/devices/pci0000:00/0000:00:03.0/sound/card1
		  │ sound:card1 &quot;HDMI&quot;
		  │ ├─/sys/devices/pci0000:00/0000:00:03.0/sound/card1/input16
		  │ │ input:input16 &quot;HDA Intel HDMI HDMI/DP,pcm=3&quot;
		  │ ├─/sys/devices/pci0000:00/0000:00:03.0/sound/card1/input17
		  │ │ input:input17 &quot;HDA Intel HDMI HDMI/DP,pcm=7&quot;
		  │ ├─/sys/devices/pci0000:00/0000:00:03.0/sound/card1/input18
		  │ │ input:input18 &quot;HDA Intel HDMI HDMI/DP,pcm=8&quot;
		  │ ├─/sys/devices/pci0000:00/0000:00:03.0/sound/card1/input19
		  │ │ input:input19 &quot;HDA Intel HDMI HDMI/DP,pcm=9&quot;
		  │ └─/sys/devices/pci0000:00/0000:00:03.0/sound/card1/input20
		  │   input:input20 &quot;HDA Intel HDMI HDMI/DP,pcm=10&quot;
		  ├─/sys/devices/pci0000:00/0000:00:14.0/usb2
		  │ usb:usb2
		  │ └─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-3
		  │   usb:2-3
		  │   └─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-3/2-3:1.2/0003:046D:C52B.0003/0003:046D:404D.0004/input/input15
		  │     input:input15 &quot;Logitech K400 Plus&quot;
		  │     ├─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-3/2-3:1.2/0003:046D:C52B.0003/0003:046D:404D.0004/input/input15/input15::capslock
		  │     │ leds:input15::capslock
		  │     ├─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-3/2-3:1.2/0003:046D:C52B.0003/0003:046D:404D.0004/input/input15/input15::compose
		  │     │ leds:input15::compose
		  │     ├─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-3/2-3:1.2/0003:046D:C52B.0003/0003:046D:404D.0004/input/input15/input15::kana
		  │     │ leds:input15::kana
		  │     ├─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-3/2-3:1.2/0003:046D:C52B.0003/0003:046D:404D.0004/input/input15/input15::numlock
		  │     │ leds:input15::numlock
		  │     └─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-3/2-3:1.2/0003:046D:C52B.0003/0003:046D:404D.0004/input/input15/input15::scrolllock
		  │       leds:input15::scrolllock
		  ├─/sys/devices/pci0000:00/0000:00:14.0/usb3
		  │ usb:usb3
		  ├─/sys/devices/pci0000:00/0000:00:1b.0/sound/card0
		  │ sound:card0 &quot;PCH&quot;
		  │ └─/sys/devices/pci0000:00/0000:00:1b.0/sound/card0/input8
		  │   input:input8 &quot;HDA Intel PCH Headphone&quot;
		  ├─/sys/devices/pci0000:00/0000:00:1d.0/usb1
		  │ usb:usb1
		  │ └─/sys/devices/pci0000:00/0000:00:1d.0/usb1/1-1
		  │   usb:1-1
		  └─/sys/devices/platform/pcspkr/input/input7
		    input:input7 &quot;PC Speaker&quot;
</code></pre><p>找寻对应的口：</p><pre><code>/sys/devices/pci0000:00/0000:00:02.0/drm/card0/card0-eDP-1
/sys/devices/pci0000:00/0000:00:14.0/usb2/2-2/2-2.3

/sys/devices/pci0000:00/0000:00:14.0/usb2/2-3

/sys/devices/pci0000:00/0000:00:02.0/drm/card0/card0-HDMI-A-1

</code></pre><p>Attach:</p><pre><code># loginctl attach seat1  /sys/devices/pci0000:00/0000:00:02.0/drm/card0/card0-eDP-1
# loginctl attach seat1  /sys/devices/pci0000:00/0000:00:14.0/usb2/2-2/2-2.3
➜  ~ loginctl seat-status seat1
seat1
	 Devices:
		  ├─/sys/devices/pci0000:00/0000:00:02.0/drm/card0/card0-eDP-1
		  │ [MASTER] drm:card0-eDP-1
		  │ └─/sys/devices/pci0000:00/0000:00:02.0/drm/card0/card0-eDP-1/intel_backlight
		  │   backlight:intel_backlight
		  └─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-2/2-2.3
		    usb:2-2.3
		    ├─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-2/2-2.3/2-2.3:1.0/0003:1A81:2019.0005/input/input10
		    │ input:input10 &quot;G-Tech Fuhlen SM680 Mechanical Keyboard&quot;
		    │ ├─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-2/2-2.3/2-2.3:1.0/0003:1A81:2019.0005/input/input10/input10::capslock
		    │ │ leds:input10::capslock
		    │ ├─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-2/2-2.3/2-2.3:1.0/0003:1A81:2019.0005/input/input10/input10::compose
		    │ │ leds:input10::compose
		    │ ├─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-2/2-2.3/2-2.3:1.0/0003:1A81:2019.0005/input/input10/input10::kana
		    │ │ leds:input10::kana
		    │ ├─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-2/2-2.3/2-2.3:1.0/0003:1A81:2019.0005/input/input10/input10::numlock
		    │ │ leds:input10::numlock
		    │ └─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-2/2-2.3/2-2.3:1.0/0003:1A81:2019.0005/input/input10/input10::scrolllock
		    │   leds:input10::scrolllock
		    ├─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-2/2-2.3/2-2.3:1.1/0003:1A81:2019.0006/input/input11
		    │ input:input11 &quot;G-Tech Fuhlen SM680 Mechanical Keyboard Mouse&quot;
		    ├─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-2/2-2.3/2-2.3:1.1/0003:1A81:2019.0006/input/input12
		    │ input:input12 &quot;G-Tech Fuhlen SM680 Mechanical Keyboard&quot;
		    ├─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-2/2-2.3/2-2.3:1.1/0003:1A81:2019.0006/input/input13
		    │ input:input13 &quot;G-Tech Fuhlen SM680 Mechanical Keyboard Consumer Control&quot;
		    ├─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-2/2-2.3/2-2.3:1.1/0003:1A81:2019.0006/input/input14
		    │ input:input14 &quot;G-Tech Fuhlen SM680 Mechanical Keyboard System Control&quot;
		    ├─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-2/2-2.3/2-2.3:1.2/0003:1A81:2019.0007/input/input16
		    │ input:input16 &quot;G-Tech Fuhlen SM680 Mechanical Keyboard&quot;
		    └─/sys/devices/pci0000:00/0000:00:14.0/usb2/2-2/2-2.3/2-2.3:1.3/0003:1A81:2019.0008/input/input17
		      input:input17 &quot;G-Tech Fuhlen SM680 Mechanical Keyboard&quot;
</code></pre><p>Verify:</p><pre><code>➜  ~ ls -l  /etc/udev/rules.d/
total 12
-rw-r--r-- 1 root root  76 Jul  6 06:22 72-seat-drm-pci-0000_00_02_0.rules
-rw-r--r-- 1 root root  86 Jul  6 06:23 72-seat-usb-pci-0000_00_14_0-usb-0_2_3.rules
-rw-r--r-- 1 root root 432 Aug 10  2020 99-kvmd.rules.pacsave

</code></pre><p>Disable lxdm and testing:</p><pre><code># systemctl disable lxdm
Removed /etc/systemd/system/display-manager.service.
# reboot
</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2021/07/04/workingtipsonlxdjuju/>WorkingTipsOnLXDJuju</a></h1><span class=post-date>Jul 4, 2021<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=commands>Commands</h3><p>Bootstrap via:</p><pre><code>test@edge5:~$ juju bootstrap localhost overlord
Creating Juju controller &quot;overlord&quot; on localhost/localhost
Looking for packaged Juju agent version 2.9.5 for amd64
Located Juju agent version 2.9.5-ubuntu-amd64 at https://streams.canonical.com/juju/tools/agent/2.9.5/juju-2.9.5-ubuntu-amd64.tgz
To configure your system to better support LXD containers, please see: https://github.com/lxc/lxd/blob/master/doc/production-setup.md
Launching controller instance(s) on localhost/localhost...
 - juju-55e209-0 (arch=amd64)                 
Installing Juju agent on bootstrap instance
Fetching Juju Dashboard 0.7.1
Waiting for address
Attempting to connect to 10.53.118.136:22
Connected to 10.53.118.136
Running machine configuration script...
Bootstrap agent now started
Contacting Juju controller at 10.53.118.136 to verify accessibility...

Bootstrap complete, controller &quot;overlord&quot; is now available
Controller machines are in the &quot;controller&quot; model
Initial model &quot;default&quot; added
</code></pre><p>Verify bootstrap status:</p><pre><code>test@edge5:~$ lxc ls
+---------------+---------+----------------------+------+-----------+-----------+
|     NAME      |  STATE  |         IPV4         | IPV6 |   TYPE    | SNAPSHOTS |
+---------------+---------+----------------------+------+-----------+-----------+
| juju-55e209-0 | RUNNING | 10.53.118.136 (eth0) |      | CONTAINER | 0         |
+---------------+---------+----------------------+------+-----------+-----------+
test@edge5:~$ juju status
Model    Controller  Cloud/Region         Version  SLA          Timestamp
default  overlord    localhost/localhost  2.9.5    unsupported  15:32:12+08:00

Model &quot;admin/default&quot; is empty.
</code></pre><h3 id=microk8s>microk8s</h3><p>Deploy microk8s via:</p><pre><code>$ juju deploy -n3 cs:~pjdc/microk8s
Located charm &quot;microk8s&quot; in charm-store, revision 24
Deploying &quot;microk8s&quot; from charm-store charm &quot;microk8s&quot;, revision 24 in channel stable
</code></pre><p>View juju status:</p><pre><code>test@edge5:~$ juju status
Model    Controller  Cloud/Region         Version  SLA          Timestamp
default  overlord    localhost/localhost  2.9.5    unsupported  15:35:54+08:00

App       Version  Status   Scale  Charm     Store       Channel  Rev  OS      Message
microk8s           waiting    0/3  microk8s  charmstore  stable    24  ubuntu  waiting for machine

Unit        Workload  Agent       Machine  Public address  Ports  Message
microk8s/0  waiting   allocating  0        10.53.118.110          waiting for machine
microk8s/1  waiting   allocating  1        10.53.118.99           waiting for machine
microk8s/2  waiting   allocating  2        10.53.118.115          waiting for machine

Machine  State    DNS            Inst id        Series  AZ  Message
0        pending  10.53.118.110  juju-585d2d-0  focal       Running
1        pending  10.53.118.99   juju-585d2d-1  focal       Running
2        pending  10.53.118.115  juju-585d2d-2  focal       Running
</code></pre><p>Until succeed:</p><pre><code>test@edge5:~$ juju status
Model    Controller  Cloud/Region         Version  SLA          Timestamp
default  overlord    localhost/localhost  2.9.5    unsupported  15:49:48+08:00

App       Version  Status  Scale  Charm     Store       Channel  Rev  OS      Message
microk8s           active      3  microk8s  charmstore  stable    24  ubuntu  

Unit         Workload  Agent  Machine  Public address  Ports                     Message
microk8s/0*  active    idle   0        10.53.118.110   80/tcp,443/tcp,16443/tcp  
microk8s/1   active    idle   1        10.53.118.99    80/tcp,443/tcp,16443/tcp  
microk8s/2   active    idle   2        10.53.118.115   80/tcp,443/tcp,16443/tcp  

Machine  State    DNS            Inst id        Series  AZ  Message
0        started  10.53.118.110  juju-585d2d-0  focal       Running
1        started  10.53.118.99   juju-585d2d-1  focal       Running
2        started  10.53.118.115  juju-585d2d-2  focal       Running
</code></pre><h3 id=own-cloud>own cloud</h3><p>Ip and hostname listed as:</p><pre><code>192.168.89.6	edge5
192.168.89.7	edge6
192.168.89.8	edge7
192.168.89.9	edge8
192.168.89.10	edge9
</code></pre><p>Added :</p><pre><code>test@edge5:~$ ssh-copy-id test@192.168.89.7
test@edge5:~$ juju add-cloud                                                                                                                                  
This operation can be applied to both a copy on this client and to the one on a controller.
No current controller was detected and there are no registered controllers on this client: either bootstrap one or register one.
Cloud Types
  lxd
  maas
  manual
  openstack
  vsphere

Select cloud type: manual

Enter a name for your manual cloud: manual-cloud

Enter the ssh connection string for controller, username@&lt;hostname or IP&gt; or &lt;hostname or IP&gt;: test@192.168.89.7

Cloud &quot;manual-cloud&quot; successfully added to your local client.
</code></pre><p>verify clouds available:</p><pre><code>test@edge5:~$ juju clouds
Only clouds with registered credentials are shown.
There are more clouds, use --all to see them.
You can bootstrap a new controller using one of these clouds...

Clouds available on the client:
Cloud         Regions  Default    Type    Credentials  Source    Description
localhost     1        localhost  lxd     0            built-in  LXD Container Hypervisor
manual-cloud  1        default    manual  0            local 
</code></pre><p>Now bootstrap the <code>manual-cloud</code>:</p><pre><code>$ juju bootstrap manual-cloud
</code></pre><p>Add machines:</p><pre><code>test@edge5:~$ juju add-machine ssh:test@192.168.89.8
created machine 0
test@edge5:~$ juju add-machine ssh:test@192.168.89.9 &amp;&amp; juju add-machine ssh:test@192.168.89.10
created machine 1
created machine 2
test@edge5:~$ juju machines
Machine  State    DNS            Inst id               Series  AZ  Message
0        started  192.168.89.8   manual:192.168.89.8   focal       Manually provisioned machine
1        started  192.168.89.9   manual:192.168.89.9   focal       Manually provisioned machine
2        started  192.168.89.10  manual:192.168.89.10  focal       Manually provisioned machine
</code></pre><p>Deploy microk8s via:</p><pre><code>$  juju deploy -n3 cs:~pjdc/microk8s
</code></pre><p>After deployment, the juju status show:</p><pre><code>test@edge5:~$ juju status
Model    Controller            Cloud/Region          Version  SLA          Timestamp
default  manual-cloud-default  manual-cloud/default  2.9.5    unsupported  17:27:04+08:00

App       Version  Status  Scale  Charm     Store       Channel  Rev  OS      Message
microk8s           active      3  microk8s  charmstore  stable    24  ubuntu  

Unit         Workload  Agent  Machine  Public address  Ports                     Message
microk8s/0*  active    idle   0        192.168.89.8    80/tcp,443/tcp,16443/tcp  
microk8s/1   active    idle   1        192.168.89.9    80/tcp,443/tcp,16443/tcp  
microk8s/2   active    idle   2        192.168.89.10   80/tcp,443/tcp,16443/tcp  

Machine  State    DNS            Inst id               Series  AZ  Message
0        started  192.168.89.8   manual:192.168.89.8   focal       Manually provisioned machine
1        started  192.168.89.9   manual:192.168.89.9   focal       Manually provisioned machine
2        started  192.168.89.10  manual:192.168.89.10  focal       Manually provisioned machine

</code></pre><p>Verify Ha:</p><pre><code>test@edge5:~$ juju exec --application microk8s -- 'microk8s status | grep -A2 high-availability:'
- return-code: 0
  stdout: |
    high-availability: yes
      datastore master nodes: 192.168.89.8:19001 192.168.89.9:19001 192.168.89.10:19001
      datastore standby nodes: none
  unit: microk8s/0
- return-code: 0
  stdout: |
    high-availability: yes
      datastore master nodes: 192.168.89.8:19001 192.168.89.9:19001 192.168.89.10:19001
      datastore standby nodes: none
  unit: microk8s/1
- return-code: 0
  stdout: |
    high-availability: yes
      datastore master nodes: 192.168.89.8:19001 192.168.89.9:19001 192.168.89.10:19001
      datastore standby nodes: none
  unit: microk8s/2

</code></pre><p>Verify the k8s status:</p><pre><code>test@edge5:~$ juju exec --application microk8s -- microk8s kubectl get node
- return-code: 0
  stdout: |
    NAME    STATUS   ROLES    AGE   VERSION
    edge8   Ready    &lt;none&gt;   13m   v1.21.1-3+ba118484dd39df
    edge7   Ready    &lt;none&gt;   18m   v1.21.1-3+ba118484dd39df
    edge9   Ready    &lt;none&gt;   13m   v1.21.1-3+ba118484dd39df
  unit: microk8s/0
- return-code: 0
  stdout: |
    NAME    STATUS   ROLES    AGE   VERSION
    edge8   Ready    &lt;none&gt;   13m   v1.21.1-3+ba118484dd39df
    edge7   Ready    &lt;none&gt;   18m   v1.21.1-3+ba118484dd39df
    edge9   Ready    &lt;none&gt;   13m   v1.21.1-3+ba118484dd39df
  unit: microk8s/1
- return-code: 0
  stdout: |
    NAME    STATUS   ROLES    AGE   VERSION
    edge8   Ready    &lt;none&gt;   13m   v1.21.1-3+ba118484dd39df
    edge7   Ready    &lt;none&gt;   18m   v1.21.1-3+ba118484dd39df
    edge9   Ready    &lt;none&gt;   13m   v1.21.1-3+ba118484dd39df
  unit: microk8s/2
</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2021/07/03/workingtipsonlxdandjuju/>WorkingTipsOnLXDAndJuju</a></h1><span class=post-date>Jul 3, 2021<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=before>Before</h3><p>A LXD cluster and juju status:</p><p>lxd cluster status and lxc instance before deploy work load:</p><pre><code>test@freeedge1:~$ lxc cluster list
+-----------+---------------------------+----------+--------------+----------------+-------------+--------+-------------------+
|   NAME    |            URL            | DATABASE | ARCHITECTURE | FAILURE DOMAIN | DESCRIPTION | STATE  |      MESSAGE      |
+-----------+---------------------------+----------+--------------+----------------+-------------+--------+-------------------+
| freeedge1 | https://192.168.89.2:8443 | YES      | x86_64       | default        |             | ONLINE | Fully operational |
+-----------+---------------------------+----------+--------------+----------------+-------------+--------+-------------------+
| freeedge2 | https://192.168.89.3:8443 | YES      | x86_64       | default        |             | ONLINE | Fully operational |
+-----------+---------------------------+----------+--------------+----------------+-------------+--------+-------------------+
| freeedge3 | https://192.168.89.4:8443 | YES      | x86_64       | default        |             | ONLINE | Fully operational |
+-----------+---------------------------+----------+--------------+----------------+-------------+--------+-------------------+
| freeedge4 | https://192.168.89.5:8443 | YES      | x86_64       | default        |             | ONLINE | Fully operational |
+-----------+---------------------------+----------+--------------+----------------+-------------+--------+-------------------+
test@freeedge1:~$ lxc ls
+------+-------+------+------+------+-----------+----------+
| NAME | STATE | IPV4 | IPV6 | TYPE | SNAPSHOTS | LOCATION |
+------+-------+------+------+------+-----------+----------+
</code></pre><p><code>juju boostrap</code> for provisioning a machine with LXD and create a controller
running within it:</p><pre><code>$ juju bootstrap localhost overlord
Creating Juju controller &quot;overlord&quot; on localhost/localhost
Looking for packaged Juju agent version 2.9.5 for amd64
WARNING Got error requesting &quot;https://streams.canonical.com/juju/tools/streams/v1/index2.sjson&quot;: Get &quot;https://streams.canonical.com/juju/tools/streams/v1/index2.sjson&quot;: dial tcp [2001:67c:1360:8001::33]:443: connect: network is unreachable
No packaged binary found, preparing local Juju agent binary
To configure your system to better support LXD containers, please see: https://github.com/lxc/lxd/blob/master/doc/production-setup.md
Launching controller instance(s) on localhost/localhost...
 - juju-5a91a2-0 (arch=amd64)                 
Installing Juju agent on bootstrap instance
Fetching Juju Dashboard 0.7.1
Waiting for address
Attempting to connect to 192.168.89.155:22
Connected to 192.168.89.155
Running machine configuration script...
Bootstrap agent now started
Contacting Juju controller at 192.168.89.155 to verify accessibility...

Bootstrap complete, controller &quot;overlord&quot; is now available
Controller machines are in the &quot;controller&quot; model
Initial model &quot;default&quot; added
</code></pre><p>This command automatically create a new lxd instance and running the script in
it, like:</p><pre><code>$ test@freeedge1:~$ lxc ls
+---------------+---------+-----------------------+------+-----------+-----------+-----------+
|     NAME      |  STATE  |         IPV4          | IPV6 |   TYPE    | SNAPSHOTS | LOCATION  |
+---------------+---------+-----------------------+------+-----------+-----------+-----------+
| juju-5a91a2-0 | RUNNING | 192.168.89.155 (eth0) |      | CONTAINER | 0         | freeedge1 |
+---------------+---------+-----------------------+------+-----------+-----------+-----------+
</code></pre><p>添加juju machines:</p><pre><code>test@freeedge1:~$ juju add-machine -n 2
created machine 0
created machine 1
test@freeedge1:~$ juju machines
Machine  State    DNS  Inst id  Series  AZ  Message
0        pending       pending  focal       starting
1        pending       pending  focal       starting
</code></pre><p>等待一段时间，直到状态变为<code>started</code>:</p><pre><code>$ juju machines
Machine  State    DNS             Inst id        Series  AZ  Message
0        started  192.168.89.197  juju-a5a008-0  focal       Running
1        started  192.168.89.173  juju-a5a008-1  focal       Running
</code></pre><p>实际上是lxc工作负载，exec进入到该实例中可发现其实是没有做任何资源限制的lxc工作实例, 如果资源需要做限制，则可以参考<code>https://juju.is/docs/olm/constraints#heading--constraints-and-lxd-containers</code>:</p><pre><code>test@freeedge1:~$ lxc ls
+---------------+---------+-----------------------+------+-----------+-----------+-----------+
|     NAME      |  STATE  |         IPV4          | IPV6 |   TYPE    | SNAPSHOTS | LOCATION  |
+---------------+---------+-----------------------+------+-----------+-----------+-----------+
| juju-5a91a2-0 | RUNNING | 192.168.89.155 (eth0) |      | CONTAINER | 0         | freeedge1 |
+---------------+---------+-----------------------+------+-----------+-----------+-----------+
| juju-a5a008-0 | RUNNING | 192.168.89.197 (eth0) |      | CONTAINER | 0         | freeedge2 |
+---------------+---------+-----------------------+------+-----------+-----------+-----------+
| juju-a5a008-1 | RUNNING | 192.168.89.173 (eth0) |      | CONTAINER | 0         | freeedge2 |
+---------------+---------+-----------------------+------+-----------+-----------+-----------+

</code></pre><p>销毁刚才创建的machine:</p><pre><code>test@freeedge1:~$ juju remove-machine 0
removing machine 0
test@freeedge1:~$ juju machines
Machine  State    DNS             Inst id        Series  AZ  Message
0        stopped  192.168.89.197  juju-a5a008-0  focal       Running
1        started  192.168.89.173  juju-a5a008-1  focal       Running

test@freeedge1:~$ juju remove-machine 1
removing machine 1
test@freeedge1:~$ juju machines
Machine  State    DNS             Inst id        Series  AZ  Message
1        stopped  192.168.89.173  juju-a5a008-1  focal       Running
</code></pre><p>创建一个<code>hello-juju</code>的charmed operator:</p><pre><code>test@freeedge1:~$ juju deploy hello-juju
Located charm &quot;hello-juju&quot; in charm-hub, revision 8
Deploying &quot;hello-juju&quot; from charm-hub charm &quot;hello-juju&quot;, revision 8 in channel stable
test@freeedge1:~$ juju status
Model    Controller  Cloud/Region         Version  SLA          Timestamp
default  overlord    localhost/localhost  2.9.5    unsupported  16:27:44+08:00

App         Version  Status  Scale  Charm       Store     Channel  Rev  OS      Message
hello-juju           active      1  hello-juju  charmhub  stable     8  ubuntu  

Unit           Workload  Agent  Machine  Public address  Ports   Message
hello-juju/0*  active    idle   2        192.168.89.160  80/tcp  

Machine  State    DNS             Inst id        Series  AZ  Message
2        started  192.168.89.160  juju-a5a008-2  focal       Running

test@freeedge1:~$ juju expose hello-juju
test@freeedge1:~$ juju status
Model    Controller  Cloud/Region         Version  SLA          Timestamp
default  overlord    localhost/localhost  2.9.5    unsupported  16:28:06+08:00

App         Version  Status  Scale  Charm       Store     Channel  Rev  OS      Message
hello-juju           active      1  hello-juju  charmhub  stable     8  ubuntu  

Unit           Workload  Agent  Machine  Public address  Ports   Message
hello-juju/0*  active    idle   2        192.168.89.160  80/tcp  

Machine  State    DNS             Inst id        Series  AZ  Message
2        started  192.168.89.160  juju-a5a008-2  focal       Running
</code></pre><p><img src=/images/2021_07_03_16_31_31_680x551.jpg alt=/images/2021_07_03_16_31_31_680x551.jpg></p><h3 id=部署charmed-kubernetes>部署charmed Kubernetes</h3><p>记录一下步骤, 在LXD集群的情况下，直接部署会出现以下错误:</p><pre><code># juju deploy charmed-kubernetes
# juju status
Machine  State  DNS  Inst id  Series  AZ  Message
0        down        pending  focal       Failed creating instance record: Failed initialising instance: Failed loading storage pool: No such object
1        down        pending  focal       Failed creating instance record: Failed initialising instance: Failed loading storage pool: No such object
2        down        pending  focal       Failed creating instance record: Failed initialising instance: Failed loading storage pool: No such object
3        down        pending  focal       Failed creating instance record: Failed initialising instance: Failed loading storage pool: No such object
....
</code></pre><p>这是因为没有默认的<code>default</code>格式的storage定义，</p><pre><code>test@freeedge1:~$ lxc storage list
+-------+--------+-------------+---------+---------+
| NAME  | DRIVER | DESCRIPTION | USED BY |  STATE  |
+-------+--------+-------------+---------+---------+
| local | zfs    |             | 3       | CREATED |
+-------+--------+-------------+---------+---------+
</code></pre><p>先行删除已经部署好的<code>charmed-kubernetes</code>（当前没有直接删除charmed
operator的方法，只能将app都删除):</p><pre><code> juju remove-application hello-juju
 juju remove-application containerd
 juju remove-application easyrsa
 juju remove-application etcd
 juju remove-application flannel
 juju remove-application kubeapi-load-balancer
 juju remove-application kubernetes-master
 juju remove-application kubernetes-worker
</code></pre><p>所有节点上创建相同的目录并添加到一个新的storage定义:</p><pre><code> $ ansible -i hosts.ini all -m shell -a &quot;sudo mkdir -p /data/lxd &amp;&amp; sudo chmod 777 -R /data/lxd&quot;
$ lxc storage create --target freeedge1 default dir source=/data/lxd 
Storage pool default pending on member freeedge1
$ lxc storage create --target freeedge2 default dir source=/data/lxd 
Storage pool default pending on member freeedge2
$ lxc storage create --target freeedge3 default dir source=/data/lxd 
Storage pool default pending on member freeedge3
$ lxc storage create --target freeedge4 default dir source=/data/lxd 
Storage pool default pending on member freeedge4
$ lxc storage create default dir
Storage pool default created
$ lxc storage volume create default lxdvol --target freeedge1
Storage volume lxdvol created
$ lxc storage volume show default lxdvol --target freeedge1
config: {}
description: &quot;&quot;
name: lxdvol
type: custom
used_by: []
location: freeedge1
content_type: filesystem
</code></pre><p>各种乱七八糟的操作以后，清空:</p><pre><code>juju destroy-controller overlord
</code></pre><p>现在重新开始部署juju(参考<code>https://juju.is/docs/olm/lxd</code>):</p><pre><code>$ juju bootstrap localhost overlord
$ juju deploy charmed-kubernetes
Located bundle &quot;charmed-kubernetes&quot; in charm-hub, revision 679
WARNING &quot;services&quot; key found in bundle file is deprecated, superseded by &quot;applications&quot; key.
Located charm &quot;containerd&quot; in charm-store, revision 130
Located charm &quot;easyrsa&quot; in charm-store, revision 384
Located charm &quot;etcd&quot; in charm-store, revision 594
Located charm &quot;flannel&quot; in charm-store, revision 558
Located charm &quot;kubeapi-load-balancer&quot; in charm-store, revision 798
Located charm &quot;kubernetes-master&quot; in charm-store, revision 1008
Located charm &quot;kubernetes-worker&quot; in charm-store, revision 768
Executing changes:
- upload charm containerd from charm-store for series focal with architecture=amd64
- deploy application containerd from charm-store on focal
- set annotations for containerd
- upload charm easyrsa from charm-store for series focal with architecture=amd64
- deploy application easyrsa from charm-store on focal
  added resource easyrsa
- set annotations for easyrsa
- upload charm etcd from charm-store for series focal with architecture=amd64
- deploy application etcd from charm-store on focal
  added resource core
  added resource etcd
  added resource snapshot
- set annotations for etcd
- upload charm flannel from charm-store for series focal with architecture=amd64
- deploy application flannel from charm-store on focal
  added resource flannel-amd64
  added resource flannel-arm64
  added resource flannel-s390x
- set annotations for flannel
- upload charm kubeapi-load-balancer from charm-store for series focal with architecture=amd64
- deploy application kubeapi-load-balancer from charm-store on focal
- expose all endpoints of kubeapi-load-balancer and allow access from CIDRs 0.0.0.0/0 and ::/0
- set annotations for kubeapi-load-balancer
- upload charm kubernetes-master from charm-store for series focal with architecture=amd64
- deploy application kubernetes-master from charm-store on focal
  added resource cdk-addons
  added resource core
  added resource kube-apiserver
  added resource kube-controller-manager
  added resource kube-proxy
  added resource kube-scheduler
  added resource kubectl
- set annotations for kubernetes-master
- upload charm kubernetes-worker from charm-store for series focal with architecture=amd64
- deploy application kubernetes-worker from charm-store on focal
  added resource cni-amd64
  added resource cni-arm64
  added resource cni-s390x
  added resource core
  added resource kube-proxy
  added resource kubectl
  added resource kubelet
- expose all endpoints of kubernetes-worker and allow access from CIDRs 0.0.0.0/0 and ::/0
- set annotations for kubernetes-worker
- add relation kubernetes-master:kube-api-endpoint - kubeapi-load-balancer:apiserver
- add relation kubernetes-master:loadbalancer - kubeapi-load-balancer:loadbalancer
- add relation kubernetes-master:kube-control - kubernetes-worker:kube-control
- add relation kubernetes-master:certificates - easyrsa:client
- add relation etcd:certificates - easyrsa:client
- add relation kubernetes-master:etcd - etcd:db
- add relation kubernetes-worker:certificates - easyrsa:client
- add relation kubernetes-worker:kube-api-endpoint - kubeapi-load-balancer:website
- add relation kubeapi-load-balancer:certificates - easyrsa:client
- add relation flannel:etcd - etcd:db
- add relation flannel:cni - kubernetes-master:cni
- add relation flannel:cni - kubernetes-worker:cni
- add relation containerd:containerd - kubernetes-worker:container-runtime
- add relation containerd:containerd - kubernetes-master:container-runtime
- add unit easyrsa/0 to new machine 0
- add unit etcd/0 to new machine 1
- add unit etcd/1 to new machine 2
- add unit etcd/2 to new machine 3
- add unit kubeapi-load-balancer/0 to new machine 4
- add unit kubernetes-master/0 to new machine 5
- add unit kubernetes-master/1 to new machine 6
- add unit kubernetes-worker/0 to new machine 7
- add unit kubernetes-worker/1 to new machine 8
- add unit kubernetes-worker/2 to new machine 9
Deploy of bundle completed.
</code></pre><p>部署中可以通过<code>juju status</code>看即时的状态变更:</p><p><img src=/images/2021_07_03_17_51_14_1359x899.jpg alt=/images/2021_07_03_17_51_14_1359x899.jpg></p><p>涉及到的落地实体:</p><p><img src=/images/2021_07_03_17_51_58_956x946.jpg alt=/images/2021_07_03_17_51_58_956x946.jpg></p><p>安装<code>kubectl</code>用于管控集群:</p><pre><code>test@freeedge1:~$ sudo snap install kubectl --classic
.....
kubectl 1.21.1 from Canonical✓ installed
</code></pre><p>中间状态：</p><p><img src=/images/2021_07_03_18_00_15_1526x913.jpg alt=/images/2021_07_03_18_00_15_1526x913.jpg></p><p><img src=/images/2021_07_03_18_07_12_1525x940.jpg alt=/images/2021_07_03_18_07_12_1525x940.jpg></p><p><img src=/images/2021_07_03_18_09_07_1489x922.jpg alt=/images/2021_07_03_18_09_07_1489x922.jpg></p><p>集群就绪：</p><p><img src=/images/2021_07_03_18_40_34_1384x934.jpg alt=/images/2021_07_03_18_40_34_1384x934.jpg></p><pre><code>$ juju scp kubernetes-master/0:config ~/.kube/config
$ kubectl get nodes -A
NAME            STATUS   ROLES    AGE   VERSION
juju-08ae08-7   Ready    &lt;none&gt;   19m   v1.21.1
juju-08ae08-8   Ready    &lt;none&gt;   19m   v1.21.1
juju-08ae08-9   Ready    &lt;none&gt;   13m   v1.21.1

</code></pre><p><img src=/images/2021_07_03_18_42_18_1481x912.jpg alt=/images/2021_07_03_18_42_18_1481x912.jpg></p><p>添加节点:</p><pre><code>$ juju add-unit kubernetes-worker
</code></pre><p><img src=/images/2021_07_03_18_43_47_709x158.jpg alt=/images/2021_07_03_18_43_47_709x158.jpg></p><p>增加完后：</p><pre><code>$ kubectl get nodes -A                                                                                                                                                      
NAME             STATUS   ROLES    AGE   VERSION
juju-08ae08-10   Ready    &lt;none&gt;   12m   v1.21.1
juju-08ae08-7    Ready    &lt;none&gt;   42m   v1.21.1
juju-08ae08-8    Ready    &lt;none&gt;   42m   v1.21.1
juju-08ae08-9    Ready    &lt;none&gt;   36m   v1.21.1
</code></pre><p>增加3个:</p><pre><code>$ juju add-unit kubernetes-worker -n 3
$ juju status
.....
11       pending                  pending         focal       starting
12       pending                  pending         focal       starting
13       pending                  pending         focal       starting
$ kubectl get nodes -A
NAME             STATUS   ROLES    AGE     VERSION
juju-08ae08-10   Ready    &lt;none&gt;   29m     v1.21.1
juju-08ae08-11   Ready    &lt;none&gt;   5m2s    v1.21.1
juju-08ae08-12   Ready    &lt;none&gt;   3m21s   v1.21.1
juju-08ae08-13   Ready    &lt;none&gt;   86s     v1.21.1
juju-08ae08-7    Ready    &lt;none&gt;   59m     v1.21.1
juju-08ae08-8    Ready    &lt;none&gt;   59m     v1.21.1
juju-08ae08-9    Ready    &lt;none&gt;   53m     v1.21.1

</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2021/07/01/workingtipsonfreerouter/>WorkingTipsOnFreeRouter</a></h1><span class=post-date>Jul 1, 2021<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=1-环境>1. 环境</h3><p>物理机，全隔离环境，可通过隧道开通的proxy(10.xx.xx.xxx:8118)访问互联网.<br>终极目标：构建一个可自由连接互联网的网段用于虚拟机及虚拟网络方案的验证。</p><h3 id=2-配置步骤>2. 配置步骤</h3><h4 id=21-准备router虚拟机>2.1 准备Router虚拟机</h4><p>virt-manager内创建一个网段为<code>192.168.89.0/24</code>, dhcp关闭，转发模式。而后创建一台ubuntu20.04的虚拟机，配置为1核1G，使用该网络，启动。</p><p>启动后配置其网络地址为<code>192.168.89.2/24</code>:</p><pre><code>$ cat /etc/netplan/01-netcfg.yaml 
    # This file describes the network interfaces available on your system
    # For more information, see netplan(5).
    network:
      version: 2
      renderer: networkd
      ethernets:
        eth0:
          dhcp4: no
      bridges: 
        br0:
          dhcp4: no
          addresses: [ 192.168.89.2/24 ]
          gateway4: 192.168.100.1
          interfaces:
             - eth0
$ sudo netplan apply
$ sudo hostnamectl set-hostname router
$ sudo vim /etc/hosts
....
127.0.1.1       router  router
....
</code></pre><p>因为<code>192.168.89.0/24</code>网段内无dncp服务器，dns服务器，因此我们希望Router这台机器用于接管整个网段的地址分配。这个网段我们预留前50个地址用于静态IP使用, 后面的<code>192.168.89.51～192.168.89.254</code>用于dhcp段地址分配。</p><p>dnsmasq安装及配置:</p><pre><code>$ sudo apt-get install -y dnsmasq
$ sudo systemctl stop systemd-resolved.service
$ sudo systemctl disable systemd-resolved.service
$ sudo systemctl restart dnsmasq
$ sudo  vim /etc/dnsmasq.conf
dhcp-range=192.168.89.51,192.168.89.254,12h
dhcp-option=3,192.168.89.2
interface=br0
bind-interfaces
$ sudo systemctl restart dnsmasq
</code></pre><p>此时可以新建一台虚拟机接入该网段用于测试，可以看到该网段的dhcp确实由router承载，且分配了网关为<code>192.168.89.2</code>. 但是此时因为router上未开启流量转发，因而此虚拟机使用<code>192.168.89.2</code>作为网关时不能连接外部网络。</p><p>使能Ip转发：</p><pre><code>$ cat /proc/sys/net/ipv4/ip_forward
0
$ sudo vim /etc/sysctl.conf 
$ sudo sysctl -p
net.ipv4.ip_forward = 1
$ cat /proc/sys/net/ipv4/ip_forward
1
</code></pre><p>做到这里，我突然想到，如果一个局域网段有两个出口，有可能会带来隐患。不排除有的容器实例里某些诡异的操作会从192.168.89.1直接出也不一定。所以我决定彻底删除这个网段，将这个网段的forward属性也去掉，在libvirtd中，这就是一个完全隔离的网段，而Router这台机器我们将它配置成为一个双网卡的机器，而增加的eth1则连接到另一个网段(<code>192.168.100.4</code>)。</p><p><img src=/images/2021_07_01_07_16_52_592x324.jpg alt=/images/2021_07_01_07_16_52_592x324.jpg></p><p><img src=/images/2021_07_01_07_17_26_625x344.jpg alt=/images/2021_07_01_07_17_26_625x344.jpg></p><p>值得注意的是，在更改完isolated网络类型后，虚拟机的配置需要重新确认(libvirtd可能在增删网络配置的过程中将原有的网络回归到default)。</p><p>最终router上的网络及路由配置如下：</p><pre><code>test@router:~$ cat /etc/netplan/01-netcfg.yaml 
# This file describes the network interfaces available on your system
# For more information, see netplan(5).
network:
  version: 2
  renderer: networkd
  ethernets:
    eth0:
      dhcp4: no
      addresses: [ 192.168.89.2/24 ]
      #routes:
      #  - to: 192.168.89.0/24
      #    via: 192.168.89.1
test@router:~$ cat /etc/netplan/02-eth1.yaml 
# This file describes the network interfaces available on your system
# For more information, see netplan(5).
network:
  version: 2
  renderer: networkd
  ethernets:
    eth1:
      dhcp4: no
      addresses: [ 192.168.100.4/24 ]
      gateway4: 192.168.100.1
test@router:~$ route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.100.1   0.0.0.0         UG    0      0        0 eth1
192.168.89.0    0.0.0.0         255.255.255.0   U     0      0        0 eth0
192.168.100.0   0.0.0.0         255.255.255.0   U     0      0        0 eth1
</code></pre><p>重启过程中发现<code>/etc/resolv.conf</code>无效，执行以下操作:</p><pre><code>test@router:/etc$ sudo rm -f resolv.conf 
test@router:/etc$ sudo vim /etc/resolv.conf
nameserver 223.5.5.5
test@router:/etc$ sudo chattr +i /etc/resolv.conf 
</code></pre><p>执行转发：</p><pre><code># sudo apt-get install -y iptables-persistent
# sudo iptables -t nat -A POSTROUTING -s 192.168.89.1/24 ! -d 192.168.89.0/24 -j SNAT --to-source 192.168.100.4
# apt-get install -y libevent-devel
##### get redsocks
# cd /opt/src/redsocks/
# make clean &amp;&amp; make
# cd /opt/src/redsocks
配置
# ./redsocks.sh start
</code></pre><p>这里值得注意的是，redsocks需要配置为socks5代理才可以让本机访问到外面。</p><h3 id=udp转发>udp转发</h3><p>外部机器建立ssh隧道用于转发:</p><pre><code># ssh -o GatewayPorts=true -f -N -T -R \*:18888:localhost:18888 docker@10.xx.xx.xxx
</code></pre><p>外部机器做udp到tcp的<code>socat</code>转发:</p><pre><code># sudo socat tcp-listen:18888,reuseaddr,fork udp:127.0.0.1:53
</code></pre><p>内部机器做tcp到udp的<code>socat</code>转发:</p><pre><code># socat -T15 udp4-recvfrom:53,bind=10.xx.xxx.xxx,fork tcp:localhost:18888
</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2021/06/25/makeipvsadmofflinerpms/>MakeipvsadmOfflineRPMs</a></h1><span class=post-date>Jun 25, 2021<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>现场测试时，测试人员报告在安装ipvsadm时，有包缺失现象。报错现象如下:</p><p><img src=/images/2021_06_26_06_58_40_903x356.jpg alt=/images/2021_06_26_06_58_40_903x356.jpg></p><p>原因是因为在安装ipvsadm时的libnl3依赖更新开始，客户安装的操作系统是centos7.2,而我们做包的系统是>centos7.5以后的，因而导致了libnl3-cli因libnl3的更新抱怨更新后确实依赖而不能进行安装。</p><p>解决方案：</p><p><img src=/images/2021_06_26_06_57_10_1704x154.jpg alt=/images/2021_06_26_06_57_10_1704x154.jpg></p></div><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/42/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/42/>42</a></li><li class="page-item active"><a class=page-link href=/page/43/>43</a></li><li class=page-item><a class=page-link href=/page/44/>44</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/248/>248</a></li><li class=page-item><a href=/page/44/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/248/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div><script src=http://purplepalmdash.github.io/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>