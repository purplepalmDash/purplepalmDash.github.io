<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta name=generator content="Hugo 0.64.0"><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Dash &#183; Dash</title><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124"><link rel=stylesheet href=https://purplepalmdash.github.io/css/highlight/googlecode.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js></script><script type=text/javascript src=/js/html2canvas.js></script><script type=text/javascript>function genPostShot(){var rightNow=new Date();var imageName=rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");imageName+='.jpg'
html2canvas(document.getElementsByClassName('post'),{background:'#FFFFFF',onrendered:function(canvas){$('#test').attr('href',canvas.toDataURL("image/jpeg"));$('#test').attr('download',imageName);$('#test')[0].click();}});};</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124"><link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel=icon><link href=%7balternate%20%7bRSS%20application/rss+xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://purplepalmdash.github.io/index.xml%7d rel=alternate type=application/rss+xml title="Dash &#183; Dash"><meta name=description content><meta name=keywords content="unix,virtualization,embedded,linux"></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><img src=http://purplepalmdash.github.io/images/mylogo.jpeg alt=gravatar><h1><a href=http://purplepalmdash.github.io/>很惭愧，就做了一点微小的工作</a></h1><a href=http://purplepalmdash.github.io/><p>Dash</p></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/post/>All Posts</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/technology/>Technology</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/life/>Life</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/linuxtips/>LinuxTips</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/purplepalmdash><i class="fa fa-github-square fa-3x"></i></a><a href=https://cn.linkedin.com/in/yang-feipeng-1b909319><i class="fa fa-linkedin-square fa-3x"></i></a><a href=https://plus.google.com/u/0/106572959364703833986><i class="fa fa-google-plus-square fa-3x"></i></a><a href=https://www.facebook.com/yang.feipeng><i class="fa fa-facebook-square fa-3x"></i></a><a href=https://twitter.com/dashwillfly><i class="fa fa-twitter-square fa-3x"></i></a></li></ul></div></div><div class="content container"><div class=posts><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2018/05/19/workingtipsonportus/>WorkingTipsOnPortus</a></h1><span class=post-date>May 19, 2018<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=environment>Environment</h3><p>Runtime environment:</p><pre><code>OS: Ubuntu 14.04.3 LTS
docker version: 18.03.1-ce
docker-compose version: docker-compose version 1.21.2, build a133471
IP: 192.192.189.53
domain name: portus.xxxx.com
</code></pre><p>For installing docker:</p><pre><code>$ sudo apt-get purge lxc-docker-1.9.0
$ sudo apt-get install \
    linux-image-extra-$(uname -r) \
    linux-image-extra-virtual
$ sudo apt-get update
$ sudo apt-get install -y \
    apt-transport-https \
        ca-certificates \
            curl \
                software-properties-common

$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
$ sudo apt-key fingerprint 0EBFCD88
$ sudo add-apt-repository \
   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
      $(lsb_release -cs) \
         stable&quot;
$ sudo apt-get update
$ sudo apt-get install -y docker-ce
$ sudo apt-get install -y libyaml-dev libpython-dev
$ sudo pip uninstall docker-py
$ sudo pip uninstall docker-compose
$ sudo pip install --upgrade --force-reinstall  docker-compose
</code></pre><h3 id=steps>Steps</h3><p>Clone the source code from github:</p><pre><code># git clone https://github.com/SUSE/Portus.git
</code></pre><p>Make certification in secrets folder:</p><pre><code># cd /home/vagrant/Portus/examples/compose/secrets
# openssl req -newkey rsa:4096 -nodes -sha256 -keyout portus.key -x509 -days 3650 -out portus.crt
</code></pre><p>In the above steps, input following items:</p><pre><code>Country Name (2 letter code) [AU]:CN
State or Province Name (full name) [Some-State]:Guangdong
Locality Name (eg, city) []:Guangzhou
Organization Name (eg, company) [Internet Widgits Pty Ltd]:kkkk
Organizational Unit Name (eg, section) []:cloud
Common Name (e.g. server FQDN or YOUR name) []:portus.kkkk.com
Email Address []:xxxx@xxxx.com
</code></pre><h3 id=docker-compose-file>Docker Compose File</h3><p>The docker compose file is the critical for portus deployment, following is
my configuration file:</p><pre><code>version: &quot;2&quot;

services:
  portus:
    image: opensuse/portus:head
    environment:
      - PORTUS_MACHINE_FQDN_VALUE=${MACHINE_FQDN}

      # DB. The password for the database should definitely not be here. You are
      # probably better off with Docker Swarm secrets.
      - PORTUS_DB_HOST=db
      - PORTUS_DB_DATABASE=portus_production
      - PORTUS_DB_PASSWORD=${DATABASE_PASSWORD}
      - PORTUS_DB_POOL=5

      # Secrets. It can possibly be handled better with Swarm's secrets.
      - PORTUS_SECRET_KEY_BASE=${SECRET_KEY_BASE}
      - PORTUS_KEY_PATH=/certificates/portus.key
      - PORTUS_PASSWORD=${PORTUS_PASSWORD}

      # SSL
      - PORTUS_PUMA_TLS_KEY=/certificates/portus.key
      - PORTUS_PUMA_TLS_CERT=/certificates/portus.crt

      # NGinx is serving the assets instead of Puma. If you want to change this,
      # uncomment this line.
      #- RAILS_SERVE_STATIC_FILES='true'
    ports:
      - 3000:3000
    links:
      - db
    volumes:
      - ./secrets:/certificates:ro
      - static:/srv/Portus/public
    extra_hosts:
      - &quot;portus.xxxx.com:192.192.189.53&quot;

  background:
    image: opensuse/portus:head
    depends_on:
      - portus
      - db
    environment:
      # Theoretically not needed, but cconfig's been buggy on this...
      - CCONFIG_PREFIX=PORTUS
      - PORTUS_MACHINE_FQDN_VALUE=${MACHINE_FQDN}

      # DB. The password for the database should definitely not be here. You are
      # probably better off with Docker Swarm secrets.
      - PORTUS_DB_HOST=db
      - PORTUS_DB_DATABASE=portus_production
      - PORTUS_DB_PASSWORD=${DATABASE_PASSWORD}
      - PORTUS_DB_POOL=5

      # Secrets. It can possibly be handled better with Swarm's secrets.
      - PORTUS_SECRET_KEY_BASE=${SECRET_KEY_BASE}
      - PORTUS_KEY_PATH=/certificates/portus.key
      - PORTUS_PASSWORD=${PORTUS_PASSWORD}

      - PORTUS_BACKGROUND=true
    links:
      - db
    volumes:
      - ./secrets:/certificates:ro
    extra_hosts:
      - &quot;portus.xxxx.com:192.192.189.53&quot;

  db:
    image: library/mariadb:10.0.23
    command: mysqld --character-set-server=utf8 --collation-server=utf8_unicode_ci --init-connect='SET NAMES UTF8;' --innodb-flush-log-at-trx-commit=0
    environment:
      - MYSQL_DATABASE=portus_production

      # Again, the password shouldn't be handled like this.
      - MYSQL_ROOT_PASSWORD=${DATABASE_PASSWORD}
    volumes:
      - /var/lib/portus/mariadb:/var/lib/mysql
    extra_hosts:
      - &quot;portus.xxxx.com:192.192.189.53&quot;

  registry:
    image: library/registry:2.6
    command: [&quot;/bin/sh&quot;, &quot;/etc/docker/registry/init&quot;]
    environment:
      # Authentication
      REGISTRY_AUTH_TOKEN_REALM: https://${MACHINE_FQDN}:3000/v2/token
      REGISTRY_AUTH_TOKEN_SERVICE: ${MACHINE_FQDN}:5000
      REGISTRY_AUTH_TOKEN_ISSUER: ${MACHINE_FQDN}
      #REGISTRY_AUTH_TOKEN_ISSUER: portus.test.lan
      REGISTRY_AUTH_TOKEN_ROOTCERTBUNDLE: /secrets/portus.crt

      # SSL
      REGISTRY_HTTP_TLS_CERTIFICATE: /secrets/portus.crt
      REGISTRY_HTTP_TLS_KEY: /secrets/portus.key

      # Portus endpoint
      REGISTRY_NOTIFICATIONS_ENDPOINTS: &gt;
        - name: portus
          url: https://${MACHINE_FQDN}:3000/v2/webhooks/events
          #url: https://192.192.189.53:3000/v2/webhooks/events
          timeout: 2000ms
          threshold: 5
          backoff: 1s
    volumes:
      - /var/lib/portus/registry:/var/lib/registry
      - ./secrets:/secrets:ro
      - ./registry/config.yml:/etc/docker/registry/config.yml:ro
      - ./registry/init:/etc/docker/registry/init:ro
    ports:
      - 5000:5000
      - 5001:5001 # required to access debug service
    links:
      - portus:portus
    extra_hosts:
      - &quot;portus.xxxx.com:192.192.189.53&quot;

  nginx:
    image: library/nginx:alpine
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./secrets:/secrets:ro
      - static:/srv/Portus/public:ro
    ports:
      - 80:80
      - 443:443
    links:
      - registry:registry
      - portus:portus
    extra_hosts:
      - &quot;portus.xxxx.com:192.192.189.53&quot;

volumes:
  static:
    driver: local
</code></pre><p>When everything is configured, startup the service via:</p><pre><code># docker-compose -f docker-compose.yml up
</code></pre><h3 id=configuration>Configuration</h3><p>Before open your browser for accessing the portus service, do following :</p><pre><code>$ sudo echo &quot;192.192.189.53	portus.xxxx.com&quot;&gt;&gt;/etc/hosts
</code></pre><p>Now open your browser for <code>https://portus.xxxx.com</code>:</p><p><img src=/images/2018_05_19_17_36_55_626x496.jpg alt=/images/2018_05_19_17_36_55_626x496.jpg></p><p>Configure the registry via:</p><p><img src=/images/2018_05_19_17_37_51_488x262.jpg alt=/images/2018_05_19_17_37_51_488x262.jpg></p><p>Team->Create new team, create team for kismatic deployment:</p><p><img src=/images/2018_05_19_17_40_16_397x279.jpg alt=/images/2018_05_19_17_40_16_397x279.jpg></p><p>Admin->User->Create new user:</p><p><img src=/images/2018_05_19_17_41_59_434x313.jpg alt=/images/2018_05_19_17_41_59_434x313.jpg></p><p>The created user is listed as:</p><p><img src=/images/2018_05_19_17_42_41_1023x262.jpg alt=/images/2018_05_19_17_42_41_1023x262.jpg></p><p>Team->members->Add members:</p><p><img src=/images/2018_05_19_17_43_32_640x169.jpg alt=/images/2018_05_19_17_43_32_640x169.jpg></p><p>Create a new namespace for kismatic 1.10 deployment images:</p><p><img src=/images/2018_05_19_17_44_28_440x297.jpg alt=/images/2018_05_19_17_44_28_440x297.jpg></p><p>You can easily view portus logs at dashboard:</p><p><img src=/images/2018_05_19_17_45_14_584x314.jpg alt=/images/2018_05_19_17_45_14_584x314.jpg></p><h3 id=push-images>Push images</h3><p>upload the portus.crt to remote machine(kismatic deployment node):</p><pre><code># scp ./portus.crt  kkkkk@192.192.189.1:/home/kkkkk/
root@registry3:~/Portus/examples/compose/secrets# pwd
/home/vagrant/Portus/examples/compose/secrets
</code></pre><p>Add the crt file into your system folder and trust this file, take ArchLinux
for example:</p><pre><code>$ sudo cp portus.crt /etc/ca-certificates/trust-source/anchors/portus.xxxx.com.crt
$ sudo update-ca-trust
$ sudo trust extract-compat
$ sudo systemctl restart docker
$ sudo docker login portus.xxxx.com:5000
$ sudo docker login portus.xxxx.com
Username: kismatic
Password: 
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Are you sure you want to proceed? [y/N] y
Login Succeeded
</code></pre><p>kismatic configuration items:</p><pre><code># vim kismatic-cluster.yaml
docker_registry:

  # IP or hostname and port for your registry.
  server: &quot;portus.xxxx.com:5000/kismatic110&quot;

  # Absolute path to the certificate authority that should be trusted when
  # connecting to your registry.
  CA: &quot;/home/xxxxx/portus.xxxx.com.crt&quot;

  # Leave blank for unauthenticated access.
  username: &quot;kismatic&quot;

  # Leave blank for unauthenticated access.
  password: &quot;xxxxxxxx&quot;
# ./kismatic seed-registry --verbose
</code></pre><p>Now you will see the output for uploading:</p><p><img src=/images/2018_05_19_18_07_42_537x460.jpg alt=/images/2018_05_19_18_07_42_537x460.jpg></p><p>namespace for kismatic110:</p><p><img src=/images/2018_05_19_18_08_08_881x445.jpg alt=/images/2018_05_19_18_08_08_881x445.jpg></p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2018/05/16/workingtipsonistiodemo/>WorkingTipsOnIstioDemo</a></h1><span class=post-date>May 16, 2018<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=istio-08-download>istio 0.8 download</h3><p>Download from its daily build:</p><p><a href=https://gcsweb.istio.io/gcs/istio-prerelease/daily-build/>https://gcsweb.istio.io/gcs/istio-prerelease/daily-build/</a></p><pre><code># wget https://storage.googleapis.com/istio-prerelease/daily-build/release-0.8-20180515-17-26/istio-release-0.8-20180515-17-26-linux.tar.gz
# mkdir -p /root/istio/bin/
# cp /root/Code/istio-release-0.8-20180515-17-26/bin/istioctl /root/istio/bin/
</code></pre><h3 id=build>Build</h3><p>Install following packages:</p><pre><code># yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel java-1.8.0-openjdk-debug
# export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-0.b14.el7_4.x86_64
# export PATH=$PATH:$JAVA_HOME/bin
</code></pre><p>Build the cars-api service via:</p><pre><code># ./mvnw -Distio.home=/root/Code/istio-release-0.8-20180515-17-26 clean package fabric8:build
# docker images | grep cars-api
kameshsampath/cars-api                                                      0.0.1               28647076e814        8 minutes ago       439 MB
</code></pre><h3 id=verification>Verification</h3><p>Install cars-api:</p><pre><code># kubectl  apply -f istio-cars-api-0.0.1-all.yml 
deployment.extensions &quot;cars-api&quot; created
# kubectl  get pods
NAME                                      READY     STATUS    RESTARTS   AGE
cars-api-777b9574bf-jvxvk                 2/2       Running   0          3m
</code></pre><p><code>car-api-ingress.yaml</code> definition:</p><pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  labels:
    expose: &quot;true&quot;
    app: cars-api
    version: 0.0.1
  name: cars-api
  namespace: default
  annotations:
    kubernetes.io/ingress.class: istio
spec:
  rules:
  - http:
      paths:
      - backend:
          serviceName: cars-api
          servicePort: 8080
</code></pre><p>Create a ingress:</p><pre><code># kubectl create -f car-api-ingress.yaml
</code></pre><p>Test the car api via:</p><pre><code># curl -vvv http://192.192.189.41:32204/cars/list
</code></pre><h3 id=auth>auth</h3><p>Definition file <code>auth.yaml</code>:</p><pre><code>apiVersion: &quot;authentication.istio.io/v1alpha1&quot;
kind: &quot;Policy&quot;
metadata:
  name: &quot;cars-api&quot;
spec:
  targets:
  - name: cars-api
  peers:
  - mtls:
  origins:
  - jwt:
      issuer: http://keycloak.default:8080/auth/realms/istio
      jwksUri: http://keycloak.default:8080/auth/realms/istio/protocol/openid-connect/certs
      audiences: 
      - cars-web  
  principalBinding: USE_ORIGIN
</code></pre><p>Create the Policy via:</p><pre><code># /root/istio/bin/istioctl create  -f auth.yaml
</code></pre><p>Now re-visit the ingress item you will see 401 issue:</p><pre><code># curl -vvv http://192.192.189.41:32204/cars/list

*   Trying 192.192.189.41...
* TCP_NODELAY set
* Connected to 192.192.189.41 (192.192.189.41) port 32204 (#0)
&gt; GET /cars/list HTTP/1.1
&gt; Host: 192.192.189.41:32204
&gt; User-Agent: curl/7.59.0
&gt; Accept: */*
&gt; 
&lt; HTTP/1.1 401 Unauthorized
&lt; content-length: 29
&lt; content-type: text/plain
&lt; date: Wed, 16 May 2018 03:16:47 GMT
&lt; server: envoy
&lt; x-envoy-upstream-service-time: 8
&lt; 
* Connection #0 to host 192.192.189.41 left intact
Origin authentication failed.%              
</code></pre><p>Get the token, then :</p><pre><code># curl -vvv -H &quot;Authorization: Bearer $token&quot; http://192.192.189.41:32204/cars/list
</code></pre><p>you will get the right result.</p><h3 id=different-namespace>Different Namespace</h3><p>Create the auth.yaml for different namespace is OK:</p><pre><code># /root/istio/bin/istioctl create -f auth-myproject.yaml  -n myproject
</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2018/05/14/nfsstoragefork8s/>NFSStorageForK8s</a></h1><span class=post-date>May 14, 2018<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=code>Code</h3><p>Code is git from following url:</p><pre><code># git clone https://github.com/kubernetes-incubator/external-storage.git
</code></pre><h3 id=steps>Steps</h3><p>You have to manually get the docker image for
<code>quay.io/external_storage/nfs-client-provisioner</code>, then create the storage
class via following command:</p><p>deployment.yaml:</p><pre><code>kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: nfs-client-provisioner
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: quay.io/external_storage/nfs-client-provisioner:latest
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: fuseim.pri/ifs
            - name: NFS_SERVER
              value: 192.192.189.31
            - name: NFS_PATH
              value: /opt/nfs
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.192.189.31
            path: /opt/nfs
</code></pre><p>Install via <code>kubectl create -f deployment.yaml</code>.</p><p>StorageClass yaml is listed as:</p><p>sc.yaml:</p><pre><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storage
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
provisioner: fuseim.pri/ifs
</code></pre><p>Create it via <code>kubectl create -f sc.yaml</code>.</p><p>test-claim.yaml:</p><pre><code>kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-claim
  annotations:
    volume.beta.kubernetes.io/storage-class: &quot;nfs-storage&quot;
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Mi
</code></pre><p>And test-pod.yaml:</p><pre><code>kind: Pod
apiVersion: v1
metadata:
  name: test-pod
spec:
  containers:
  - name: test-pod
    image: gcr.io/google_containers/busybox:1.24
    command:
      - &quot;/bin/sh&quot;
    args:
      - &quot;-c&quot;
      - &quot;touch /mnt/SUCCESS &amp;&amp; exit 0 || exit 1&quot;
    volumeMounts:
      - name: nfs-pvc
        mountPath: &quot;/mnt&quot;
  restartPolicy: &quot;Never&quot;
  volumes:
    - name: nfs-pvc
      persistentVolumeClaim:
        claimName: test-claim
</code></pre><p>Verify them via install and delete them.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2018/05/01/ratelimitingoninstio/>RateLimitingOnInstio</a></h1><span class=post-date>May 1, 2018<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=service-example>Service Example</h3><p>The yaml file is directly taken from the official example of <code>helloworld</code>, but
I remove the v2 deployment, thus the yaml file is listed as following:</p><pre><code>apiVersion: v1
kind: Service
metadata:
  name: helloworld
  labels:
    app: helloworld
spec:
  type: NodePort
  ports:
  - port: 5000
    name: http
  selector:
    app: helloworld
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: helloworld-v1
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: helloworld
        version: v1
    spec:
      containers:
      - name: helloworld
        image: istio/examples-helloworld-v1
        resources:
          requests:
            cpu: &quot;100m&quot;
        imagePullPolicy: IfNotPresent #Always
        ports:
        - containerPort: 5000
</code></pre><p>Use istioctl for injecting the sidecar, thus we could later use prometheus for
monitoring its traffic flow:</p><pre><code># kubectl create -f &lt;(istioctl kube-inject -f helloworld.yaml)
</code></pre><p>Examine the deployment/service/pods:</p><pre><code># kubectl get svc helloworld       
NAME         TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
helloworld   NodePort   10.96.242.5   &lt;none&gt;        5000:31241/TCP   27m
# kubectl get deployment helloworld-v1
NAME            DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
helloworld-v1   1         1         1            1           27m
# kubectl get pods | grep helloworld
helloworld-v1-7d57446779-dctlv    2/2       Running   0          27m
</code></pre><h3 id=make-ingress>Make ingress</h3><p>The <code>helloworld-ingress.yaml</code> is listed as following:</p><pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: helloworld
  annotations:
    kubernetes.io/ingress.class: &quot;istio&quot;
spec:
  rules:
  - http:
      paths:
      - path: /hello
        backend:
          serviceName: helloworld
          servicePort: 5000
</code></pre><p>Create the ingress and verify it:</p><pre><code># kubectl create -f helloworld-ingress.yaml
# kubectl get ingress helloworld
NAME         HOSTS     ADDRESS   PORTS     AGE
helloworld   *                   80        1h
# curl http://192.168.99.100:30039/hello
Hello version: v1, instance: helloworld-v1-7d57446779-dctlv
</code></pre><h3 id=rate-limiting>Rate Limiting</h3><p>Write following rete limiting yaml for defining its traffic:</p><pre><code>apiVersion: &quot;config.istio.io/v1alpha2&quot;
kind: memquota
metadata:
  name: helloworldservicehandler
  namespace: istio-system
spec:
  quotas:
  - name: helloworldservicerequestcount.quota.istio-system
    maxAmount: 5000
    validDuration: 1s
    # The first matching override is applied.
    # A requestcount instance is checked against override dimensions.
    overrides:
    # The following override applies to 'helloworld' regardless
    # of the source.
    - dimensions:
        destination: helloworld
      maxAmount: 2
      validDuration: 1s

---
apiVersion: &quot;config.istio.io/v1alpha2&quot;
kind: quota
metadata:
  name: helloworldservicerequestcount
  namespace: istio-system
spec:
  dimensions:
    source: source.labels[&quot;app&quot;] | source.service | &quot;unknown&quot;
    sourceVersion: source.labels[&quot;version&quot;] | &quot;unknown&quot;
    destination: destination.labels[&quot;app&quot;] | destination.service | &quot;unknown&quot;
    destinationVersion: destination.labels[&quot;version&quot;] | &quot;unknown&quot;

---
apiVersion: &quot;config.istio.io/v1alpha2&quot;
kind: rule
metadata:
  name: helloworldservicequota
  namespace: istio-system
spec:
  actions:
  - handler: helloworldservicehandler.memquota
    instances:
    - helloworldservicerequestcount.quota
---
apiVersion: config.istio.io/v1alpha2
kind: QuotaSpec
metadata:
  creationTimestamp: null
  name: helloworldservicerequest-count
  namespace: istio-system
spec:
  rules:
  - quotas:
    - charge: 1
      quota: RequestCount
---
apiVersion: config.istio.io/v1alpha2
kind: QuotaSpecBinding
metadata:
  creationTimestamp: null
  name: helloworldservicerequest-count
  namespace: istio-system
spec:
  quotaSpecs:
  - name: helloworldservicerequest-count
    namespace: istio-system
  services:
  - name: helloworld
    namespace: default
</code></pre><p>The above items define a 2 qps rate limiting.</p><h3 id=monitoring>Monitoring</h3><p>Use prometheus for monitoring the traffic, enable prometheus via:</p><pre><code># kubectl create -f ~/Code/istio-0.7.1/install/kubernetes/addons/prometheus.yaml
</code></pre><p>You could configure the prometheus&rsquo;s service type to NodePort, thus you could
directly access it.</p><p><img src=/images/2018_05_01_22_21_56_779x473.jpg alt=/images/2018_05_01_22_21_56_779x473.jpg></p><p>Make the traffic:</p><pre><code># while true; do curl -s -o /dev/null http://192.168.99.100:30039/hello;done
</code></pre><p>Then view the prometheus via following:</p><pre><code># increase(istio_request_count{destination_service=&quot;helloworld.default.svc.cluster.local&quot;, response_code=&quot;429&quot;}[5m])
</code></pre><p>Initial:</p><p><img src=/images/2018_05_01_22_24_18_764x343.jpg alt=/images/2018_05_01_22_24_18_764x343.jpg></p><p>After aboult 3 minutes:</p><p><img src=/images/2018_05_01_22_24_50_784x327.jpg alt=/images/2018_05_01_22_24_50_784x327.jpg></p><p>You could change the response code from <code>429</code> to <code>200</code>, this means you get the
succeed rate.</p><h3 id=fetch-back-the-result>Fetch back the result</h3><p>Refers to:</p><p><a href=https://www.robustperception.io/prometheus-query-results-as-csv/>https://www.robustperception.io/prometheus-query-results-as-csv/</a></p><pre><code># wget https://raw.githubusercontent.com/RobustPerception/python_examples/master/csv/query_csv.py
</code></pre><p>For querying the 429/200:</p><pre><code>#  python query_csv.py http://127.0.0.1:9090 'increase(istio_request_count{destination_service=&quot;helloworld.default.svc.cluster.local&quot;, response_code=&quot;429&quot;}[5m])'
name,timestamp,value,connection_mtls,destination_service,destination_version,instance,job,response_code,source_service,source_version
,1525185609.906,8145.762711864407,false,helloworld.default.svc.cluster.local,v1,172.17.0.10:42422,istio-mesh,429,istio-ingress.istio-system.svc.cluster.local,unknown
#  python query_csv.py http://127.0.0.1:9090 'increase(istio_request_count{destination_service=&quot;helloworld.default.svc.cluster.local&quot;, response_code=&quot;200&quot;}[5m])'
name,timestamp,value,connection_mtls,destination_service,destination_version,instance,job,response_code,source_service,source_version
,1525185628.005,886.7796610169491,false,helloworld.default.svc.cluster.local,v1,172.17.0.10:42422,istio-mesh,200,istio-ingress.istio-system.svc.cluster.local,unknown
</code></pre><p><code>8145</code> and <code>886</code> are the values for the query, we could use them for 2nd
development.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2018/04/29/workingtipsonistiodev/>WorkingTipsOnIstioDev</a></h1><span class=post-date>Apr 29, 2018<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=sample-svc>Sample SVC</h3><p>Create a sample svc using minikube:</p><pre><code># sudo docker save jrelva/nginx-autoindex&gt;autoindex.tar
# eval $(minikube docker-env)
# docker load&lt;autoindex.tar
# kubectl run --image=jrelva/nginx-autoindex:latest nginx-autoindex --port=80 --image-pull-policy=IfNotPresent
deployment &quot;nginx-autoindex&quot; created
# kubectl get deployment
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-autoindex    1         1         1            1           6s
# kubectl expose deployment nginx-autoindex --name nginx-autoindex-svc
# kubectl get svc | grep nginx
nginx-autoindex-svc   ClusterIP   10.107.181.75    &lt;none&gt;        80/TCP           29s
</code></pre><p>Istio Configuration:</p><pre><code># kubectl get svc --all-namespaces | grep istio-ingress
istio-system   istio-ingress          LoadBalancer   10.100.152.241   &lt;pending&gt;     80:30336/TCP,443:32004/TCP  
</code></pre><h3 id=istio-ingress>Istio Ingress</h3></div><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/78/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/78/>78</a></li><li class="page-item active"><a class=page-link href=/page/79/>79</a></li><li class=page-item><a class=page-link href=/page/80/>80</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/242/>242</a></li><li class=page-item><a href=/page/80/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/242/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div><script src=http://purplepalmdash.github.io/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>