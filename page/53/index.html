<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta name=generator content="Hugo 0.64.0"><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Dash &#183; Dash</title><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124"><link rel=stylesheet href=https://purplepalmdash.github.io/css/highlight/googlecode.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js></script><script type=text/javascript src=/js/html2canvas.js></script><script type=text/javascript>function genPostShot(){var rightNow=new Date();var imageName=rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");imageName+='.jpg'
html2canvas(document.getElementsByClassName('post'),{background:'#FFFFFF',onrendered:function(canvas){$('#test').attr('href',canvas.toDataURL("image/jpeg"));$('#test').attr('download',imageName);$('#test')[0].click();}});};</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124"><link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel=icon><link href=%7balternate%20%7bRSS%20application/rss+xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://purplepalmdash.github.io/index.xml%7d rel=alternate type=application/rss+xml title="Dash &#183; Dash"><meta name=description content><meta name=keywords content="unix,virtualization,embedded,linux"></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><img src=http://purplepalmdash.github.io/images/mylogo.jpeg alt=gravatar><h1><a href=http://purplepalmdash.github.io/>很惭愧，就做了一点微小的工作</a></h1><a href=http://purplepalmdash.github.io/><p>Dash</p></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/post/>All Posts</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/technology/>Technology</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/life/>Life</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/linuxtips/>LinuxTips</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/purplepalmdash><i class="fa fa-github-square fa-3x"></i></a><a href=https://cn.linkedin.com/in/yang-feipeng-1b909319><i class="fa fa-linkedin-square fa-3x"></i></a><a href=https://plus.google.com/u/0/106572959364703833986><i class="fa fa-google-plus-square fa-3x"></i></a><a href=https://www.facebook.com/yang.feipeng><i class="fa fa-facebook-square fa-3x"></i></a><a href=https://twitter.com/dashwillfly><i class="fa fa-twitter-square fa-3x"></i></a></li></ul></div></div><div class="content container"><div class=posts><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/03/13/usingredashforvisualization/>UsingRedashForVisualization</a></h1><span class=post-date>Mar 13, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=aim>AIM</h3><p>Using redash for representing Wuhan&rsquo;s nCov dataset.</p><h3 id=environment>Environment</h3><p>Hardware/Software/OS is listed as following:</p><pre><code>kvm vm with 2cores and 3072MB memory plus 200G disk.
Using docker/docker-compose for redash website.
OS is ubuntu 18.04.3
IP: 192.168.137.100
</code></pre><h3 id=redash-bootstrap>Redash bootstrap</h3><p>Using the repository which located at:<br><a href=https://github.com/getredash/setup>https://github.com/getredash/setup</a></p><p>You have to git clone this repository and run setup.sh under the directory. After setup the docker-compose file located in <code>/opt/redash</code>, if you want to quickly setup another docker/docker-compose based environment, simply copy the folder and the related docker images to remote machine&rsquo;s <code>/opt/redash</code>, up and running again.</p><pre><code>root@node:/opt/redash# ls
docker-compose.yml  env  postgres-data
root@node:/opt/redash# docker-compose run --rm server create_db
root@node:/opt/redash# docker-compose up -d
Creating network &quot;redash_default&quot; with the default driver
Creating redash_redis_1    ... done
Creating redash_postgres_1 ... done
Creating redash_scheduled_worker_1 ... done
Creating redash_adhoc_worker_1     ... done
Creating redash_scheduler_1        ... done
Creating redash_server_1           ... done
Creating redash_nginx_1            ... done
</code></pre><p>The redash environment rested in 7 docker instance, could be examined using <code>docker ps</code>:</p><pre><code>root@node:/opt/redash# docker ps
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                         NAMES
1e701e0bb8d2        redash/nginx:latest          &quot;nginx -g 'daemon of…&quot;   6 minutes ago       Up 6 minutes        0.0.0.0:80-&gt;80/tcp, 443/tcp   redash_nginx_1
5c00c609baa6        redash/redash:8.0.0.b32245   &quot;/app/bin/docker-ent…&quot;   7 minutes ago       Up 6 minutes        0.0.0.0:5000-&gt;5000/tcp        redash_server_1
8dcd4c2db0f8        redash/redash:8.0.0.b32245   &quot;/app/bin/docker-ent…&quot;   7 minutes ago       Up 6 minutes        5000/tcp                      redash_scheduled_worker_1
5d1dbd77f4ce        redash/redash:8.0.0.b32245   &quot;/app/bin/docker-ent…&quot;   7 minutes ago       Up 6 minutes        5000/tcp                      redash_adhoc_worker_1
6d56d7cd9040        redash/redash:8.0.0.b32245   &quot;/app/bin/docker-ent…&quot;   7 minutes ago       Up 6 minutes        5000/tcp                      redash_scheduler_1
c8efb3ff7437        postgres:9.6-alpine          &quot;docker-entrypoint.s…&quot;   7 minutes ago       Up 7 minutes        5432/tcp                      redash_postgres_1
a78dd1b1727f        redis:5.0-alpine             &quot;docker-entrypoint.s…&quot;   7 minutes ago       Up 7 minutes        6379/tcp                      redash_redis_1
</code></pre><p>Now visit the vm&rsquo;s 80 port you will see:</p><p><img src=/images/2020_03_13_22_52_24_544x553.jpg alt=/images/2020_03_13_22_52_24_544x553.jpg></p><p>After login you will get the following page:</p><p><img src=/images/2020_03_13_22_55_16_827x528.jpg alt=/images/2020_03_13_22_55_16_827x528.jpg></p><p>Until now your redash environment has been bootstraped.</p><h3 id=configure-data-source>Configure data source</h3><p>Click your username and select <code>Data Sources</code>:</p><p><img src=/images/2020_03_13_22_55_46_255x410.jpg alt=/images/2020_03_13_22_55_46_255x410.jpg></p><p>Click <code>New Data Source</code> button for adding a new data source:</p><p><img src=/images/2020_03_13_22_58_02_812x505.jpg alt=/images/2020_03_13_22_58_02_812x505.jpg></p><p>Select sqlite and you will be leading to following window:</p><p><img src=/images/2020_03_13_23_00_27_509x445.jpg alt=/images/2020_03_13_23_00_27_509x445.jpg></p><p>Fill in some infos for finishing:</p><p><img src=/images/2020_03_13_23_00_53_405x372.jpg alt=/images/2020_03_13_23_00_53_405x372.jpg></p><h3 id=get-sqlite-db>Get Sqlite db</h3><p>We take following page for refrence:</p><p><a href=https://discuss.redash.io/t/example-data-source-for-the-choropleth-maps/3696>https://discuss.redash.io/t/example-data-source-for-the-choropleth-maps/3696</a></p><p>First fetch the example sqlite db using following command:</p><p><img src=/images/2020_03_13_23_03_22_565x368.jpg alt=/images/2020_03_13_23_03_22_565x368.jpg></p><pre><code># wget https://dbhub.io/x/download/justinclift/DB4S%20daily%20users%20by%20country.sqlite?commit=28a554c2795170d5739b7a41df9baa2ad13b985b325d238bd869a14d1148f9ea
# mv DB4S\ daily\ users\ by\ country.sqlite first.db
</code></pre><p>Copy this db into following docker instance:</p><pre><code># docker cp first.db redash_scheduled_worker_1:/app
# docker cp first.db redash_server_1:/app
# docker cp first.db redash_adhoc_worker_1:/app
# docker cp first.db redash_scheduler_1:/app
</code></pre><p>Now click <code>Test Connection</code> you will get <code>Succeed</code>.</p><h3 id=create-query>Create Query</h3><p>Click <code>Create Query</code>, you will be leading to following window:</p><p><img src=/images/2020_03_13_23_10_34_613x380.jpg alt=/images/2020_03_13_23_10_34_613x380.jpg></p><p>Create a new query:</p><pre><code>SELECT country, users
FROM query_6
WHERE date = '2019-04-22'
</code></pre><p><img src=/images/2020_03_13_23_13_35_681x275.jpg alt=/images/2020_03_13_23_13_35_681x275.jpg></p><p>Click <code>Execute</code> you will get the table listed as:</p><p><img src=/images/2020_03_13_23_13_59_1157x317.jpg alt=/images/2020_03_13_23_13_59_1157x317.jpg></p><p>Click <code>New Visualization</code> for editing the visualization:</p><p><img src=/images/2020_03_13_23_14_47_955x617.jpg alt=/images/2020_03_13_23_14_47_955x617.jpg></p><p>Edit like following:</p><p><img src=/images/2020_03_13_23_17_03_634x623.jpg alt=/images/2020_03_13_23_17_03_634x623.jpg></p><p>The map will be rendered like:</p><p><img src=/images/2020_03_13_23_17_39_922x579.jpg alt=/images/2020_03_13_23_17_39_922x579.jpg></p><p>Click <code>save</code> for saving the map, then rename this query:</p><p><img src=/images/2020_03_13_23_18_36_559x261.jpg alt=/images/2020_03_13_23_18_36_559x261.jpg></p><h3 id=dashboard>Dashboard</h3><p>Create the first dashboard:</p><p><img src=/images/2020_03_13_23_19_59_566x223.jpg alt=/images/2020_03_13_23_19_59_566x223.jpg></p><p>An empty dashboard:</p><p><img src=/images/2020_03_13_23_20_20_753x511.jpg alt=/images/2020_03_13_23_20_20_753x511.jpg></p><p>Click <code>Add Widget</code>, select our newly created map:</p><p><img src=/images/2020_03_13_23_20_39_676x251.jpg alt=/images/2020_03_13_23_20_39_676x251.jpg></p><p>Select the map:</p><p><img src=/images/2020_03_13_23_21_16_691x280.jpg alt=/images/2020_03_13_23_21_16_691x280.jpg></p><p>Effect:</p><p><img src=/images/2020_03_13_23_21_33_921x659.jpg alt=/images/2020_03_13_23_21_33_921x659.jpg></p><h3 id=conclusion>Conclusion</h3><p>Now we have created the first visualization in redash easily displayed a
sqlite database, in next chapter we will take a look at how to display the
nCov statistics.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/03/13/%E7%94%A8superset%E5%8F%AF%E8%A7%86%E5%8C%96%E6%AD%A6%E6%B1%89%E8%82%BA%E7%82%8E%E6%95%B0%E6%8D%AE/>用Superset可视化武汉肺炎数据</a></h1><span class=post-date>Mar 13, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=前言>前言</h3><p>最近因为武汉肺炎的原因一直宅在家里，刷什么值得买的时候看到了一个玩mac mini server的哥们写的用superset可视化武汉肺炎的文章，照着做了一遍，将具体的步骤都写在了下面。后续做实际项目的BI内容可视化的时候可以用来参考。</p><p>该文章中也有少许操作步骤方面的错误，做的时候经常会卡在那里半天。当然如果按照下面记录的步骤来进行的话，这些问题应该不会出现。</p><h3 id=环境>环境</h3><p>涉及到的硬件、操作系统、软件等的情况列举如下：</p><pre><code>KVM虚拟机，4核，3G内存， 200G硬盘
Ubuntu18.04.3 x86_64
docker/docker-compose
</code></pre><h3 id=搭建superset>搭建superset</h3><p>通过 docker启动superset, 启动后监听 <code>8088</code> 端口:</p><pre><code># sudo docker run -d --name superset -p 8088:8088 amancevice/superset
</code></pre><p>初始化数据库：</p><pre><code># sudo docker run -d --name superset -p 8088:8088 amancevice/superset
cf39f0c9e6a1232796cfe9012d110d6dd50613a8e2022873c963974eb74c18ba
root@node:/mnt2# docker exec -it superset superset-init
Username [admin]: 
User first name [admin]: 
User last name [user]: 
Email [admin@fab.org]: 
Password: 
Repeat for confirmation: 
</code></pre><p>现在可以用admin用户及刚才创建的密码登录superset界面了：</p><p><img src=/images/2020_03_13_13_10_20_741x496.jpg alt=/images/2020_03_13_13_10_20_741x496.jpg></p><p>登录后界面如下：</p><p><img src=/images/2020_03_13_13_15_05_996x482.jpg alt=/images/2020_03_13_13_15_05_996x482.jpg></p><h3 id=准备数据>准备数据</h3><p>肺炎的数据从以下页面取得：</p><p><a href=https://github.com/canghailan/Wuhan-2019-nCoV>https://github.com/canghailan/Wuhan-2019-nCoV</a></p><p>点击下面链接中数据接口里的csv，将csv文件下载到本地:</p><p><img src=/images/2020_03_13_13_17_00_846x360.jpg alt=/images/2020_03_13_13_17_00_846x360.jpg></p><p>csv文件可以方便的用libreoffice或者excel打开，后面我们将用libreoffice对数据做一点点更改以支持superset中的地图显示。</p><p>手动建立省份代码csv文件( <code>province.csv</code> ), 这个文件是ISO3316标准下的中国省份代码，因为superset的地图中需要使用该标准下的代码:</p><pre><code>编码,城市
北京市,CN-11
天津市,CN-12
河北省,CN-13
山西省,CN-14
内蒙古自治区,CN-15
辽宁省,CN-21
吉林省,CN-22
黑龙江省,CN-23
上海市,CN-31
江苏省,CN-32
浙江省,CN-33
安徽省,CN-34
福建省,CN-35
江西省,CN-36
山东省,CN-37
河南省,CN-41
湖北省,CN-42
湖南省,CN-43
广东省,CN-44
广西壮族自治区,CN-45
海南省,CN-46
重庆市,CN-50
四川省,CN-51
贵州省,CN-52
云南省,CN-53
西藏自治区,CN-54
陕西省,CN-61
甘肃省,CN-62
青海省,CN-63
宁夏回族自治区,CN-64
新疆维吾尔自治区,CN-65
</code></pre><p>在libreoffice中打开两个csv文件后，在<code>Wuhan-2019-nCov</code> 的工作空间里新建一个tab将 <code>province.csv</code>内容全盘复制进来, 并将此tab命名为 <code>province</code>:</p><p><img src=/images/2020_03_13_13_27_26_501x223.jpg alt=/images/2020_03_13_13_27_26_501x223.jpg></p><p>接下来我们使用vlookup用来将ISO3316的省份代码插入到<code>Wuhan-2019-nCov</code>表中, 首先在province栏后新建一栏，命名为<code>3316code</code>:</p><p><img src=/images/2020_03_13_13_34_44_688x239.jpg alt=/images/2020_03_13_13_34_44_688x239.jpg></p><p>此栏目现在是空的，我们需要用函数将其批量替换为省份对应的代码，如，<code>湖北省</code> -> <code>CN-41</code>, 首先选中E中的第一个空白栏，而后点击<code>fx</code>按钮，在弹出的函数选择框中输入 <code>vookup</code> 后，启动函数编辑器:</p><p><img src=/images/2020_03_13_13_38_55_713x634.jpg alt=/images/2020_03_13_13_38_55_713x634.jpg></p><p>点击 <code>Next</code> 后进入到函数参数输入界面, 首先配置搜索条件, 在<code>Search criterion</code>中填入<code>D:D</code>， libreoffice里将自动选择D全栏, 代表搜索省份中所有的条目:</p><p><img src=/images/2020_03_13_13_44_49_494x457.jpg alt=/images/2020_03_13_13_44_49_494x457.jpg></p><p>选中<code>Array</code>后，鼠标点击切换到 <code>Province</code> 表， 选中A/B全栏， 或者直接输入 <code>$Province.A:B</code>，匹配:</p><p><img src=/images/2020_03_13_14_05_21_736x482.jpg alt=/images/2020_03_13_14_05_21_736x482.jpg></p><p><code>Index</code> 输入数值2, 代表从匹配变量一栏开始需要匹配的数据为第2栏，而 <code>Sort order</code>则输入0后，可以看到输出结果为 <code>CN-42</code>， 代表已经匹配成功:</p><p><img src=/images/2020_03_13_14_05_52_666x477.jpg alt=/images/2020_03_13_14_05_52_666x477.jpg></p><p>点击 <code>OK</code> 后，点击函数将其扩展到以下几行，观察结果:</p><p><img src=/images/2020_03_13_14_06_49_278x253.jpg alt=/images/2020_03_13_14_06_49_278x253.jpg></p><p>可以看到有 <code>#N/A</code> 的错误输出结果，我们需要修改函数忽略掉此输出:</p><p><img src=/images/2020_03_13_14_06_49_278x253.jpg alt=/images/2020_03_13_14_06_49_278x253.jpg></p><p>修改函数为 <code>IFERROR(VLOOKUP(D:D, $Province.A:B,2,0),"")</code>之后，查看结果:</p><p><img src=/images/2020_03_13_14_12_02_492x203.jpg alt=/images/2020_03_13_14_12_02_492x203.jpg></p><p>结果显示正常:</p><p><img src=/images/2020_03_13_14_13_04_364x293.jpg alt=/images/2020_03_13_14_13_04_364x293.jpg></p><p>接下来将此函数应用到全列, 选中一个已经应用公式的条目后，如E30, 按住shift键，鼠标一直拉到E的最后一行后，点最后一个元素，选中全列，而后按<code>Ctrl+D</code>，将公式应用到全列，如:</p><p><img src=/images/2020_03_13_14_24_38_788x636.jpg alt=/images/2020_03_13_14_24_38_788x636.jpg></p><p>现在保存此csv文件，命名为 <code>nCovForSuperset.csv</code>后退出, 注意要选择 <code>Use Text CSV Format </code>。</p><p><img src=/images/2020_03_13_14_25_31_545x184.jpg alt=/images/2020_03_13_14_25_31_545x184.jpg></p><p>至此，数据准备完毕。</p><h3 id=建立数据源>建立数据源</h3><p>用sqlite3建立一个 <code>假</code> 的数据文件:</p><pre><code># sqlite3 test.db
SQLite version 3.22.0 2018-01-22 18:45:57
Enter &quot;.help&quot; for usage hints.
sqlite&gt; .quit
# ls
nCovForSuperset.csv  province.csv  test.db  Wuhan-2019-nCoV.csv
</code></pre><p>我们将 此 <code>test.db</code>文件拷贝到容器中, 并通过root用户改变其文件权限:</p><pre><code># docker cp test.db superset:/home/superset/
# docker exec -it  --workdir /root --user root superset chmod 777 /home/superset/test.db
</code></pre><p>在Superset界面中点击 <code>Sources</code> -> <code>Database</code>, 后，进入到配置数据库的界面:</p><p><img src=/images/2020_03_13_14_31_43_1049x376.jpg alt=/images/2020_03_13_14_31_43_1049x376.jpg></p><p>点击 <code>+</code> 号，新加一个数据库, 输入数据库名，及数据库所在的URI，此例子中为 <code>sqlite:////home/superset/test.db</code>:</p><p><img src=/images/2020_03_13_14_38_24_837x348.jpg alt=/images/2020_03_13_14_38_24_837x348.jpg></p><p>勾选 <code>Allow Csv Upload</code> 及 <code>Allow CREATE TABLE AS</code>:</p><p><img src=/images/2020_03_13_14_39_50_568x222.jpg alt=/images/2020_03_13_14_39_50_568x222.jpg></p><p>之后点击 <code>Save</code>， 可以看到数据库已经被建立起来:</p><p><img src=/images/2020_03_13_14_40_15_631x301.jpg alt=/images/2020_03_13_14_40_15_631x301.jpg></p><p>上传刚才编辑好的CSV文件:</p><p><img src=/images/2020_03_13_14_44_15_571x374.jpg alt=/images/2020_03_13_14_44_15_571x374.jpg></p><p>会报出出错，提示因权限问题无法上传此CSV文件：</p><p><img src=/images/2020_03_13_14_45_42_1191x277.jpg alt=/images/2020_03_13_14_45_42_1191x277.jpg></p><p>手动建立目录并改变其权限:</p><pre><code># docker exec -it  --workdir /root --user root superset mkdir -p /usr/local/lib/python3.6/site-packages/superset/app
# docker exec -it  --workdir /root --user root superset chmod 777 -R /usr/local/lib/python3.6/site-packages/superset/app
</code></pre><p>上传成功后可以看到<code>Tables</code>中有了新的文件:</p><p><img src=/images/2020_03_13_14_47_11_726x357.jpg alt=/images/2020_03_13_14_47_11_726x357.jpg></p><h3 id=更改tables属性>更改tables属性</h3><p>CSV上传后，大部分的字段（数据) 并没有被确定为准确的类型，superset需要从这些数据中知道哪些是数据，哪些是时间，哪些又可以被归类，为此我们需要编辑此Table中的数据:</p><p>点击<code>Edit Table</code> 进入编辑界面，首先更改<code>Detail</code>中的 <code>Offset</code>为8,代表时区与UTC差别为8：</p><p><img src=/images/2020_03_13_14_49_40_436x247.jpg alt=/images/2020_03_13_14_49_40_436x247.jpg></p><p><img src=/images/2020_03_13_14_49_51_483x269.jpg alt=/images/2020_03_13_14_49_51_483x269.jpg></p><p>而后开始编辑 <code>Columns</code>, 相关属性说明: <code>groupable</code> 代表是否可以分组; <code>filterable</code> 代表是否可以分类; <code>is temporal</code> 代表是否是时间参数。我们需要把date的type设置为timestamp, 并选取其 <code>is temporal</code> 属性，其他的所有字段的 <code>groupable</code> 及 <code>filterable</code> 都勾上:</p><p><img src=/images/2020_03_13_14_53_24_1080x643.jpg alt=/images/2020_03_13_14_53_24_1080x643.jpg></p><p>Metrics一栏暂时不做任何设置，至此数据已经清晰化，下一步进入数据分析环节.</p><h3 id=数据分析>数据分析</h3><p>点击 <code>Sources</code> -> <code>Tables</code> ，选中我们刚才上传的CSV文件建立的表后，进入到数据可视化编辑界面，初次进入时界面是完全空白的，我们需要在这里添加相关图表。</p><h4 id=中国地图>中国地图</h4><p>点击 <code>Visualization Type</code>， 选择 <code>Country Map</code>:</p><p><img src=/images/2020_03_13_14_56_14_398x333.jpg alt=/images/2020_03_13_14_56_14_398x333.jpg></p><p>选择为以下值时候，运行 <code>Run Query</code>:</p><p><img src=/images/2020_03_13_14_59_04_866x838.jpg alt=/images/2020_03_13_14_59_04_866x838.jpg></p><p>结果如下，比如，选择甘肃，可以看到累计的确诊人数为127人：</p><p><img src=/images/2020_03_13_14_59_44_872x550.jpg alt=/images/2020_03_13_14_59_44_872x550.jpg></p><p>编辑完后，点击save即可保存，我们保存为<code>chinamap</code>:</p><p><img src=/images/2020_03_13_15_01_10_417x324.jpg alt=/images/2020_03_13_15_01_10_417x324.jpg></p><h4 id=时间线地图>时间线地图</h4><p>建立一个类型为 <code>Line Charts</code>的图表，数据类型如下：</p><p><img src=/images/2020_03_13_15_34_03_620x820.jpg alt=/images/2020_03_13_15_34_03_620x820.jpg></p><p>结果如下:</p><p><img src=/images/2020_03_13_15_35_25_1239x955.jpg alt=/images/2020_03_13_15_35_25_1239x955.jpg></p><p>保存名称为 <code>Trend</code>.</p><h4 id=sunburst类型>sunburst类型</h4><p>数据类型如下:</p><p><img src=/images/2020_03_13_15_37_55_629x795.jpg alt=/images/2020_03_13_15_37_55_629x795.jpg></p><p>结果：</p><p><img src=/images/2020_03_13_15_38_18_1131x822.jpg alt=/images/2020_03_13_15_38_18_1131x822.jpg></p><p>保存名称为 <code>SunburstChina</code></p><h4 id=forcedirected类型>ForceDirected类型</h4><p>数据定义如下:</p><p><img src=/images/2020_03_13_15_41_06_620x724.jpg alt=/images/2020_03_13_15_41_06_620x724.jpg></p><p>结果:</p><p><img src=/images/2020_03_13_15_41_56_991x744.jpg alt=/images/2020_03_13_15_41_56_991x744.jpg></p><p>保存名称为: <code>totalDead</code>.</p><p>至此所有的单图表创建完毕.</p><h3 id=dashboard>Dashboard</h3><p>增加一个Dashboard:</p><p><img src=/images/2020_03_13_15_43_41_523x258.jpg alt=/images/2020_03_13_15_43_41_523x258.jpg></p><p>点击Edit，添加对应的图表到该dashboard上即可:</p><p><img src=/images/2020_03_13_16_13_23_1124x480.jpg alt=/images/2020_03_13_16_13_23_1124x480.jpg></p><p>至此，可视化完毕，可以探索更多的样例做出更多的可视化效果。</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/02/29/linuxtips11/>LinuxTips11</a></h1><span class=post-date>Feb 29, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/linuxtips>LinuxTips</a></span><h3 id=1-docker-exec-with-root>1. docker exec with root</h3><p>Via following commands:</p><pre><code>$ sudo docker exec -it --workdir /root --user root 1e61b0cce4f2 bash
</code></pre><h3 id=2-vncserver-for-ubuntu1804>2. vncserver for ubuntu18.04</h3><p>Install xfce4 and tigervnc-server:</p><pre><code># sudo apt-get install -y tigervnc-standalone-server tigervnc-common xfce4
</code></pre><p>But now you won&rsquo;t access the vnc desktop, cause the default setting is only listening on 127.0.0.1:</p><pre><code># netstat -anp | grep 5901
tcp        0      0 127.0.0.1:5901          0.0.0.0:*               LISTEN      59902/Xtigervnc     
tcp6       0      0 ::1:5901                :::*                    LISTEN      59902/Xtigervnc 
</code></pre><p>Edit the vnc.conf file add set the listening port not to localhost, then setup
the xstartup files:</p><pre><code># sudo vim /etc/vnc.conf
Default: $localhost = &quot;no&quot;;  # Otherwise

# sudo vim ~/.vnc/xstartup
    #!/bin/sh
    unset SESSION_MANAGER
    unset DBUS_SESSION_BUS_ADDRESS
    exec startxfce4
# vncserver
</code></pre><p>Now open your favorate vnc viewer for connecting the remote desktop.</p><h3 id=3-bundle-use-aliyun>3. bundle use aliyun</h3><p>Via following:</p><pre><code># bundle config 'mirror.https://rubygems.org' 'https://ruby.taobao.org'
</code></pre><h3 id=4-netplan-bridge-setting>4. netplan bridge setting</h3><p>Via following:</p><pre><code># vim /etc/netplan/01-netcfg.yaml 
# This file describes the network interfaces available on your system
# For more information, see netplan(5).
#
network:
  version: 2
  renderer: networkd
  ethernets:
    enakkkc2i2:
      dhcp4: no
  bridges:
    br0:
      dhcp4: no
      addresses: [ 192.168.190.167/24 ]
      gateway4: 192.168.190.254
      nameservers:
          addresses:
              - &quot;192.168.190.254&quot;
      interfaces:
        - enakkkc2i2
#network:
#  version: 2
#  renderer: networkd
#  ethernets:
#    enakkkc2i2:
#      addresses: [ 192.168.190.167/24 ]
#      gateway4: 192.168.190.254
#      nameservers:
#          addresses:
#              - &quot;192.168.190.254&quot;

</code></pre><h3 id=5-shrink-lvm-volumenot-for-xfs>5. shrink lvm volume(not for xfs)</h3><p>CentOS7 installation with root only 50GB, shrink:</p><pre><code>umount /dev/mapper/centos-home
lvreduce -L 200G /dev/mapper/centos-home
Mount back your home partition as you're done with it.

Then just extend your root volume.

lvextend -t -r -l+100%FREE /dev/mapper/centos-root
-t is test, if it's ok just run the command a second time without -t
</code></pre><h3 id=6-dd-times>6. dd times</h3><p>It takes around 8 hours to copy to img files.</p><pre><code>➜  ~ sudo dd if=/dev/sdc | gzip -c &gt; /media/sda/kylin_fuck.img
1953525167+0 records in
1953525167+0 records out
1000204885504 bytes (1.0 TB, 932 GiB) copied, 28336.5 s, 35.3 MB/s
➜  ~ python
Python 2.7.17 (default, Nov  7 2019, 10:07:09) 
[GCC 7.4.0] on linux2
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; 28336/60
472
&gt;&gt;&gt; 

</code></pre><p>And its size if not so large:</p><pre><code>➜  ~ ls -l -h /media/sda/kylin_fuck.img 
-rw-r--r-- 1 dash dash 4.3G 3月  18 01:48 /media/sda/kylin_fuck.img
</code></pre><h3 id=7-alpine-china-repository>7. alpine china repository</h3><p>Fuck the GFW:</p><pre><code># sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories

</code></pre><h3 id=8-metrci-server-always-restart>8. metrci-server always restart</h3><p>for cpu/memory is low(vm cases), enlarge them:</p><pre><code>metrics_server_cpu: 400m
metrics_server_memory: 350Mi
metrics_server_memory_per_node: 40Mi
metrics_server_min_cluster_size: 5
addon_resizer_limits_cpu: 200m
addon_resizer_limits_memory: 600Mi
addon_resizer_requests_cpu: 50m
addon_resizer_requests_memory: 500Mi

</code></pre><h3 id=9-ssh-passwordless-configuration>9. ssh passwordless configuration</h3><p>Configure ssh login without input password:</p><pre><code>Check that your Centos machine has:

RSAAuthentication yes
PubkeyAuthentication yes
in sshd_config

and ensure that you have proper permission on the centos machine's ~/.ssh/ directory.

chmod 700 ~/.ssh/
chmod 600 ~/.ssh/*
should do the trick.
</code></pre><h3 id=10-tasksmax-limitation>10. Tasksmax limitation</h3><p>The TasksMax Systemd/Linux feature can cause various operational issues related to creating new processes including failures starting containers and failures setting up iptables rules for running containers. Customers affected by this issue will observe that the Docker daemon is unable to create more processes than the TasksMax configured limit.</p><p>Refers to :<br><a href=https://success.docker.com/article/how-to-reserve-resource-temporarily-unavailable-errors-due-to-tasksmax-setting>https://success.docker.com/article/how-to-reserve-resource-temporarily-unavailable-errors-due-to-tasksmax-setting</a></p><pre><code>systemctl set-property docker.service TasksMax=infinity
systemctl daemon-reload
systemctl restart docker
</code></pre><h3 id=11-bluetooth-for-vervebuds-115>11. bluetooth for verveBuds 115</h3><p>On Archlinux, do following:</p><pre><code># sudo pacman -S alsa-utils alsa-plugins alsa-tools bluez bluez-utils bluez-libs
pulseaudio blueman bluez-hid2hci pulseaudio-bluetooth audacious
# vim /etc/pulse/default.pa (Add following lines)
load-module module-switch-on-connect
# vim /etc/pulse/system.pa (Add following lines)
load-module module-bluetooth-policy
load-module module-bluetooth-discovery
</code></pre><p>Restart the bluetooth service:</p><pre><code># systemctl restart bluetooth
# pluseaudio -k
# pluseaudio --start
</code></pre><p>Blueman setup:</p><p><img src=/images/2020_04_07_10_04_32_474x167.jpg alt=/images/2020_04_07_10_04_32_474x167.jpg></p><p>In audacious, select the output for pulseaudio.</p><h3 id=12-untaint-kube-master>12. untaint kube-master</h3><p>via :</p><pre><code>kubectl taint nodes --all node-role.kubernetes.io/master-

</code></pre><h3 id=13-docker-compose-down-one-svc>13. docker-compose down one svc</h3><p>Via:</p><pre><code># docker-compose rm -f -s -v ui
# docker-compose up -d
</code></pre><h3 id=14-es-logs>14. es logs</h3><p>via:</p><pre><code># curl http://172.18.0.5:9200/_cat/indices
yellow open ko-log-2020.04  FQxfXJ0ASXid36DN_TiziA 1 1   0 0    283b    283b
yellow open ououojjj-2020.4 oBcDjztES_C1JFHhMK23JQ 1 1 830 0 312.3kb 312.3kb
$ curl http://172.18.0.5:9200/ko-log-2020.04/_count?pretty
{
  &quot;count&quot; : 0,
  &quot;_shards&quot; : {
    &quot;total&quot; : 1,
    &quot;successful&quot; : 1,
    &quot;skipped&quot; : 0,
    &quot;failed&quot; : 0
  }
}


</code></pre><h3 id=15-include-ansible-roles>15. include ansible roles</h3><p>via:</p><pre><code>---
- import_playbook: 0_preinstall/init.yml

- import_playbook: 1_k8s/cluster.yml

- import_playbook: 2_addons/addons.yml


</code></pre><h3 id=16-pool-define-in-virsh>16. pool define in virsh</h3><p>via following commands:</p><pre><code>sudo virsh pool-define-as nvme --type dir --target /media/nvme
sudo virsh pool-start nvme
sudo virsh pool-autostart nvme
</code></pre><h3 id=20-2004-vagrant-issue>20. 20.04 vagrant issue</h3><p>Should install <code>ifupdown</code> for letting vagrant working.</p><h3 id=21-write-inventory>21. write inventory</h3><pre><code>            with open('/tmp/clusterinventory.yaml', 'w+') as f:
                f.write( str(self.project.inventory_obj.parse_resource()) )
</code></pre><p>Error log:</p><pre><code>Traceback (most recent call last):
  File &quot;/opt/kubeOperator-api/apps/kubeops_api/models/deploy.py&quot;, line 62, in start
    result = self.on_install(extra_vars)
  File &quot;/opt/kubeOperator-api/apps/kubeops_api/models/deploy.py&quot;, line 169, in on_install
    return self.run_playbooks(extra_vars)
  File &quot;/opt/kubeOperator-api/apps/kubeops_api/models/deploy.py&quot;, line 271, in run_playbooks
    _result = playbook.execute(extra_vars=extra_vars)
  File &quot;/opt/kubeOperator-api/apps/ansible_api/models/playbook.py&quot;, line 272, in execute
    result = execution.start()
  File &quot;/opt/kubeOperator-api/apps/ansible_api/models/playbook.py&quot;, line 339, in start
    result = runner.run(self.playbook.playbook_path, extra_vars=extra_vars)
  File &quot;/opt/kubeOperator-api/apps/ansible_api/ansible/runner.py&quot;, line 248, in run
    self.variable_manager._extra_vars = extra_vars
AttributeError: can't set attribute
</code></pre><h3 id=22-duplicated-machine>22. Duplicated machine</h3><p>Via following commands we could re-generate the machine id:</p><pre><code>rm -f /etc/machine-id
dbus-uuidgen --ensure=/etc/machine-id
rm /var/lib/dbus/machine-id
dbus-uuidgen --ensure
</code></pre><h3 id=23-rename-files-with-special-characters>23. Rename files with special characters</h3><p><code>rename.sh</code> like following, could rename with the files with <code>1.mp4, 2.mp4, etc</code>:</p><pre><code>var=1
for file in *.mp4
do 
	echo &quot;$file&quot;
	echo $var
	mv &quot;$file&quot; $var.mp4
	let &quot;var=var+1&quot;
done

</code></pre><h3 id=24-tips-on-kubeoperator-package-management>24. tips on kubeoperator package management</h3><p>Edit file <code>apps/kubeops_api/models/package.py</code>, comment the start container
issue:</p><pre><code>line 67 - 71 should be commented
</code></pre><p>thus you will get your package management online.</p><h3 id=25-ss-for-listening>25. ss for listening</h3><p>without netstat, using ss for listening:</p><pre><code># sudo ss -tunlp
</code></pre><h3 id=251-docker-regsitry-issue>25.1. docker-regsitry issue</h3><p>registry v2.6 depends on musl, install it via:</p><pre><code># apt-get install -y musl
</code></pre><h3 id=26-ps-output-wide>26. ps output wide</h3><p>via :</p><pre><code># ps -efww
</code></pre><h3 id=27-install-source-code-pro-font>27. install source code pro font</h3><p>via following script:</p><pre><code>#!/usr/bin/env bash
cd Downloads
wget https://github.com/adobe-fonts/source-code-pro/archive/2.030R-ro/1.050R-it.zip
if [ ! -d &quot;~/.fonts&quot; ] ; then
    mkdir ~/.fonts
fi
unzip 1.050R-it.zip 
cp source-code-pro-*-it/OTF/*.otf ~/.fonts/
rm -rf source-code-pro* 
rm 1.050R-it.zip 
cd ~/
fc-cache -f -v
</code></pre><h3 id=28-install-puppeteer-problem>28. Install puppeteer problem</h3><p>Via:</p><pre><code>sudo npm install -g puppeteer --unsafe-perm=true
</code></pre><h3 id=29-css-for-layout>29. css for layout</h3><p>Via:</p><pre><code>iPadCSS控制横屏/竖屏布局（Landscape/PortraitModes）
</code></pre><h3 id=30-git-tag-and-push>30. git tag and push</h3><p>Via:</p><pre><code> git tag -a v0.4.2 -m &quot;v0.4.2 for ansible v2.9.6&quot;
 git show v0.4.2
 git push origin master --tags
</code></pre><h3 id=31-indesign>31. indesign</h3><p>Adobe indesign for creating pdf could get good effects.</p><h3 id=32-tips-on-rpi-ubuntu18044>32. Tips On rpi ubuntu18.04.4</h3><p>Via following for getting the packages:</p><pre><code># sudo su
# systemctl stop apt-daily.timer;systemctl disable apt-daily.timer ; systemctl stop apt-daily-upgrade.timer ; systemctl disable apt-daily-upgrade.timer; systemctl stop apt-daily.service;  systemctl mask apt-daily.service; systemctl daemon-reload
# sudo lsof /var/lib/dpkg/lock
# sudo lsof /var/lib/apt/lists/lock
# sudo lsof /var/cache/apt/archives/lock
# sudo kill -9 PID
</code></pre><h3 id=33-change-default-python-in-ubuntu1804>33. Change default python in ubuntu18.04</h3><pre><code># apt-get install python3.7
# update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 1
# update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 2
# update-alternatives --config python3
There are 2 choices for the alternative python3 (providing /usr/bin/python3).

  Selection    Path                Priority   Status
------------------------------------------------------------
* 0            /usr/bin/python3.7   2         auto mode
  1            /usr/bin/python3.6   1         manual mode
  2            /usr/bin/python3.7   2         manual mode
Press &lt;enter&gt; to keep the current choice[*], or type selection number: 2
# update-alternatives --install /usr/bin/python python /usr/bin/python3 10
# python3
Python 3.7.5 (default, Nov  7 2019, 10:50:52) 
[GCC 8.3.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; 
# python
Python 3.7.5 (default, Nov  7 2019, 10:50:52) 
[GCC 8.3.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; quit()
</code></pre><p>Install correspoding packages:</p><pre><code># apt-get install python3.7-dev python3.7-venv libpython3.7 libpython3.7-dev libpython3.7-dbg
</code></pre><h3 id=34-upgrade-to-latest-nodejs>34. upgrade to latest nodejs</h3><p>Via following methods:</p><pre><code># cnpm install -g n
# n latest
</code></pre><p>Then you could use <code>ng new Angular8ClientCrud</code>.</p><pre><code># npm install -g @angular/cli
</code></pre><h3 id=35-python-venv>35. python venv</h3><p>Ubuntu 18.04 install and configure via:</p><pre><code># apt-get install -y python3-virtualenv
# python3 -m venv piya_venv
# source piya_venv/bin/activate
(piya_venv) root@build:~/Code/piya# which python
/root/Code/piya/piya_venv/bin/python
</code></pre><h3 id=36-nvm>36. nvm</h3><p>Install nvm via:</p><pre><code>$ yaourt nvm
</code></pre><p>Then</p><pre><code>$ echo 'source /usr/share/nvm/init-nvm.sh' &gt;&gt; ~/.zshrc
$ which nvm
...
$ nvm install 10.18.0
then you have nodejs 10.18.0 version.
</code></pre><h3 id=37-npm-install-priviledge>37. npm install priviledge</h3><p>Run npm install as root, do following:</p><pre><code>$ npm install -g --unsafe-perm
</code></pre><h3 id=38-dnsmasq-leases-file>38. dnsmasq leases file</h3><p>In <code>/var/lib/misc/dnsmasq.leases</code> file, you could see all of the leased ip address.</p><h3 id=39-docker-pullother-arch>39. docker pull(other arch)</h3><p>via:</p><pre><code># docker pull centos:7@sha256:aogwuoguwougowuoguwoeguowugouwg
</code></pre><h3 id=40-boomaga-on-archlinux>40. boomaga on archlinux</h3><p>Install cups and cups-pdf then building:</p><pre><code># sudo pacman -S cups cups-pdf
# sudo systemctl enable org.cups.cupsd.service
# sudo systemctl start org.cups.cupsd.service
# yaourt boomaga
</code></pre><p>After this time&rsquo;s yaourt boomaga will be system-wide available.</p><h3 id=41-zip-with-password>41. zip with password</h3><p>via:</p><pre><code># zip -re aaa.zip aaa/
</code></pre><h3 id=42-vncserver-listen-only-on-local>42. vncserver listen only on local</h3><p>start vncserver via:</p><pre><code>alias vncserver='vncserver -localhost'
vncserver
</code></pre><h3 id=43-nvm-tips>43. nvm tips</h3><p>Sometips:</p><pre><code>→ nvm ls
         v4.5.0
-&gt;       v5.9.0
         system
node -&gt; stable (-&gt; v5.9.0) (default)
stable -&gt; 5.9 (-&gt; v5.9.0) (default)
iojs -&gt; N/A (default)
# nvm use v4.5.0
# nvm current
system
# node -v
</code></pre><h3 id=44-helm-tips>44. helm tips</h3><p>helm install from local directory (helm v3):</p><pre><code>helm install --generate-name cilium  --namespace kube-system    --set global.etcd.enabled=true    --set global.etcd.managed=true
</code></pre><h3 id=45-install-hwe>45. Install hwe</h3><p>Via following command:</p><pre><code># sudo apt-get install -y linux-generic-hwe-18.04
</code></pre><h3 id=46-xz-and-write-to-sd>46. xz and write to sd</h3><p>via following command:</p><pre><code># xz -d &lt; bone-debian-10.3-iot-armhf-2020-04-06-4gb.img.xz - | dd of=/dev/sdb
</code></pre><h3 id=47-pikvm-思路>47. pikvm 思路</h3><p>首先是有rpi 3b ,后面买了hdmid dongle和teensy 2.0.
开始玩pikvm后:</p><ol><li>v4l udev调试, 平安</li><li>arduino它用的pro micro, 卒</li><li>platformio更改为teensy，编译不通过，卒</li><li>看代码发现依赖的HID-Project库不支持teensy，卒</li><li>看HID-Project说支持HoodLoader2的Uno，希望燃起</li><li>platformio编译uno，卒</li><li>platformio中引入自定义board HoodLoader2atmega16u2, 希望燃起</li><li>无USB PID/VID，编译终止，卒
9, 强行传入PID/VID， 希望燃起</li><li>TimerOne库不支持atmega16u2， 卒</li><li>强行定义Timer1管脚，希望燃起</li><li>编译通过。未上板验证。</li></ol><p>接下来：</p><ol><li>刷HoodLoader2</li><li>连接RPI+Uno</li><li>调试(是否可刷入？Timer0是否正常？USB是否被识别？）</li></ol><p>问题重重，风险重重</p><h3 id=48-redirect-usb-to-virtualbox>48. Redirect usb to virtualbox</h3><p>Added via following command:</p><pre><code>sudo usermod -aG vboxusers dash
</code></pre><h3 id=49-install-latest-virtualbox>49. Install latest virtualbox</h3><pre><code>echo &quot;deb https://download.virtualbox.org/virtualbox/debian $(lsb_release -cs) contrib&quot; | sudo tee /etc/apt/sources.list.d/virtualbox.list
</code></pre><h3 id=50-platformio-issue>50. platformio issue</h3><p>Issue:</p><pre><code>/root/.platformio/packages/tool-avrdude/avrdude: error while loading shared libraries: /usr/lib/libtinfo.so.5: file too short
</code></pre><p>Solved by:</p><pre><code>sudo ln -s /usr/lib64/libtinfo.so.6 /usr/lib64/libtinfo.so
</code></pre><h3 id=51-build-pkgarch>51. build pkg(Arch)</h3><p>Via followinig command:</p><pre><code>$ cd ~/builds &amp;&amp; yaourt -G pkgname &amp;&amp; cd $pkgname &amp;&amp; makepkg
</code></pre><h3 id=52-diffpatch>52. diff/patch</h3><p>via following commands:</p><pre><code> cd kvmd-1.82-x86
 rm -f ../x86.patch
 diff -Naru ../kvmd-1.82 . &gt; ../x86.patch
 cp -r kvmd-1.82 test
 cd test
 patch -s -p0 &lt; ../x86.patch
</code></pre><h3 id=53-centos-76-ssh-too-slow>53. centos 76 ssh too slow</h3><p>Edit the <code>/etc/ssh/sshd_config</code>, for:</p><pre><code>GSSAPIAuthentication no
UseDNS	no
</code></pre><h3 id=54-create-md0-in-arm64>54. Create md0 in arm64</h3><p>Via following commands:</p><pre><code># cat /proc/mdstat
# lsblk -o NAME,SIZE,FSTYPE,TYPE,MOUNTPOINT
NAME                SIZE FSTYPE      TYPE MOUNTPOINT
sda               557.9G             disk 
├─sda1              512M vfat        part /boot/efi
└─sda2            557.4G LVM2_member part 
  ├─vgnode-root   556.4G xfs         lvm  /
  └─vgnode-swap_1   980M swap        lvm  
sdb                 3.3T             disk 
└─sdb1              3.3T ext4        part 
sdc                 3.3T             disk 
└─sdc1              3.3T ext4        part 
sdd                 3.3T             disk 
└─sdd1              3.3T ext4        part 
sde                 3.3T             disk 
└─sde1              3.3T ext4        part 
sdf                 3.3T             disk 
└─sdf1              3.3T ext4        part 
sdg                 3.3T             disk 
└─sdg1              3.3T ext4        part 
# parted /dev/sdc
GNU Parted 3.3
Using /dev/sdc
Welcome to GNU Parted! Type 'help' to view a list of commands.
(parted) p                                                                
Model: AVAGO HW-SAS3508 (scsi)
Disk /dev/sdc: 3598GB
Sector size (logical/physical): 512B/4096B
Partition Table: gpt
Disk Flags: 

Number  Start   End     Size    File system  Name     Flags
 1      1049kB  3598GB  3598GB  ext4         logical  msftdata

(parted) rm 1                                                             
(parted) q                                                                
Information: You may need to update /etc/fstab.
# parted /dev/sdb
# parted /dev/sdd
# parted /dev/sdf
# parted /dev/sdg
# parted /dev/sde
# lsblk -o NAME,SIZE,FSTYPE,TYPE,MOUNTPOINT
NAME                SIZE FSTYPE      TYPE MOUNTPOINT
sda               557.9G             disk 
├─sda1              512M vfat        part /boot/efi
└─sda2            557.4G LVM2_member part 
  ├─vgnode-root   556.4G xfs         lvm  /
  └─vgnode-swap_1   980M swap        lvm  
sdb                 3.3T             disk 
sdc                 3.3T             disk 
sdd                 3.3T             disk 
sde                 3.3T             disk 
sdf                 3.3T             disk 
sdg                 3.3T             disk 
# mdadm --create --verbose /dev/md0 --level=0 --raid-devices=6 /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg
# cat /proc/mdstat 
Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid0 sdg[5] sdf[4] sde[3] sdd[2] sdc[1] sdb[0]
      21078902784 blocks super 1.2 512k chunks
      
unused devices: &lt;none&gt;
# mkfs.ext4 -F /dev/md0
# mkdir -p /media/md0
# mount /dev/md0 /media/md0
# df -h -x devtmpfs -x tmpfs
# sudo mdadm --detail --scan | sudo tee -a /etc/mdadm/mdadm.conf
# echo '/dev/md0 /mnt/md0 ext4 defaults,nofail,discard 0 0' | sudo tee -a /etc/fstab
</code></pre><h3 id=55-nvidia-xorg-howto>55. Nvidia Xorg Howto</h3><p>Install <code>nvidia</code> from pacman, or <code>nvidia-lts</code> is also ok.
via <code>lspci</code> to detect the graphic card&rsquo;s BusID:</p><pre><code>$ lspci | egrep 'VGA|3D'
00:02.0 VGA compatible controller: Intel Corporation UHD Graphics 630 (Mobile)
01:00.0 VGA compatible controller: NVIDIA Corporation GP106M [GeForce GTX 1060 Mobile] (rev a1)
</code></pre><p>Edit the <code>xorg.conf</code> file:</p><pre><code>$ sudo vim /etc/X11/xorg.conf
Section &quot;Module&quot;                                                      #可能没有，自行添加
    load &quot;modesetting&quot;
EndSection

Section &quot;Device&quot;
    Identifier     &quot;Device0&quot;
    Driver         &quot;nvidia&quot;
    VendorName     &quot;NVIDIA Corporation&quot;
    BusID          &quot;1:0:0&quot;                           #此处填刚刚查询到的BusID
    Option         &quot;AllowEmptyInitialConfiguration&quot;
EndSection
</code></pre><p>Edit the <code>mkinitcpio.conf</code>:</p><pre><code>$ sudo vim /etc/mkinitcpio.conf
MODULES=(intel_agp i915 nvidia nvidia_modeset nvidia_uvm  nvidia_drm)
$ sudo mkinitcpio -P
$ sudo vim /etc/default/grub
GRUB_CMDLINE_LINUX_DEFAULT=&quot;loglevel=3 quiet nvidia-drm.modeset=1&quot;
$ sudo grub-mkconfig -o /boot/grub/grub.cfg
$ sudo vim /etc/modprobe.d/nouveau_blacklist.conf
blacklist nouveau
</code></pre><h3 id=56-go-speedup>56. go speedup</h3><p>Via following command, fast:</p><pre><code>go env -w GO111MODULE=on 
go env -w GOPROXY=https://goproxy.cn,direct
</code></pre><h3 id=57-dnsmasq-leased-ip-addr>57. dnsmasq leased ip addr</h3><p>Via cat following files:</p><pre><code>/var/lib/misc/dnsmasq.leases
</code></pre><h3 id=58-lxd-cluster>58. lxd cluster</h3><p>via following commands:</p><pre><code># root@arm-a1:/home/test/focal_desktop# lxd init
Would you like to use LXD clustering? (yes/no) [default=no]: yes
What name should be used to identify this node in the cluster? [default=arm-a1]:
What IP address or DNS name should be used to reach this node? [default=192.192.190.163]:
Are you joining an existing cluster? (yes/no) [default=no]: ys
Invalid input, try again.

Are you joining an existing cluster? (yes/no) [default=no]: yes
IP address or FQDN of an existing cluster node: 192.192.190.165
Cluster fingerprint: 4ce809765a307ee90593495413f0dc919f30b6709e5d9a16275fac8e66b407c5
You can validate this fingerprint by running &quot;lxc info&quot; locally on an existing node.
Is this the correct fingerprint? (yes/no) [default=no]: yes
Cluster trust password:
All existing data is lost when joining a cluster, continue? (yes/no) [default=no] yes
Choose &quot;source&quot; property for storage pool &quot;local&quot;:
Would you like a YAML &quot;lxd init&quot; preseed to be printed? (yes/no) [default=no]:
root@arm-a1:/home/test/focal_desktop#
</code></pre><h3 id=59-lxc-issue>59. lxc issue</h3><p>for installing kubernetes in lxc, should not change the conntrack parameters!:</p><pre><code>kube_proxy_conntrack_max: 0
kube_proxy_conntrack_max_per_core: 0
</code></pre><h3 id=60-svn-working-tips>60. svn working tips</h3><p>In intra-net working like:</p><pre><code>$ export LC_ALL=zh_CN.UTF-8
$ svn co --username=xxxx --password=xxxx123 https://192.192.xxx.xxx/svn/trunk/xxxx/cloud
$ cd cloud
$ svn add xxxx.zip yyyy.zip
$ svn ci -m &quot;Added new release files in 20200925&quot;
$  svn add --force * --auto-props --parents --depth infinity -q  
$ svm commit -m 'Adding more file'
</code></pre><h3 id=61-pdf-to-png>61. pdf to png</h3><p>via following command you could convert pdf to pngs and cut its margin:</p><pre><code>pdftoppm Rong.pdf B2 -png -x 110 -y 65 -W 1050 -H 1500
</code></pre><h3 id=62-manually-update-time>62. Manually update time</h3><p>Via:</p><pre><code>ntpd -s -d
</code></pre><h3 id=63-manually-delete-journal>63. Manually delete journal</h3><p>Manually delete journals thus the size if under 100M:</p><pre><code># journalctl --vacuum-size=100M
</code></pre><p>Clean logs before 2 weeks:</p><pre><code># journalctl --vacuum-time=2weeks
</code></pre><h3 id=64-specify-ssh-exchange-method>64. Specify ssh exchange method</h3><p>issue:</p><pre><code>no matching key exchange method found. Their offer: diffie-hellman-group1-sha1 while accessing the TEA shell from ssh client
</code></pre><p>Solution:</p><pre><code># ssh -oKexAlgorithms=+diffie-hellman-group1-sha1  root@xxx.xxx.xxx.xx
</code></pre><h3 id=65-vxlan-tips>65. vxlan tips</h3><p>Via:</p><pre><code>  ip link add vxlan0 type vxlan id 1 group 239.1.1.1 dstport 4789 dev enp130s0f1
 bridge fdb append to 00:00:00:00:00:00 dst 192.192.xxx.xx7 dev vxlan0
 bridge fdb append to 00:00:00:00:00:00 dst 192.192.xxx.xx9 dev vxlan0
ip addr add 10.0.0.7/24 dev vxlan0
brctl addbr br-kkk
ip link set br-kkk up
brctl addif br-kkk vxlan0
</code></pre><h3 id=66-repair-127-servers-disk>66. Repair 127 server&rsquo;s disk</h3><p>Centos7, <code>/dev/mapper/cl-root</code>, repair via using a livecd bootup the server then in terminal input:</p><pre><code>$ sudo xfs_repair -L /dev/mapper/cl-root
</code></pre><h3 id=67-vxlan>67. vxlan</h3><p>Via:</p><pre><code>ip link add vxlan0 type vxlan id 42 dstport 4789 local 192.192.ddd.ddd dev br0 group 224.1.1.1
ip link add br-vxlan0 type bridge
ip link set vxlan0 master br-vxlan0
ip link add vrf0 type vrf table 10
ip link set br-vxlan0 master vrf0
ip link set vxlan0 up
ip link set br-vxlan0 up
ip link set vrf0 up
</code></pre><h3 id=68-rclocal-in-centos7>68. rc.local in centos7</h3><p>Via following method you could make <code>/etc/rc.local</code> working:</p><pre><code>chmod +x /etc/rc.d/rc.local
systemctl enable rc-local
systemctl start rc-local
systemctl status rc-local
</code></pre><h3 id=69-archlinux-byobu-issue>69. archlinux byobu issue</h3><p>When running byobu we got following issues:</p><pre><code>tmux: need UTF-8 locale (LC_CTYPE) but have ANSI_X3.4-1968
</code></pre><p>solved via:</p><pre><code># sudo vim /etc/locale.gen
# locale-gen
# sudo localectl set-locale LANG=en_CA.UTF-8

### 69.1. Enable sidebar toc of MPE
Setting the Markdown Preview Enhanced setting:   

![/images/2020_10_20_17_17_22_280x143.jpg](/images/2020_10_20_17_17_22_280x143.jpg)

![/images/2020_10_20_17_17_41_612x389.jpg](/images/2020_10_20_17_17_41_612x389.jpg)

Then you could get the sidebar toc

### 70. Remove docker service(manually)
via:    

</code></pre><p>rm -f /etc/systemd/system/multi-user.target.wants/docker.service</p><pre><code>
### 71. undo last commit
git undo last commitment via:    

</code></pre><p>git reset &ndash;soft HEAD~1</p><pre><code>
### 72. delete tmp files:    
Via find :   

</code></pre><p>find ./ -type f ( -name &lsquo;*.swp&rsquo; -o -name &lsquo;<em>~&rsquo; -o -name &lsquo;</em>.bak&rsquo; -o -name &lsquo;.netrwhist&rsquo; ) -delete</p><pre><code>
### 73. vncviewer without passwd
Generate password:    

</code></pre><p>$ vncpasswd
Using password file /home/user/.vnc/passwd
Password:
Verify:
Would you like to enter a view-only password (y/n)? n</p><pre><code>Login with password file:    

</code></pre><h1 id=vncviewer--passwd-vncpasswd-1013714922>vncviewer -passwd ~/.vnc/passwd 10.137.149.2:2</h1><pre><code>
### 74. DPMS
Display Power Management Signaling, disable it via:    

</code></pre><p>xset -dpms</p><pre><code>Query it via:    

</code></pre><p>xset q</p><pre><code>Re-enable it via:    

</code></pre><p>xset +dpms</p><pre><code>### 75. ufw open port
via:    

</code></pre><p>$ sudo ufw allow from any to any port 10000 proto tcp</p><pre><code>
### 76. windows route via net
via:    

</code></pre><h1 id=route--p-add-1017180-mask-2552552550-192192189128>route -p add 10.17.18.0 MASK 255.255.255.0 192.192.189.128</h1><pre><code>
### 77. view timestamp of pem
via :    

</code></pre><h1 id=openssl-x509--enddate--noout--in-keysadminpem>openssl x509 -enddate -noout -in keys/admin.pem</h1><p>notAfter=Feb 11 02:06:00 2020 GMT</p><pre><code>
### 78. ia32 support
ia32 support in bionic:   

</code></pre><p>$ sudo dpkg &ndash;add-architecture i386
$ sudo apt update
$ sudo apt install libc6:i386</p><pre><code>
### 79. user run docker
via:    

</code></pre><p>sudo usermod -aG docker dash
sudo systemctl restart docker
#relogin dash</p><pre><code>
### 80. CentOS8 repo
edit centos8 repo using cdrom:    

</code></pre><p>[InstallMedia-BaseOS]
name=CentOS Linux 8 - BaseOS
metadata_expire=-1
gpgcheck=1
enabled=1
baseurl=file:///opt/BaseOS/
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficial</p><p>[InstallMedia-AppStream]
name=CentOS Linux 8 - AppStream
metadata_expire=-1
gpgcheck=1
enabled=1
baseurl=file:///opt/AppStream/
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficial</p><pre><code>
### 81. kubeadm 签名问题
查看签名所剩时间并重新更新签名时间:    

</code></pre><h1 id=kubeadm-alpha-certs-check-expiration>kubeadm alpha certs check-expiration</h1><h1 id=kubeadm-alpha-certs-renew-allkubeadm---configkubeadm-configyaml>kubeadm alpha certs renew all=kubeadm &ndash;config=kubeadm-config.yaml</h1><h1 id=kubeadm-alpha-certs-check-expiration-1>kubeadm alpha certs check-expiration</h1><pre><code>
### 82. kubelet auto approve
via adding:    

</code></pre><p>&ndash;rotate-certificates &ndash;rotate-server-certificates</p><p>my kubelet.config</p><p>Environment="KUBELET_ARGS=&ndash;rotate-certificates &ndash;rotate-server-certificates &ndash;cert-dir=/var/lib/kubelet/pki &ndash;logtostderr=true &ndash;v=3&rdquo;</p><pre><code>
### 83. kubeadm check timeout
via:    

</code></pre><p>kubeadm alpha certs check-expiration</p><pre><code>
### 84. sync time
syn time via ssh :    

</code></pre><h1 id=ssh--o-stricthostkeycheckingno--i-rongdeploykey-root10137149232-date--s--date--u-s->ssh -o &ldquo;StrictHostKeyChecking=no&rdquo; -i .rong/deploy.key <a href=mailto:root@10.137.149.232>root@10.137.149.232</a> date -s @<code>( date -u +"%s" )</code></h1><pre><code>
### 85. limit conn per ip via ufw
via following commands:    

</code></pre><h1 id=vim-etcufwbeforerules>vim /etc/ufw/before.rules</h1><h1 id=end-required-lines>End required lines</h1><h1 id=limit-to-5-concurrent-connections-on-port-80-per-ip>Limit to 5 concurrent connections on port 80 per IP</h1><p>-A ufw-before-input -p tcp &ndash;syn &ndash;dport 80 -m connlimit &ndash;connlimit-above 5 -j DROP</p><h1 id=limit-to-10-connections-on-port-80-per-3-seconds-per-ip>Limit to 10 connections on port 80 per 3 seconds per IP</h1><p>-A ufw-before-input -p tcp &ndash;dport 80 -i eth0 -m state &ndash;state NEW -m recent &ndash;set
-A ufw-before-input -p tcp &ndash;dport 80 -i eth0 -m state &ndash;state NEW -m recent &ndash;update &ndash;seconds 3 &ndash;hitcount 10 -j DROP</p><h1 id=allow-all-on-loopback>allow all on loopback</h1><h1 id=ufw-allow-3333tcp>ufw allow 3333/tcp</h1><h1 id=ufw-enable>ufw enable</h1><pre><code>
### 86. ss setup
via:    

</code></pre><p>docker run -d -p 9000:9000 -p 9000:9000/udp &ndash;name ss-libev &ndash;restart=always -v /etc/shadowsocks-libev:/etc/shadowsocks-libev teddysun/shadowsocks-libev
ufw allow 9000/udp
ufw allow 9000/tcp</p><pre><code>
### 87. ufw security vps
via:    

</code></pre><p>sudo ufw default deny outgoing
sudo ufw default deny incoming
sudo ufw allow out 53
sudo ufw allow out http
sudo ufw allow out https
&mldr;..</p><pre><code>examine:    

</code></pre><h1 id=ufw-status-verbose>ufw status verbose</h1><pre><code>My configuration is: 

</code></pre><p>in: 9345, 22
out: 9345, 80 , 443, 53</p><pre><code>
### 88. check serve(vm or not)
via dmidecode:    

</code></pre><p>$ sudo dmidecode -s system-manufacturer<br>Hasee Computer
$ sudo dmidecode -s system-product-name
GJ5CN64</p><pre><code>
### 89. vagrant mutate box
Before:    

</code></pre><h1 id=vagrant-box-list>vagrant box list</h1><p>bento/centos-7.6 (virtualbox, 201907.24.0)
generic/ubuntu1804 (virtualbox, 0)</p><h1 id=vagrant-mutate-genericubuntu1804-libvirt>vagrant mutate generic/ubuntu1804 libvirt</h1><pre><code>After:    

</code></pre><h1 id=vagrant-box-list-1>vagrant box list</h1><p>bento/centos-7.6 (virtualbox, 201907.24.0)
generic/ubuntu1804 (libvirt, 0)
generic/ubuntu1804 (virtualbox, 0)</p><pre><code>
### 90. docker in china
via:    

</code></pre><p>{
&ldquo;registry-mirrors&rdquo;: [
&ldquo;https://docker.mirrors.ustc.edu.cn&rdquo;,
&ldquo;http://hub-mirror.c.163.com&rdquo;
],
&ldquo;max-concurrent-downloads&rdquo;: 10,
&ldquo;log-driver&rdquo;: &ldquo;json-file&rdquo;,
&ldquo;log-level&rdquo;: &ldquo;warn&rdquo;,
&ldquo;log-opts&rdquo;: {
&ldquo;max-size&rdquo;: &ldquo;10m&rdquo;,
&ldquo;max-file&rdquo;: &ldquo;3&rdquo;
},
&ldquo;data-root&rdquo;: &ldquo;/var/lib/docker&rdquo;
}</p><pre><code>
### 91. Use aliyun for installing docker-ce
via:    

</code></pre><h1 id=step-1-安装必要的一些系统工具>step 1: 安装必要的一些系统工具</h1><p>sudo apt-get update
sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common</p><h1 id=step-2-安装gpg证书>step 2: 安装GPG证书</h1><p>curl -fsSL <a href=https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg>https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg</a> | sudo apt-key add -</p><h1 id=step-3-写入软件源信息>Step 3: 写入软件源信息</h1><p>sudo add-apt-repository &ldquo;deb [arch=amd64] <a href=https://mirrors.aliyun.com/docker-ce/linux/ubuntu>https://mirrors.aliyun.com/docker-ce/linux/ubuntu</a> $(lsb_release -cs) stable&rdquo;</p><h1 id=step-4-更新并安装docker-ce>Step 4: 更新并安装Docker-CE</h1><p>sudo apt-get -y update
sudo apt-get -y install docker-ce</p><pre><code>
### 92. use aliyun for install docker-ce on rhel7
via:    

</code></pre><h1 id=vi-etcyumpluginconfdsubscription-managerconf>vi /etc/yum/pluginconf.d/subscription-manager.conf</h1><p>enabled=0</p><h1 id=vi-etcyumreposdcentos7repo>vi /etc/yum.repos.d/Centos7.repo</h1><p>[base]
name=CentOS-$releasever - Base - 163.com
#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=os
baseurl=http://mirrors.163.com/centos/7/os/$basearch/
gpgcheck=1
gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-7</p><p>#released updates
[updates]
name=CentOS-$releasever - Updates - 163.com
#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=updates
baseurl=http://mirrors.163.com/centos/7/updates/$basearch/
gpgcheck=1
gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-7</p><p>#additional packages that may be useful
[extras]
name=CentOS-$releasever - Extras - 163.com
#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=extras
baseurl=http://mirrors.163.com/centos/7/extras/$basearch/
gpgcheck=1
gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-7</p><p>#additional packages that extend functionality of existing packages
[centosplus]
name=CentOS-$releasever - Plus - 163.com
baseurl=http://mirrors.163.com/centos/7/centosplus/$basearch/
gpgcheck=1
enabled=0
gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-7</p><h1 id=sudo-yum-install--y-yum-utils-device-mapper-persistent-data-lvm2>sudo yum install -y yum-utils device-mapper-persistent-data lvm2</h1><h1 id=sudo-yum-config-manager---add-repo-httpmirrorsaliyuncomdocker-celinuxcentosdocker-cerepo>sudo yum-config-manager &ndash;add-repo <a href=http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo>http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</a></h1><h1 id=sudo-yum-makecache-fast>sudo yum makecache fast</h1><h1 id=sudo-yum--y-install-docker-ce>sudo yum -y install docker-ce</h1><pre><code>
###  93. gem sources
Use china repository:    

</code></pre><p>gem sources &ndash;add <a href=https://mirrors.tuna.tsinghua.edu.cn/rubygems/>https://mirrors.tuna.tsinghua.edu.cn/rubygems/</a> &ndash;remove <a href=https://rubygems.org/>https://rubygems.org/</a></p><pre><code>Examine via:    

</code></pre><h1 id=gem-sources--l>gem sources -l</h1><p>*** CURRENT SOURCES ***</p><p><a href=https://mirrors.tuna.tsinghua.edu.cn/rubygems/>https://mirrors.tuna.tsinghua.edu.cn/rubygems/</a></p><pre><code>### 94. zip with folder
via:     

</code></pre><p>zip -r archivename.zip directory_name</p><pre><code>
### 95. helm issues
For repository has been changed:    

</code></pre><h1 id=helm-repo-add-incubator--httpschartshelmshincubator>helm repo add incubator <a href=https://charts.helm.sh/incubator>https://charts.helm.sh/incubator</a></h1><h1 id=mkdir-tmp>mkdir tmp</h1><h1 id=cd-tmp>cd tmp/</h1><h1 id=helm-pull-incubatorsparkoperator>helm pull incubator/sparkoperator</h1><h1 id=ls>ls</h1><p>sparkoperator-0.8.6.tgz</p><pre><code>Transfer the tgz to k8s system, install via:    

</code></pre><h1 id=tar-xzvf-sparkoperator-086tgz>tar xzvf sparkoperator-0.8.6.tgz</h1><h1 id=cd-sparkoperator>cd sparkoperator</h1><h1 id=helm-install---namespace-spark-operator---set-enablebatchschedulertrue---set-enablewebhooktrue->helm install &ndash;namespace spark-operator &ndash;set enableBatchScheduler=true &ndash;set enableWebhook=true .</h1><h1 id=kubectl-get-pods---all-namespaces>kubectl get pods &ndash;all-namespaces</h1><h1 id=kubectl-get-pods--n-spark-operator>kubectl get pods -n spark-operator</h1><p>NAME READY STATUS RESTARTS AGE
brown-joey-sparkoperator-87868577f-68cts 0/1 ContainerCreating 0 67s
brown-joey-sparkoperator-webhook-init-85ndb 0/1 ErrImagePull 0 67s</p><pre><code>
### 96. codeblock in mardkown preview enhanced
换行:     

</code></pre><p>F1 => Markdown Preview Enhanced : Customize CSS</p><p>Then, in the style.less:</p><p>.markdown-preview.markdown-preview {
pre, code {
white-space: pre-wrap;
}
}
should enable wrapping for code blocks.</p><pre><code>
### 97. startup timeout
problem:   

</code></pre><p>A start Job is running for sys-devices-virtual-mis-vmbus\x21hv_kvp.devices</p><pre><code>
via:    

</code></pre><h1 id=systemctl-disable-hv-kvp-daemonservice>systemctl disable hv-kvp-daemon.service</h1><pre><code>
### 98. landscape info
via following command we could get landscape info:    

</code></pre><h1 id=landscape-sysinfo>landscape-sysinfo</h1><p>System load: 1.48 Temperature: 66.7 C
Usage of /: 8.9% of 27.91GB Processes: 134
Memory usage: 7% Users logged in: 1
Swap usage: 0% IPv4 address for eth0: 192.168.1.191</p><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><p>6178707df429ac1cc84f27a300e54265b6bad107</p></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><pre><code></code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/02/28/%E5%A6%82%E6%9E%9C%E4%BA%92%E8%81%94%E7%BD%91%E6%9C%89%E8%AE%B0%E5%BF%86/>如果互联网有记忆</a></h1><span class=post-date>Feb 28, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/life>Life</a></span><p>下午在家里等编译完成，忽然想看看有没有工具可以将微博上的文章都批量抓取下来，一搜，果然有：</p><p><a href=https://github.com/dataabc/weiboSpider>https://github.com/dataabc/weiboSpider</a></p><p>简单的配置以后开始了漫长的下载过程，</p><p><img src=/images/2020_02_28_15_24_29_774x758.jpg alt=/images/2020_02_28_15_24_29_774x758.jpg></p><p>分页后大概有1400多页，估计有半个小时就能生成归档。</p><p>微博近些年来被阉割得不行了，什么都不能说，偶尔看到的一点稍微敏感点的东西马上就无法访问。照此下去，没准哪一天它就突然被全网关停我一定都不会奇怪。我的微博账户早年因为寿光大水的话题多说了两句话已经被封。所以想用工具将以前说过的东西都保留下来。很多当时说过的话、转过的话题、有过的情绪，再看起来也只能嗟叹了。尽管如此，记忆还是应该被保留下来的。年老的时候偶尔翻起，会重温起年轻时的golden old days.</p><p>多么抓狂天真而幼稚的文字啊，也不亏我好好度完了这一生&mdash;-那时候的我一定会这么想吧。</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/02/26/codechangesinofflinekubespray/>CodeChangesInOfflineKubespray</a></h1><span class=post-date>Feb 26, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>For setting listor storage on kubernetes offlinely.</p><h3 id=linstor-package-preparation>Linstor Package Preparation</h3><p>Fetch the deb pkgs in docker for offline usage:</p><pre><code>add-apt-repository ppa:linbit/linbit-drbd9-stack
apt-get update

apt install -y drbd-dkms drbd-utils
</code></pre><p>Transfer the packages onto the deploy node and update the repository, then install in all of the nodes via:</p><pre><code># apt-get update -y &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get install -y drbd-dkms drbd-utils 2&gt;&amp;1 
</code></pre><p><code>noninteractive</code> make sure the postfix in default configuration.</p><h3 id=storage-preparation>Storage Preparation</h3><p>Create qcow2 files for vm usage:</p><pre><code># qemu-img create -f qcow2 /media/sda/listor1.qcow2 500G
# qemu-img create -f qcow2 /media/sdb/listor1.qcow2 500G
# qemu-img create -f qcow2 /media/sdc3/listor1.qcow2 500G
</code></pre><p>Attach them to vms:</p><p><img src=/images/2020_02_29_11_00_05_541x322.jpg alt=/images/2020_02_29_11_00_05_541x322.jpg></p><p>Each of the node have the vdb installed:</p><pre><code># fdisk -l
Disk /dev/vdb: 500 GiB, 536870912000 bytes, 1048576000 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
</code></pre><p>Create pv, vg, lv via following commands:</p><pre><code># pvcreate /dev/vdb &amp;&amp; vgcreate vg /dev/vdb &amp;&amp; lvcreate -l 100%FREE --thinpool vg/lvmthinpool
</code></pre></div><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/52/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/52/>52</a></li><li class="page-item active"><a class=page-link href=/page/53/>53</a></li><li class=page-item><a class=page-link href=/page/54/>54</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/243/>243</a></li><li class=page-item><a href=/page/54/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/243/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div><script src=http://purplepalmdash.github.io/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>