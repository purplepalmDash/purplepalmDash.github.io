<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta name=generator content="Hugo 0.64.0"><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Dash &#183; Dash</title><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124"><link rel=stylesheet href=https://purplepalmdash.github.io/css/highlight/googlecode.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js></script><script type=text/javascript src=/js/html2canvas.js></script><script type=text/javascript>function genPostShot(){var rightNow=new Date();var imageName=rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");imageName+='.jpg'
html2canvas(document.getElementsByClassName('post'),{background:'#FFFFFF',onrendered:function(canvas){$('#test').attr('href',canvas.toDataURL("image/jpeg"));$('#test').attr('download',imageName);$('#test')[0].click();}});};</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124"><link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel=icon><link href=%7balternate%20%7bRSS%20application/rss+xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://purplepalmdash.github.io/index.xml%7d rel=alternate type=application/rss+xml title="Dash &#183; Dash"><meta name=description content><meta name=keywords content="unix,virtualization,embedded,linux"></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><img src=http://purplepalmdash.github.io/images/mylogo.jpeg alt=gravatar><h1><a href=http://purplepalmdash.github.io/>很惭愧，就做了一点微小的工作</a></h1><a href=http://purplepalmdash.github.io/><p>Dash</p></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/post/>All Posts</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/technology/>Technology</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/life/>Life</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/linuxtips/>LinuxTips</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/purplepalmdash><i class="fa fa-github-square fa-3x"></i></a><a href=https://cn.linkedin.com/in/yang-feipeng-1b909319><i class="fa fa-linkedin-square fa-3x"></i></a><a href=https://plus.google.com/u/0/106572959364703833986><i class="fa fa-google-plus-square fa-3x"></i></a><a href=https://www.facebook.com/yang.feipeng><i class="fa fa-facebook-square fa-3x"></i></a><a href=https://twitter.com/dashwillfly><i class="fa fa-twitter-square fa-3x"></i></a></li></ul></div></div><div class="content container"><div class=posts><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2018/05/16/workingtipsonistiodemo/>WorkingTipsOnIstioDemo</a></h1><span class=post-date>May 16, 2018<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=istio-08-download>istio 0.8 download</h3><p>Download from its daily build:</p><p><a href=https://gcsweb.istio.io/gcs/istio-prerelease/daily-build/>https://gcsweb.istio.io/gcs/istio-prerelease/daily-build/</a></p><pre><code># wget https://storage.googleapis.com/istio-prerelease/daily-build/release-0.8-20180515-17-26/istio-release-0.8-20180515-17-26-linux.tar.gz
# mkdir -p /root/istio/bin/
# cp /root/Code/istio-release-0.8-20180515-17-26/bin/istioctl /root/istio/bin/
</code></pre><h3 id=build>Build</h3><p>Install following packages:</p><pre><code># yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel java-1.8.0-openjdk-debug
# export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-0.b14.el7_4.x86_64
# export PATH=$PATH:$JAVA_HOME/bin
</code></pre><p>Build the cars-api service via:</p><pre><code># ./mvnw -Distio.home=/root/Code/istio-release-0.8-20180515-17-26 clean package fabric8:build
# docker images | grep cars-api
kameshsampath/cars-api                                                      0.0.1               28647076e814        8 minutes ago       439 MB
</code></pre><h3 id=verification>Verification</h3><p>Install cars-api:</p><pre><code># kubectl  apply -f istio-cars-api-0.0.1-all.yml 
deployment.extensions &quot;cars-api&quot; created
# kubectl  get pods
NAME                                      READY     STATUS    RESTARTS   AGE
cars-api-777b9574bf-jvxvk                 2/2       Running   0          3m
</code></pre><p><code>car-api-ingress.yaml</code> definition:</p><pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  labels:
    expose: &quot;true&quot;
    app: cars-api
    version: 0.0.1
  name: cars-api
  namespace: default
  annotations:
    kubernetes.io/ingress.class: istio
spec:
  rules:
  - http:
      paths:
      - backend:
          serviceName: cars-api
          servicePort: 8080
</code></pre><p>Create a ingress:</p><pre><code># kubectl create -f car-api-ingress.yaml
</code></pre><p>Test the car api via:</p><pre><code># curl -vvv http://192.192.189.41:32204/cars/list
</code></pre><h3 id=auth>auth</h3><p>Definition file <code>auth.yaml</code>:</p><pre><code>apiVersion: &quot;authentication.istio.io/v1alpha1&quot;
kind: &quot;Policy&quot;
metadata:
  name: &quot;cars-api&quot;
spec:
  targets:
  - name: cars-api
  peers:
  - mtls:
  origins:
  - jwt:
      issuer: http://keycloak.default:8080/auth/realms/istio
      jwksUri: http://keycloak.default:8080/auth/realms/istio/protocol/openid-connect/certs
      audiences: 
      - cars-web  
  principalBinding: USE_ORIGIN
</code></pre><p>Create the Policy via:</p><pre><code># /root/istio/bin/istioctl create  -f auth.yaml
</code></pre><p>Now re-visit the ingress item you will see 401 issue:</p><pre><code># curl -vvv http://192.192.189.41:32204/cars/list

*   Trying 192.192.189.41...
* TCP_NODELAY set
* Connected to 192.192.189.41 (192.192.189.41) port 32204 (#0)
&gt; GET /cars/list HTTP/1.1
&gt; Host: 192.192.189.41:32204
&gt; User-Agent: curl/7.59.0
&gt; Accept: */*
&gt; 
&lt; HTTP/1.1 401 Unauthorized
&lt; content-length: 29
&lt; content-type: text/plain
&lt; date: Wed, 16 May 2018 03:16:47 GMT
&lt; server: envoy
&lt; x-envoy-upstream-service-time: 8
&lt; 
* Connection #0 to host 192.192.189.41 left intact
Origin authentication failed.%              
</code></pre><p>Get the token, then :</p><pre><code># curl -vvv -H &quot;Authorization: Bearer $token&quot; http://192.192.189.41:32204/cars/list
</code></pre><p>you will get the right result.</p><h3 id=different-namespace>Different Namespace</h3><p>Create the auth.yaml for different namespace is OK:</p><pre><code># /root/istio/bin/istioctl create -f auth-myproject.yaml  -n myproject
</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2018/05/14/nfsstoragefork8s/>NFSStorageForK8s</a></h1><span class=post-date>May 14, 2018<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=code>Code</h3><p>Code is git from following url:</p><pre><code># git clone https://github.com/kubernetes-incubator/external-storage.git
</code></pre><h3 id=steps>Steps</h3><p>You have to manually get the docker image for
<code>quay.io/external_storage/nfs-client-provisioner</code>, then create the storage
class via following command:</p><p>deployment.yaml:</p><pre><code>kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: nfs-client-provisioner
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: quay.io/external_storage/nfs-client-provisioner:latest
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: fuseim.pri/ifs
            - name: NFS_SERVER
              value: 192.192.189.31
            - name: NFS_PATH
              value: /opt/nfs
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.192.189.31
            path: /opt/nfs
</code></pre><p>Install via <code>kubectl create -f deployment.yaml</code>.</p><p>StorageClass yaml is listed as:</p><p>sc.yaml:</p><pre><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storage
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
provisioner: fuseim.pri/ifs
</code></pre><p>Create it via <code>kubectl create -f sc.yaml</code>.</p><p>test-claim.yaml:</p><pre><code>kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-claim
  annotations:
    volume.beta.kubernetes.io/storage-class: &quot;nfs-storage&quot;
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Mi
</code></pre><p>And test-pod.yaml:</p><pre><code>kind: Pod
apiVersion: v1
metadata:
  name: test-pod
spec:
  containers:
  - name: test-pod
    image: gcr.io/google_containers/busybox:1.24
    command:
      - &quot;/bin/sh&quot;
    args:
      - &quot;-c&quot;
      - &quot;touch /mnt/SUCCESS &amp;&amp; exit 0 || exit 1&quot;
    volumeMounts:
      - name: nfs-pvc
        mountPath: &quot;/mnt&quot;
  restartPolicy: &quot;Never&quot;
  volumes:
    - name: nfs-pvc
      persistentVolumeClaim:
        claimName: test-claim
</code></pre><p>Verify them via install and delete them.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2018/05/01/ratelimitingoninstio/>RateLimitingOnInstio</a></h1><span class=post-date>May 1, 2018<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=service-example>Service Example</h3><p>The yaml file is directly taken from the official example of <code>helloworld</code>, but
I remove the v2 deployment, thus the yaml file is listed as following:</p><pre><code>apiVersion: v1
kind: Service
metadata:
  name: helloworld
  labels:
    app: helloworld
spec:
  type: NodePort
  ports:
  - port: 5000
    name: http
  selector:
    app: helloworld
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: helloworld-v1
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: helloworld
        version: v1
    spec:
      containers:
      - name: helloworld
        image: istio/examples-helloworld-v1
        resources:
          requests:
            cpu: &quot;100m&quot;
        imagePullPolicy: IfNotPresent #Always
        ports:
        - containerPort: 5000
</code></pre><p>Use istioctl for injecting the sidecar, thus we could later use prometheus for
monitoring its traffic flow:</p><pre><code># kubectl create -f &lt;(istioctl kube-inject -f helloworld.yaml)
</code></pre><p>Examine the deployment/service/pods:</p><pre><code># kubectl get svc helloworld       
NAME         TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
helloworld   NodePort   10.96.242.5   &lt;none&gt;        5000:31241/TCP   27m
# kubectl get deployment helloworld-v1
NAME            DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
helloworld-v1   1         1         1            1           27m
# kubectl get pods | grep helloworld
helloworld-v1-7d57446779-dctlv    2/2       Running   0          27m
</code></pre><h3 id=make-ingress>Make ingress</h3><p>The <code>helloworld-ingress.yaml</code> is listed as following:</p><pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: helloworld
  annotations:
    kubernetes.io/ingress.class: &quot;istio&quot;
spec:
  rules:
  - http:
      paths:
      - path: /hello
        backend:
          serviceName: helloworld
          servicePort: 5000
</code></pre><p>Create the ingress and verify it:</p><pre><code># kubectl create -f helloworld-ingress.yaml
# kubectl get ingress helloworld
NAME         HOSTS     ADDRESS   PORTS     AGE
helloworld   *                   80        1h
# curl http://192.168.99.100:30039/hello
Hello version: v1, instance: helloworld-v1-7d57446779-dctlv
</code></pre><h3 id=rate-limiting>Rate Limiting</h3><p>Write following rete limiting yaml for defining its traffic:</p><pre><code>apiVersion: &quot;config.istio.io/v1alpha2&quot;
kind: memquota
metadata:
  name: helloworldservicehandler
  namespace: istio-system
spec:
  quotas:
  - name: helloworldservicerequestcount.quota.istio-system
    maxAmount: 5000
    validDuration: 1s
    # The first matching override is applied.
    # A requestcount instance is checked against override dimensions.
    overrides:
    # The following override applies to 'helloworld' regardless
    # of the source.
    - dimensions:
        destination: helloworld
      maxAmount: 2
      validDuration: 1s

---
apiVersion: &quot;config.istio.io/v1alpha2&quot;
kind: quota
metadata:
  name: helloworldservicerequestcount
  namespace: istio-system
spec:
  dimensions:
    source: source.labels[&quot;app&quot;] | source.service | &quot;unknown&quot;
    sourceVersion: source.labels[&quot;version&quot;] | &quot;unknown&quot;
    destination: destination.labels[&quot;app&quot;] | destination.service | &quot;unknown&quot;
    destinationVersion: destination.labels[&quot;version&quot;] | &quot;unknown&quot;

---
apiVersion: &quot;config.istio.io/v1alpha2&quot;
kind: rule
metadata:
  name: helloworldservicequota
  namespace: istio-system
spec:
  actions:
  - handler: helloworldservicehandler.memquota
    instances:
    - helloworldservicerequestcount.quota
---
apiVersion: config.istio.io/v1alpha2
kind: QuotaSpec
metadata:
  creationTimestamp: null
  name: helloworldservicerequest-count
  namespace: istio-system
spec:
  rules:
  - quotas:
    - charge: 1
      quota: RequestCount
---
apiVersion: config.istio.io/v1alpha2
kind: QuotaSpecBinding
metadata:
  creationTimestamp: null
  name: helloworldservicerequest-count
  namespace: istio-system
spec:
  quotaSpecs:
  - name: helloworldservicerequest-count
    namespace: istio-system
  services:
  - name: helloworld
    namespace: default
</code></pre><p>The above items define a 2 qps rate limiting.</p><h3 id=monitoring>Monitoring</h3><p>Use prometheus for monitoring the traffic, enable prometheus via:</p><pre><code># kubectl create -f ~/Code/istio-0.7.1/install/kubernetes/addons/prometheus.yaml
</code></pre><p>You could configure the prometheus&rsquo;s service type to NodePort, thus you could
directly access it.</p><p><img src=/images/2018_05_01_22_21_56_779x473.jpg alt=/images/2018_05_01_22_21_56_779x473.jpg></p><p>Make the traffic:</p><pre><code># while true; do curl -s -o /dev/null http://192.168.99.100:30039/hello;done
</code></pre><p>Then view the prometheus via following:</p><pre><code># increase(istio_request_count{destination_service=&quot;helloworld.default.svc.cluster.local&quot;, response_code=&quot;429&quot;}[5m])
</code></pre><p>Initial:</p><p><img src=/images/2018_05_01_22_24_18_764x343.jpg alt=/images/2018_05_01_22_24_18_764x343.jpg></p><p>After aboult 3 minutes:</p><p><img src=/images/2018_05_01_22_24_50_784x327.jpg alt=/images/2018_05_01_22_24_50_784x327.jpg></p><p>You could change the response code from <code>429</code> to <code>200</code>, this means you get the
succeed rate.</p><h3 id=fetch-back-the-result>Fetch back the result</h3><p>Refers to:</p><p><a href=https://www.robustperception.io/prometheus-query-results-as-csv/>https://www.robustperception.io/prometheus-query-results-as-csv/</a></p><pre><code># wget https://raw.githubusercontent.com/RobustPerception/python_examples/master/csv/query_csv.py
</code></pre><p>For querying the 429/200:</p><pre><code>#  python query_csv.py http://127.0.0.1:9090 'increase(istio_request_count{destination_service=&quot;helloworld.default.svc.cluster.local&quot;, response_code=&quot;429&quot;}[5m])'
name,timestamp,value,connection_mtls,destination_service,destination_version,instance,job,response_code,source_service,source_version
,1525185609.906,8145.762711864407,false,helloworld.default.svc.cluster.local,v1,172.17.0.10:42422,istio-mesh,429,istio-ingress.istio-system.svc.cluster.local,unknown
#  python query_csv.py http://127.0.0.1:9090 'increase(istio_request_count{destination_service=&quot;helloworld.default.svc.cluster.local&quot;, response_code=&quot;200&quot;}[5m])'
name,timestamp,value,connection_mtls,destination_service,destination_version,instance,job,response_code,source_service,source_version
,1525185628.005,886.7796610169491,false,helloworld.default.svc.cluster.local,v1,172.17.0.10:42422,istio-mesh,200,istio-ingress.istio-system.svc.cluster.local,unknown
</code></pre><p><code>8145</code> and <code>886</code> are the values for the query, we could use them for 2nd
development.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2018/04/29/workingtipsonistiodev/>WorkingTipsOnIstioDev</a></h1><span class=post-date>Apr 29, 2018<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=sample-svc>Sample SVC</h3><p>Create a sample svc using minikube:</p><pre><code># sudo docker save jrelva/nginx-autoindex&gt;autoindex.tar
# eval $(minikube docker-env)
# docker load&lt;autoindex.tar
# kubectl run --image=jrelva/nginx-autoindex:latest nginx-autoindex --port=80 --image-pull-policy=IfNotPresent
deployment &quot;nginx-autoindex&quot; created
# kubectl get deployment
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-autoindex    1         1         1            1           6s
# kubectl expose deployment nginx-autoindex --name nginx-autoindex-svc
# kubectl get svc | grep nginx
nginx-autoindex-svc   ClusterIP   10.107.181.75    &lt;none&gt;        80/TCP           29s
</code></pre><p>Istio Configuration:</p><pre><code># kubectl get svc --all-namespaces | grep istio-ingress
istio-system   istio-ingress          LoadBalancer   10.100.152.241   &lt;pending&gt;     80:30336/TCP,443:32004/TCP  
</code></pre><h3 id=istio-ingress>Istio Ingress</h3></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2018/04/18/kismatic110tips/>kismatic110tips</a></h1><span class=post-date>Apr 18, 2018<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=preparation>preparation</h3><p>Deployment machine, Download the packages:</p><pre><code># mkdir deploy
# cd deploy
# wget https://github.com/apprenda/kismatic/releases/download/v1.10.0/kismatic-v1.10.0-linux-amd64.tar.gz
# git clone https://github.com/apprenda/kismatic.git
# tar xzvf *.tar.gz;
# ls 
ansible  helm  kismatic  kismatic-master  kismatic-master.zip  kismatic-v1.10.0-linux-amd64.tar.gz  kubectl  provision
</code></pre><p>Target node(all-in-one), install python-pip, shadowsocks, redsocks, gcc, etc, for acrossing the fucking GFW!</p><h3 id=plan>plan</h3><p>plan the cluster</p><pre><code>./kismatic install plan
Plan your Kubernetes cluster:
=&gt; Number of etcd nodes [3]: 1
=&gt; Number of master nodes [2]: 1
=&gt; Number of worker nodes [3]: 1
=&gt; Number of ingress nodes (optional, set to 0 if not required) [2]: 0
=&gt; Number of storage nodes (optional, set to 0 if not required) [0]: 0
=&gt; Number of existing files or directories to be copied [0]: 0

Generating installation plan file template with: 
- 1 etcd nodes
- 1 master nodes
- 1 worker nodes
- 0 ingress nodes
- 0 storage nodes
- 0 files

Wrote plan file template to &quot;kismatic-cluster.yaml&quot;
Edit the plan file to further describe your cluster. Once ready, execute the &quot;install validate&quot; command to proceed.
</code></pre><p>An empty <code>kismatic-cluster.yaml</code> will be generated, later we will edit it.</p><h3 id=validate>validate</h3><p>validate with detailed information:</p><pre><code>./kismatic install validate -o raw
</code></pre><p>Error:</p><pre><code>ansible/bin/ansible-playbook -i ansible/inventory.ini -s ansible/playbooks/preflight.yaml --extra-vars @ansible/clustercatalog.yaml -vvvv
Traceback (most recent call last):
  File &quot;ansible/bin/ansible-playbook&quot;, line 36, in &lt;module&gt;
    import shutil
  File &quot;/usr/lib/python3.6/shutil.py&quot;, line 10, in &lt;module&gt;
    import fnmatch
  File &quot;/usr/lib/python3.6/fnmatch.py&quot;, line 14, in &lt;module&gt;
    import re
  File &quot;/usr/lib/python3.6/re.py&quot;, line 142, in &lt;module&gt;
    class RegexFlag(enum.IntFlag):
AttributeError: module 'enum' has no attribute 'IntFlag'
error running playbook: error running ansible: exit status 1
</code></pre><p>Seems because the python is python3 rather than python2.</p><p>Edit the python definition:</p><pre><code># vim ansible/bin/ansible-playbook
    #!/usr/bin/python2
</code></pre><p>Then your validation will be OK.</p><h3 id=install-apply>install apply</h3><p>Via following command:</p><pre><code># ./kismatic install apply
</code></pre></div><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/83/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/83/>83</a></li><li class="page-item active"><a class=page-link href=/page/84/>84</a></li><li class=page-item><a class=page-link href=/page/85/>85</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/246/>246</a></li><li class=page-item><a href=/page/85/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/246/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div><script src=http://purplepalmdash.github.io/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>