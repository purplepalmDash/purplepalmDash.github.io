<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta name=generator content="Hugo 0.64.0"><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Dash &#183; Dash</title><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124"><link rel=stylesheet href=https://purplepalmdash.github.io/css/highlight/googlecode.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js></script><script type=text/javascript src=/js/html2canvas.js></script><script type=text/javascript>function genPostShot(){var rightNow=new Date();var imageName=rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");imageName+='.jpg'
html2canvas(document.getElementsByClassName('post'),{background:'#FFFFFF',onrendered:function(canvas){$('#test').attr('href',canvas.toDataURL("image/jpeg"));$('#test').attr('download',imageName);$('#test')[0].click();}});};</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124"><link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel=icon><link href=%7balternate%20%7bRSS%20application/rss+xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://purplepalmdash.github.io/index.xml%7d rel=alternate type=application/rss+xml title="Dash &#183; Dash"><meta name=description content><meta name=keywords content="unix,virtualization,embedded,linux"></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><img src=http://purplepalmdash.github.io/images/mylogo.jpeg alt=gravatar><h1><a href=http://purplepalmdash.github.io/>很惭愧，就做了一点微小的工作</a></h1><a href=http://purplepalmdash.github.io/><p>Dash</p></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/post/>All Posts</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/technology/>Technology</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/life/>Life</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/linuxtips/>LinuxTips</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/purplepalmdash><i class="fa fa-github-square fa-3x"></i></a><a href=https://cn.linkedin.com/in/yang-feipeng-1b909319><i class="fa fa-linkedin-square fa-3x"></i></a><a href=https://plus.google.com/u/0/106572959364703833986><i class="fa fa-google-plus-square fa-3x"></i></a><a href=https://www.facebook.com/yang.feipeng><i class="fa fa-facebook-square fa-3x"></i></a><a href=https://twitter.com/dashwillfly><i class="fa fa-twitter-square fa-3x"></i></a></li></ul></div></div><div class="content container"><div class=posts><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2021/05/08/workingtipsondnscryption/>WorkingTipsOndnscryption</a></h1><span class=post-date>May 8, 2021<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=working-environment>Working Environment</h3><p>Centos 7.9, vm , 8core, 16G.</p><h3 id=installation>Installation</h3><p>Install dnsmasq:</p><pre><code># sudo yum install -y dnsmasq
</code></pre><p>Install dnscrypt-proxy:</p><pre><code># sudo yum install -y dnscrypt-proxy
</code></pre><p>Wget the chinadns configuration file:</p><pre><code># wget https://raw.githubusercontent.com/felixonmars/dnsmasq-china-list/master/accelerated-domains.china.conf
# mv accelerated-domains.china.conf /etc/dnsmasq.d/accelerated-domains.china.conf
</code></pre><p>you can replace the <code>114.114.114.114</code> via your own dns(china intranet dns).</p><h3 id=configuration>Configuration</h3><p>Configure dnsmasq:</p><pre><code># vim /etc/dnsmasq.conf
listen-address=127.0.0.1
no-resolv
conf-dir=/usr/local/etc/dnsmasq.d
server=127.0.0.1#5300
interface=lo
bind-interfaces
</code></pre><p>Configure dnscrypt-proxy:</p><pre><code># vim /etc/dnscrypt-proxy/dnscrypt-proxy.toml
     # 监听5300端口
     listen_addresses = ['127.0.0.1:5300', '[::1]:5300']
     # 使用下面3个公开的DNS服务
     server_names = ['google', 'cloudflare', 'cloudflare-ipv6']
     # 如果找不到合适的公开DNS服务，则使用下面的DNS服务
     fallback_resolvers = ['9.9.9.9:53', '8.8.8.8:53']
     # 担心这些DNS请求被墙，设置使用代理发送DNS请求
     force_tcp = true
     proxy = 'socks5://127.0.0.1:1086'
</code></pre><p>Configure <code>/etc/resolv.conf</code> for using <code>127.0.0.1</code>:</p><pre><code>nameserver 127.0.0.1
</code></pre><h3 id=privoxy>privoxy</h3><p>In centos 7.9. don&rsquo;t install this package from epel, download the source code from internet and compile it:</p><pre><code>$ privoxy  --version
Privoxy version 3.0.28 (https://www.privoxy.org/)
</code></pre><p>make sure you have specify the gfwlist.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2021/05/02/%E5%AE%8C%E5%85%A8%E7%94%A8ram%E8%BF%90%E8%A1%8Cubuntu2/>完全用RAM运行Ubuntu(2)</a></h1><span class=post-date>May 2, 2021<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>完全用ram工作的场景下，关机时需要回写到磁盘上，以下是用来将Ram中的数据回写到磁盘的方法。</p><pre><code># vim /bin/writeback.sh
    #!/bin/sh
    kkk=`mount | grep &quot;none on / type tmpfs&quot;`
    if [ ! -z &quot;$kkk&quot; ]
    then
    mkdir -p /writeback
    mount /dev/mapper/ubuntu--vg-root /writeback
    rsync -a --delete --exclude 'tmp'   --exclude 'proc' --exclude 'writeback' --exclude 'sys' / /writeback/
    fi
</code></pre><p>创建一个回写的服务：</p><pre><code># vim  /etc/systemd/system/run-before-shutdown.service 
[Unit]
Description=Run my custom task at shutdown
DefaultDependencies=no
Before=shutdown.target reboot.target halt.target

[Service]
Type=oneshot
ExecStart=/bin/writeback.sh
TimeoutStartSec=0

[Install]
WantedBy=shutdown.target
</code></pre><p>使能服务：</p><pre><code># systemctl enable run-before-shutdown
</code></pre><p>则关机时系统会调用回写脚本将Ram中的数据写入到磁盘中。</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2021/04/30/workingtipsonlxd20210430/>WorkingTipsOnLXD20210430</a></h1><span class=post-date>Apr 30, 2021<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=环境说明>环境说明</h3><p>新建3台虚拟机:</p><pre><code>192.168.100.13/14/15, 4核8G， 虚拟机环境
</code></pre><p>该虚拟机所在的网段为<code>192.168.100.0/24</code>, 其中dhcp范围为<code>192.168.100.128~192.168.100.254</code>, 网关为<code>192.168.100.1</code></p><h3 id=os环境初始化配置>OS环境初始化配置</h3><p>我们期待lxc实例能通过网桥获取到与宿主机(192.168.100.13/14/15)同样的IP地址范围，所以先配置各节点上的网桥br0:</p><p>删除手动连接后，NetworkManager会自动拉起另一个：</p><p><img src=./images/2021_04_30_23_15_54_819x240.jpg alt=./images/2021_04_30_23_15_54_819x240.jpg></p><p>再次删除此自动建立的连接，直到只看到lxdbr0即可：</p><p><img src=./images/2021_04_30_23_17_44_820x162.jpg alt=./images/2021_04_30_23_17_44_820x162.jpg></p><p>建立br0, 并指定eth0为br0的slave设备：</p><p><img src=/images/2021_04_30_23_21_27_1093x243.jpg alt=/images/2021_04_30_23_21_27_1093x243.jpg></p><p>依次类推完成另外两台机器的配置。</p><p>相关的配置脚本(这里以<code>192.168.100.14</code>为例说明)如下，实际环境中需根据具体的配置信息进行更改：</p><pre><code>nmcli con show | grep eth0 | awk {'print $2'} | xargs -I % nmcli con delete uuid %
nmcli con show | grep eth0 | awk {'print $4'} | xargs -I % nmcli con delete uuid %
nmcli con show
nmcli conn add type bridge ifname br0 ipv4.method manual ipv4.address &quot;192.168.100.14&quot; ipv4.gateway &quot;192.168.100.1&quot; ipv4.dns &quot;223.5.5.5&quot;
nmcli conn add type bridge-slave ifname eth0 master br0
</code></pre><h3 id=lxc使用br0网络>lxc使用br0网络</h3><p>lxc可以通过使用不同的profile定义出实例所在的网络，我们通过以下操作新建出一个可以通过网桥<code>br0</code>获取到<code>192.168.100.0/24</code>段地址的profile:</p><pre><code>[root@node13 ~]# lxc profile list
+---------+---------------------+---------+
|  NAME   |     DESCRIPTION     | USED BY |
+---------+---------------------+---------+
| default | Default LXD profile | 0       |
+---------+---------------------+---------+
[root@node13 ~]# lxc profile show default&gt;br0
[root@node13 ~]# vim br0
config: {}
description: Default LXD profile modified for using br0
devices:
  eth0:
    name: eth0
    nictype: bridged
    parent: br0
    type: nic
  root:
    path: /
    pool: default
    type: disk
name: br0
used_by: []
[root@node13 ~]# lxc profile create br0
Profile br0 created
[root@node13 ~]# lxc profile edit br0&lt;br0
[root@node13 ~]# lxc profile list
+---------+--------------------------------------------+---------+
|  NAME   |                DESCRIPTION                 | USED BY |
+---------+--------------------------------------------+---------+
| br0     | Default LXD profile modified for using br0 | 0       |
+---------+--------------------------------------------+---------+
| default | Default LXD profile                        | 0       |
+---------+--------------------------------------------+---------+

</code></pre><p>现在可使用创建出的<code>br0</code>实例化一个容器，</p><pre><code># lxc launch centos7 node1 --profile br0
# lxc ls
+-------+---------+------------------------+----------------------------------------------+-----------+-----------+
| NAME  |  STATE  |          IPV4          |                     IPV6                     |   TYPE    | SNAPSHOTS |
+-------+---------+------------------------+----------------------------------------------+-----------+-----------+
| node1 | RUNNING | 192.168.100.130 (eth0) |                                              | CONTAINER | 0         |
+-------+---------+------------------------+----------------------------------------------+-----------+-----------+

</code></pre><p>固定IP的方法：</p><pre><code>[root@node13 ~]# lxc exec node1 bash
[root@node1 ~]# vim /etc/sysconfig/network-scripts/ifcfg-eth0 
DEVICE=eth0
BOOTPROTO=static
IPADDR=192.168.100.20
NETMASK=255.255.255.0
GATEWAY=192.168.100.1
ONBOOT=yes
HOSTNAME=node1
NM_CONTROLLED=no
TYPE=Ethernet
MTU=
DHCP_HOSTNAME=node1
[root@node1 ~]#  reboot
</code></pre><p>重启之后可以看到lxc确实使用了我们设置的<code>192.168.100.20</code>IP地址。</p><pre><code>[root@node13 ~]# lxc ls
+-------+---------+------------------------+----------------------------------------------+-----------+-----------+
| NAME  |  STATE  |          IPV4          |                     IPV6                     |   TYPE    | SNAPSHOTS |
+-------+---------+------------------------+----------------------------------------------+-----------+-----------+
| node1 | RUNNING | 192.168.100.20 (eth0)  |                                              | CONTAINER | 0         |

</code></pre><p>最后验证与外部网络的互通性:</p><pre><code>[root@node13 ~]# lxc exec node1 bash
[root@node1 ~]# ping 192.168.100.10
PING 192.168.100.10 (192.168.100.10) 56(84) bytes of data.
64 bytes from 192.168.100.10: icmp_seq=1 ttl=64 time=0.742 ms
64 bytes from 192.168.100.10: icmp_seq=2 ttl=64 time=0.287 ms
[root@node1 ~]# ping 10.50.208.145
PING 10.50.208.145 (10.50.208.145) 56(84) bytes of data.
64 bytes from 10.50.208.145: icmp_seq=1 ttl=63 time=0.410 ms
64 bytes from 10.50.208.145: icmp_seq=2 ttl=63 time=0.214 ms
^C
--- 10.50.208.145 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1000ms
rtt min/avg/max/mdev = 0.214/0.312/0.410/0.098 ms
[root@node1 ~]# ping 10.50.208.147
PING 10.50.208.147 (10.50.208.147) 56(84) bytes of data.
64 bytes from 10.50.208.147: icmp_seq=1 ttl=64 time=0.146 ms
64 bytes from 10.50.208.147: icmp_seq=2 ttl=64 time=0.153 ms
^C
--- 10.50.208.147 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1000ms

</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2021/04/29/bestpracticeofcentoslxd/>BestPracticeOfCentOSLXD</a></h1><span class=post-date>Apr 29, 2021<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=1-文档目的>1. 文档目的</h3><p>文档旨在针对在CentOS 7操作系统上安装、配置及运行LXD提供最佳实践。</p><h3 id=2-环境准备>2. 环境准备</h3><p>基于快速验证的目的，本文档基于虚拟机搭建，验证机配置如下:</p><ul><li>操作系统： <code>CentOS Linux release 7.6.1810 (Core) </code>, 最小化安装</li><li>硬件配置: 36核心，246GB内存， 500G磁盘分区</li><li>软件配置:</li><li>网络配置: 192.168.100.10/24, 网关192.168.100.1</li></ul><p>访问方式(这里提供如何从办公网络直达验证机)</p><h3 id=3-环境搭建>3. 环境搭建</h3><p>离线情况下，配置内网源后，执行以下命令安装:</p><pre><code># yum install -y snapd net-tools vim
# systemctl enable --now snapd.socket
</code></pre><p>解压离线安装文件:</p><pre><code># tar xzvf lxcimages.tar.gz ; tar xzvf snap.tar.gz
</code></pre><p>进入到snap目录下安装snap:</p><pre><code># snap ack core_10958.assert ; snap ack core18_1997.assert; snap ack lxd_20211.assert
# snap install core_10958.snap; snap install core18_1997.snap; snap install lxd_20211.snap
</code></pre><p>更改内核参数后，重启机器:</p><pre><code>$ grubby --args=&quot;user_namespace.enable=1&quot; --update-kernel=&quot;$(grubby --default-kernel)&quot;
$ grubby --args=&quot;namespace.unpriv_enable=1&quot; --update-kernel=&quot;$(grubby --default-kernel)&quot;
$ sudo sh -c 'echo &quot;user.max_user_namespaces=3883&quot; &gt; /etc/sysctl.d/99-userns.conf'
# reboot
</code></pre><p>创建snap目录并添加运行权限:</p><pre><code># ln -s /var/lib/snapd/snap /snap
# usermod -a -G lxd roo
# newgrp lxd
# id
uid=0(root) gid=994(lxd) groups=994(lxd),0(root)
</code></pre><p>此时需要退出终端后重新登录终端，方可使用<code>lxc</code>相关命令.</p><p>初始化<code>lxd</code>环境:</p><pre><code>[root@lxdpaas ~]# lxd init
Would you like to use LXD clustering? (yes/no) [default=no]: 
Do you want to configure a new storage pool? (yes/no) [default=yes]: 
Name of the new storage pool [default=default]: 
Name of the storage backend to use (btrfs, dir, lvm, ceph) [default=btrfs]: dir
Would you like to connect to a MAAS server? (yes/no) [default=no]: 
Would you like to create a new local network bridge? (yes/no) [default=yes]: 
What should the new bridge be called? [default=lxdbr0]: 
What IPv4 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]: 
What IPv6 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]: 
Would you like the LXD server to be available over the network? (yes/no) [default=no]: 
Would you like stale cached images to be updated automatically? (yes/no) [default=yes] 
Would you like a YAML &quot;lxd init&quot; preseed to be printed? (yes/no) [default=no]: 
</code></pre><p>此时应无任何镜像，接下来手动导入镜像:</p><pre><code># cd lxcimages
# lxc image  import meta-50030de846c046680faf34f7dc3e60284e31f5aab38dfd19c94a2fd1bf895d0c.tar.xz 50030de846c046680faf34f7dc3e60284e31f5aab38dfd19c94a2fd1bf895d0c.squashfs --alias centos7
Image imported with fingerprint: 50030de846c046680faf34f7dc3e60284e31f5aab38dfd19c94a2fd1bf895d0c
# lxc image list
+---------+--------------+--------+----------------------------------+--------------+-----------+---------+------------------------------+
|  ALIAS  | FINGERPRINT  | PUBLIC |           DESCRIPTION            | ARCHITECTURE |   TYPE    |  SIZE   |         UPLOAD DATE          |
+---------+--------------+--------+----------------------------------+--------------+-----------+---------+------------------------------+
| centos7 | 50030de846c0 | no     | Centos 7 x86_64 (20210428_07:08) | x86_64       | CONTAINER | 83.46MB | Apr 29, 2021 at 4:53am (UTC) |
+---------+--------------+--------+----------------------------------+--------------+-----------+---------+------------------------------+

</code></pre><h3 id=4-lxc操作实练>4. lxc操作实练</h3><p>启动一个lxc 实例:</p><pre><code># lxc launch centos7 db1
Creating db1
Starting db1              
</code></pre><p>进入运行中的实例:</p><pre><code># lxc exec db1 bash
[root@db1 ~]# cat /etc/redhat-release 
CentOS Linux release 7.9.2009 (Core)
</code></pre><p>启动第二个名为<code>db2</code>的实例：</p><pre><code>[root@lxdpaas lxcimages]# lxc launch centos7 db2
Creating db2
Starting db2                              
[root@lxdpaas lxcimages]# lxc exec db2 bash
[root@db2 ~]#
</code></pre><p>查看运行中的容器实例:</p><pre><code># lxc ls
+------+---------+-----------------------+----------------------------------------------+-----------+-----------+
| NAME |  STATE  |         IPV4          |                     IPV6                     |   TYPE    | SNAPSHOTS |
+------+---------+-----------------------+----------------------------------------------+-----------+-----------+
| db1  | RUNNING | 10.159.107.72 (eth0)  | fd42:45a:636c:6e69:216:3eff:fe81:347e (eth0) | CONTAINER | 0         |
+------+---------+-----------------------+----------------------------------------------+-----------+-----------+
| db2  | RUNNING | 10.159.107.125 (eth0) | fd42:45a:636c:6e69:216:3eff:fe53:754 (eth0)  | CONTAINER | 0         |
+------+---------+-----------------------+----------------------------------------------+-----------+-----------+
</code></pre><p>停止/删除运行中的容器:</p><pre><code>[root@lxdpaas lxcimages]# lxc stop db1
[root@lxdpaas lxcimages]# lxc stop db2
[root@lxdpaas lxcimages]# lxc delete db1
[root@lxdpaas lxcimages]# lxc delete db2
[root@lxdpaas lxcimages]# lxc ls
+------+-------+------+------+------+-----------+
| NAME | STATE | IPV4 | IPV6 | TYPE | SNAPSHOTS |
+------+-------+------+------+------+-----------+
</code></pre><p>定制化:</p><pre><code># lxc launch c75dhclient k2
# lxc exec k2 /bin/bash
dhclient eth0
vi /etc/yum.repos.d/kkk.repo
yum makecache
yum install -y vim net-tools
exit
# lxc ls | grep k2
| k2   | RUNNING | 10.159.107.248 (eth0) | fd42:45a:636c:6e69:216:3eff:fea0:2c33 (eth0) | CONTAINER | 0         |
</code></pre><p>导出当前镜像：</p><pre><code>[root@lxdpaas ~]# mkdir export
[root@lxdpaas ~]# cd export/
[root@lxdpaas export]# lxc stop k2
[root@lxdpaas export]# lxc publish k2 --alias centos75withvim
Instance published with fingerprint: 7301c7d85d4d56ebcae117aa79cf88868c4821dedb22e641fe66d05cab6599f2
[root@lxdpaas export]# lxc image list
+-----------------+--------------+--------+----------------------------------+--------------+-----------+----------+------------------------------+
|      ALIAS      | FINGERPRINT  | PUBLIC |           DESCRIPTION            | ARCHITECTURE |   TYPE    |   SIZE   |         UPLOAD DATE          |
+-----------------+--------------+--------+----------------------------------+--------------+-----------+----------+------------------------------+
| c75dhclient     | 3a063c11b987 | no     |                                  | x86_64       | CONTAINER | 381.84MB | Apr 29, 2021 at 8:06am (UTC) |
+-----------------+--------------+--------+----------------------------------+--------------+-----------+----------+------------------------------+
| centos7         | 50030de846c0 | no     | Centos 7 x86_64 (20210428_07:08) | x86_64       | CONTAINER | 83.46MB  | Apr 29, 2021 at 4:53am (UTC) |
+-----------------+--------------+--------+----------------------------------+--------------+-----------+----------+------------------------------+
| centos75withvim | 7301c7d85d4d | no     |                                  | x86_64       | CONTAINER | 420.72MB | Apr 29, 2021 at 8:23am (UTC) |
+-----------------+--------------+--------+----------------------------------+--------------+-----------+----------+------------------------------+
[root@lxdpaas export]# lxc image export centos75withvim .
Image exported successfully!           
[root@lxdpaas export]# ls
7301c7d85d4d56ebcae117aa79cf88868c4821dedb22e641fe66d05cab6599f2.tar.gz
</code></pre><p>测试:</p><pre><code># lxc launch centos75withvim test1
Creating test1
Starting test1                             
[root@lxdpaas export]# lxc exec test1 /bin/bash
[root@base ~]# dhclient eth0
[root@base ~]# which vim
/usr/bin/vim
[root@base ~]# which ifconfig
/usr/sbin/ifconfig

</code></pre><p>有关数据库的更改:</p><pre><code> yum install -y mariadb-server
 systemctl enable mariadb
</code></pre><h3 id=5-资源隔离>5. 资源隔离</h3><p>制作benchmark容器:</p><pre><code>$ lxc launch centos7 bench -c security.privileged=true

	# yum install -y epel-release; yum install -y stress
	# yum install which
	# which stress
        # shutdown -h now
$ lxc publish bench --alias bench
$ lxc launch bench k1
$ lxc exec k1 /bin/bash
    stress --cpu 5
</code></pre><p>此时可以看到，宿主机上的5个cpu跑满：</p><p><img src=/images/2021_04_29_17_55_21_884x154.jpg alt=/images/2021_04_29_17_55_21_884x154.jpg></p><p>设置CPU限制:</p><pre><code># lxc config set  bench limits.cpu 2
</code></pre><p>即便容器中的进程未变，但是主机上可以看到，只有两个CPU跑满:</p><p><img src=/images/2021_04_29_17_56_10_898x161.jpg alt=/images/2021_04_29_17_56_10_898x161.jpg></p><p>对内存的使用规则是同样的。</p><h3 id=z-定制化>z. 定制化</h3><p>为了适配用户习惯，做了以下修改:</p><pre><code># yum install -y mate-desktop xrdp mate* gnome-terminal firefox wqy* evince
# echo mate-session&gt;/root/.Xclients
# chmod 777 /root/.Xclients
# systemctl start xrdp
# systemctl enable xrdp
</code></pre><p>外部需要做iptables转发:</p><pre><code>$ sudo iptables -D FORWARD -o virbr0 -j REJECT --reject-with icmp-port-unreachable
$ sudo iptables -D FORWARD -i virbr0 -j REJECT --reject-with icmp-port-unreachable
$ sudo iptables -t nat -A PREROUTING -p tcp --dport 13389 -j DNAT --to-destination  192.168.100.10:3389
$ sudo iptables -t nat -A POSTROUTING -p tcp -d 192.168.100.10 --dport 3389 -j SNAT --to-source 10.50.208.147
</code></pre><p>外部的centos7机器上，因为升级了内核的关系，需要用如下命令开始运行:</p><pre><code>lxc launch images:centos/7 blah -c security.privileged=true
</code></pre><p>当前制作的centos7.5容器似乎不能满足lxc的功能?</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2021/04/27/%E5%AE%8C%E5%85%A8%E7%94%A8ram%E8%BF%90%E8%A1%8Cubuntu/>完全用RAM运行Ubuntu</a></h1><span class=post-date>Apr 27, 2021<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=1-目的>1. 目的</h3><p>将Ubuntu18.04.1操作系统(arm64)完全运行在内存中。</p><h3 id=2-准备材料>2. 准备材料</h3><p>Ubuntu 18.04.1 arm64安装iso.<br>arm64服务器/libvirtd/virt-manager.(在没有实体服务器的情况下，可以用虚拟机来模拟测试).</p><h3 id=3-步骤>3. 步骤</h3><p>最小化安装Ubuntu 18.04.1 操作系统, 根分区最好包含所有分区(all in one)。<br>安装完毕操作系统后，定制自己需要的软件包及准备环境后，删除所有的临时文件，尽量瘦身系统。这是因为内存定制化后，所有的文件在启动时将被加载到内存！全新安装的ubuntu大约占据约1.5GB的磁盘空间。<br>以下为定制为RAM启动的流程：</p><p>步骤一：<br>更改<code>/etc/fstab</code>文件内容，首先备份该文件:</p><pre><code># cp /etc/fstab /etc/fstab.bak
</code></pre><p>编辑<code>/etc/fstab</code>文件内容，找到标识根分区(/)的行，更改为以下内容(下为示例):</p><pre><code>#/dev/mapper/ubuntu--vg-root /               ext4    errors=remount-ro 0       1
none / tmpfs defaults 0 0
</code></pre><p>步骤二:<br>更改initramfs中的<code>local</code>脚本内容, initramfs 包含的工具和脚本，在正式的根文件系统的初始化脚本 init 启动之前，就被挂载并完成相应的初始化工作。我们需要提前将磁盘根分区中的内容拷贝入<code>tmpfs</code>中，以便在<code>/etc/fstab</code>开始执行的时候找寻到正确的分区.</p><p>首先备份<code>/usr/share/initramfs-tools/scripts/local</code>文件:</p><pre><code># cp /usr/share/initramfs-tools/scripts/local /usr/share/initramfs-tools/scripts/local.bak   
</code></pre><p>编辑<code>local</code>文件，更改其<code>Mount root</code>部分的处理逻辑(约204行左右内容):</p><pre><code>	# FIXME This has no error checking
	# Mount root
	#mount ${roflag} ${FSTYPE:+-t ${FSTYPE} }${ROOTFLAGS} ${ROOT} ${rootmnt}
	# Start of ramboottmp
        mkdir /ramboottmp
        mount ${roflag} -t ${FSTYPE} ${ROOTFLAGS} ${ROOT} /ramboottmp
        mount -t tmpfs -o size=100% none ${rootmnt}
        cd ${rootmnt}
        cp -rfa /ramboottmp/* ${rootmnt}
        umount /ramboottmp
        ### End of ramboottmp
</code></pre><p>保存该文件后，重新编译initramfs:</p><pre><code># mkinitramfs -o /boot/initrd.img-ramboot
</code></pre><p>编译成功后，将<code>local</code>文件替换会原来的版本:</p><pre><code># cp -f /usr/share/initramfs-tools/scripts/local.bak /usr/share/initramfs-tools/scripts/local
</code></pre><p>步骤三:<br>更改grub，以使用刚才编译出的<code>initrd.img-ramboot</code>来启动操作系统:</p><p>更改第一启动项中的<code>/initrd</code>行，替换为:</p><pre><code># chmod +w /boot/grub/grub.cfg
# vim /boot/grub/grub.cfg
.....
.....
        linux	/boot/vmlinuz-4.15.0-29-generic root=/dev/mapper/ubuntu--vg-root ro  
	initrd	/boot/initrd.img-ramboot
......
......
# chmod -w /boot/grub/grub.cfg
</code></pre><p>步骤四：<br>重启，重启时选择第一启动项，此时根分区会整体被加载到<code>tmpfs</code>中。</p><h3 id=4-性能对比测试>4. 性能对比测试</h3><p>测试环境定义:</p><ul><li>aarch64 4核</li><li>64 GB 内存</li><li>100 GB 磁盘分区</li><li>Ubuntu 18.04.1 LTS</li><li>内核版本： 4.15.0-29-generic</li><li>fio版本: fio-3.1</li></ul><p>所有测试样例均在<code>ramdisk</code>主机及传统主机上运行并对比.</p><h4 id=41-fio-4k随机读写>4.1 fio 4k随机读写</h4><p>测试命令如下:</p><pre><code># fio --name TEST --eta-newline=5s --filename=fio-tempfile.dat --rw=randrw --size=500m --io_size=10g --blocksize=4k --ioengine=libaio --fsync=1 --iodepth=1 --numjobs=1 --runtime=60 --group_reporting
</code></pre><table><thead><tr><th>指标</th><th>内存型主机</th><th>传统主机</th></tr></thead><tbody><tr><td>READ bw</td><td>bw=513MiB/s (538MB/s)</td><td>bw=85.0KiB/s (87.0kB/s)</td></tr><tr><td>READ io</td><td>io=5133MiB (5382MB)</td><td>io=5104KiB (5226kB)</td></tr><tr><td>READ iops</td><td>IOPS=131k</td><td>IOPS=21</td></tr><tr><td>WRITE bw</td><td>bw=510MiB/s (535MB/s)</td><td>bw=88.1KiB/s (90.2kB/s)</td></tr><tr><td>WRITE io</td><td>io=5107MiB (5355MB)</td><td>io=5288KiB (5415kB)</td></tr><tr><td>WRITE iops</td><td>IOPS=131k</td><td>IOPS=22</td></tr></tbody></table><p>测试显示：4K随机读写的带宽对比，内存型主机是传统主机的约6000倍，读IOPS/写IOPS，内存型主机是传统主机的约6000倍。</p><h4 id=42-fio-4k顺序读写>4.2 fio 4k顺序读写</h4><p>测试命令如下:</p><pre><code># fio --name TEST --eta-newline=5s --filename=fio-tempfile.dat --rw=rw --size=500m --io_size=10g --blocksize=4k --ioengine=libaio --fsync=1 --iodepth=1 --numjobs=1 --runtime=60 --group_reporting
</code></pre><table><thead><tr><th>指标</th><th>内存型主机</th><th>传统主机</th></tr></thead><tbody><tr><td>READ bw</td><td>bw=640MiB/s (671MB/s)</td><td>bw=73.2KiB/s (75.0kB/s)</td></tr><tr><td>READ io</td><td>io=5133MiB (5382MB)</td><td>io=4396KiB (4502kB)</td></tr><tr><td>READ iops</td><td>IOPS=164k</td><td>IOPS=18</td></tr><tr><td>WRITE bw</td><td>bw=637MiB/s (668MB/s)</td><td>bw=76.8KiB/s (78.6kB/s)</td></tr><tr><td>WRITE io</td><td>io=5107MiB (5355MB)</td><td>io=4608KiB (4719kB)</td></tr><tr><td>WRITE iops</td><td>IOPS=163k</td><td>IOPS=19</td></tr></tbody></table><p>测试显示：4K顺序读写的带宽对比，内存型主机是传统主机的约9000倍，读IOPS/写IOPS，内存型主机是传统主机的约9000倍。</p></div><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/24/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/24/>24</a></li><li class="page-item active"><a class=page-link href=/page/25/>25</a></li><li class=page-item><a class=page-link href=/page/26/>26</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/228/>228</a></li><li class=page-item><a href=/page/26/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/228/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div><script src=http://purplepalmdash.github.io/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>