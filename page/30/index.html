<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta name=generator content="Hugo 0.64.0"><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Dash &#183; Dash</title><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124"><link rel=stylesheet href=https://purplepalmdash.github.io/css/highlight/googlecode.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js></script><script type=text/javascript src=/js/html2canvas.js></script><script type=text/javascript>function genPostShot(){var rightNow=new Date();var imageName=rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");imageName+='.jpg'
html2canvas(document.getElementsByClassName('post'),{background:'#FFFFFF',onrendered:function(canvas){$('#test').attr('href',canvas.toDataURL("image/jpeg"));$('#test').attr('download',imageName);$('#test')[0].click();}});};</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124"><link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel=icon><link href=%7balternate%20%7bRSS%20application/rss+xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://purplepalmdash.github.io/index.xml%7d rel=alternate type=application/rss+xml title="Dash &#183; Dash"><meta name=description content><meta name=keywords content="unix,virtualization,embedded,linux"></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><img src=http://purplepalmdash.github.io/images/mylogo.jpeg alt=gravatar><h1><a href=http://purplepalmdash.github.io/>很惭愧，就做了一点微小的工作</a></h1><a href=http://purplepalmdash.github.io/><p>Dash</p></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/post/>All Posts</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/technology/>Technology</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/life/>Life</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/linuxtips/>LinuxTips</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/purplepalmdash><i class="fa fa-github-square fa-3x"></i></a><a href=https://cn.linkedin.com/in/yang-feipeng-1b909319><i class="fa fa-linkedin-square fa-3x"></i></a><a href=https://plus.google.com/u/0/106572959364703833986><i class="fa fa-google-plus-square fa-3x"></i></a><a href=https://www.facebook.com/yang.feipeng><i class="fa fa-facebook-square fa-3x"></i></a><a href=https://twitter.com/dashwillfly><i class="fa fa-twitter-square fa-3x"></i></a></li></ul></div></div><div class="content container"><div class=posts><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/09/07/workingtiponaarchftp/>Workingtiponaarchftp</a></h1><span class=post-date>Sep 7, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=ftpd服务端>ftpd服务端</h3><p>加载镜像:</p><pre><code># docker load&lt;ftpd.tar
Loaded image: gists/pure-ftpd:arm64
# docker images | grep ftpd
gists/pure-ftpd                                                            arm64               1b3e76d8756b        3 months ago        5.77MB
</code></pre><p>运行以下命令, 创建一个pure-ftpd实例, 当前目录下的ftpd含有ftpd的配置文件(pureftpd)及存储目录(data):</p><pre><code># mkdir ftpd
# cd ftpd
# mkdir pureftpd data
# docker run -d --restart unless-stopped --name pure-ftpd  -e MIN_PASV_PORT=40000 -e MAX_PASV_PORT=40009 -p 21:21  -p 40000-40009:40000-40009  -v $(pwd)/pureftpd:/etc/pureftpd  -v $(pwd)/data:/home/ftpuser gists/pure-ftpd:arm64
</code></pre><p>运行以下命令配置pure-ftpd的权限，以及添加test用户，并刷新pure-ftpd本地配置文件:</p><pre><code> docker exec -it pure-ftpd chown ftpuser:ftpuser -R /home/ftpuser
 docker exec -it pure-ftpd pure-pw useradd test -m -u ftpuser -d /home/ftpuser/test
 docker exec -it pure-ftpd pure-pw mkdb
</code></pre><h3 id=客户端>客户端</h3><p>举winscp ftp连接为例, 新建一个ftp连接:</p><p><img src=./images/2020_09_07_10_25_02_615x286.jpg alt=./images/2020_09_07_10_25_02_615x286.jpg></p><p>直接在winscp里拖拉实现上传下载:</p><p><img src=./images/2020_09_07_10_26_36_990x327.jpg alt=./images/2020_09_07_10_26_36_990x327.jpg></p><p>进度:</p><p><img src=./images/2020_09_07_10_29_46_649x286.jpg alt=./images/2020_09_07_10_29_46_649x286.jpg></p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/08/27/offlinearm64desktopenvsetup/>OfflineArm64DesktopEnvSetup</a></h1><span class=post-date>Aug 27, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=download-via-docker>Download via docker</h3><p>Run a docker instance via:</p><pre><code>$ sudo docker run -v /mnt:/mnt -it ubuntu:focal-20200115 /bin/bash
</code></pre><p>In docker instance, do following:</p><pre><code>rm -f /etc/apt/apt.conf.d/docker-clean
sed -i 's/ports.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list
cd /mnt
apt-get -d -o dir::cache=`pwd` -o Debug::NoLocking=1 install xubuntu-desktop xubuntu-core chromium-browser  firefox xrdp virt-manager ubuntu-wallpapers xubuntu-community-wallpapers xubuntu-community-wallpapers-focal xubuntu-wallpapers lxd lxc
apt-get install -y snapd
snap download lxd
snap download chromium

</code></pre><h3 id=transfer>Transfer</h3><p>Transfer these packages into the offline environments, and do following:</p><pre><code># cd ~/pkgs
# dpkg-scanpackages . /dev/null | gzip -9c &gt; Packages.gz
</code></pre><h3 id=install>Install</h3><p>In a ubuntu base environment, do following:</p><pre><code># vim /etc/apt/sources.list
deb [trusted=yes] file:///home/test/focal/ ./
# apt-get update -y
# apt-get install -y  xubuntu-desktop xubuntu-core  firefox xrdp virt-manager ubuntu-wallpapers xubuntu-community-wallpapers xubuntu-community-wallpapers-focal xubuntu-wallpapers
</code></pre><h3 id=configure-xrdp>Configure xrdp</h3><p>Configure xrdp via:</p><pre><code>$ sudo systemctl status xrdp
$ sudo adduser xrdp ssl-cert  
$ sudo systemctl restart xrdp
$ sudo ufw disable
$ echo xfce4-session &gt;~/.xsession
$ sudo vim /etc/xrdp/startwm.sh
#!/bin/sh

if [ -r /etc/default/locale ]; then
  . /etc/default/locale
  export LANG LANGUAGE
fi

startxfce4
$ sudo systemctl restart xrdp
</code></pre><p>So now you could use xfce4 as your remote desktop to linux.</p><h3 id=snapd-installation>snapd installation</h3><p>In docker run:</p><pre><code># sudo apt-get install -y snapd
# snap download lxd
# snap download chromium
# snap download gtk-common-themes
# snap download core
# snap download core18
</code></pre><p>Install sequence:</p><pre><code># snap install  (core/core18/gtk-common-themes/lxd/chromium)
chromium_1253.snap  core18_1888.snap  core_9806.snap                 gtk-common-themes_1506.snap  lxd_16946.snap
chromium_1253.assert  core18_1888.assert  core_9806.assert  gtk-common-themes_1506.assert  lxd_16946.assert   
</code></pre><h3 id=updated>Updated</h3><p>xrdp configuration:</p><pre><code>#!/bin/sh -e

# Install XRDP.
sudo apt install -y xrdp
sudo sed -e 's/^new_cursors=true/new_cursors=false/g' \
     -i /etc/xrdp/xrdp.ini
sudo systemctl enable xrdp
sudo systemctl restart xrdp

# Load Ubuntu config.
echo &quot;xfce4-session&quot; &gt; ~/.xsession
D=/usr/share/xfce4:/usr/share/xubuntu:/usr/local/share
D=${D}:/usr/share:/var/lib/snapd/desktop:/usr/share
cat &lt;&lt;EOF &gt; ~/.xsessionrc
export XDG_SESSION_DESKTOP=xubuntu
export XDG_DATA_DIRS=${D}
export XDG_CONFIG_DIRS=/etc/xdg/xdg-xubuntu:/etc/xdg:/etc/xdg
EOF

# Disable light-locker for avoiding error.
sudo cp /usr/bin/light-locker /usr/bin/light-locker.orig
cat &lt;&lt;EOF | sudo tee /usr/bin/light-locker
#!/bin/sh

# The light-locker uses XDG_SESSION_PATH provided by lightdm.
if [ ! -z &quot;\${XDG_SESSION_PATH}&quot; ]; then
  /usr/bin/light-locker.orig
else
  # Disable light-locker in XRDP.
  true
fi
EOF
sudo chmod a+x /usr/bin/light-locker
</code></pre><h3 id=final-script>Final script</h3><p>Final script is listed as following:</p><pre><code>#!/bin/bash
sudo cp -r focal /home/test/
echo 'deb [trusted=yes] file:///home/test/focal/ ./'|sudo tee -a /etc/apt/sources.list
sudo apt-get update -y
sudo apt-get install -y xubuntu-desktop xubuntu-core  firefox xrdp virt-manager ubuntu-wallpapers xubuntu-community-wallpapers xubuntu-community-wallpapers-focal xubuntu-wallpapers 
sudo adduser xrdp ssl-cert  
sudo systemctl restart xrdp
sudo ufw disable

sudo sed -e 's/^new_cursors=true/new_cursors=false/g' \
	     -i /etc/xrdp/xrdp.ini
sudo systemctl enable xrdp
sudo systemctl restart xrdp

# Load Ubuntu config.
echo &quot;xfce4-session&quot; &gt; ~/.xsession
D=/usr/share/xfce4:/usr/share/xubuntu:/usr/local/share
D=${D}:/usr/share:/var/lib/snapd/desktop:/usr/share
cat &lt;&lt;EOF &gt; ~/.xsessionrc
export XDG_SESSION_DESKTOP=xubuntu
export XDG_DATA_DIRS=${D}
export XDG_CONFIG_DIRS=/etc/xdg/xdg-xubuntu:/etc/xdg:/etc/xdg
EOF

# Disable light-locker for avoiding error.
sudo cp /usr/bin/light-locker /usr/bin/light-locker.orig
cat &lt;&lt;EOF | sudo tee /usr/bin/light-locker
#!/bin/sh

# The light-locker uses XDG_SESSION_PATH provided by lightdm.
if [ ! -z &quot;\${XDG_SESSION_PATH}&quot; ]; then
  /usr/bin/light-locker.orig
else
  # Disable light-locker in XRDP.
  true
fi
EOF
sudo chmod a+x /usr/bin/light-locker

# Install snapd
sudo snap ack snap/chromium_1253.assert
sudo snap ack snap/core18_1888.assert
sudo snap ack snap/core_9806.assert
sudo snap ack snap/gtk-common-themes_1506.assert
sudo snap ack snap/lxd_16946.assert

sudo snap install snap/core_9806.snap
sudo snap install snap/core18_1888.snap
sudo snap install snap/gtk-common-themes_1506.snap
sudo snap install snap/chromium_1253.snap
#sudo snap install snap/lxd_16946.snap

</code></pre><p>New user will be added if you want to create new session.</p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/07/29/bbbhidworkingtips/>BBBHIDWorkingTips</a></h1><span class=post-date>Jul 29, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=preparation>Preparation</h3><p>The system is:</p><pre><code>debian@beaglebone:~$ cat /etc/issue
Debian GNU/Linux 9 \n \l
BeagleBoard.org Debian Stretch imgtec Image 2020-04-06
Support: http://elinux.org/Beagleboard:BeagleBoneBlack_Debian
default username:password is [debian:temppwd]
debian@beaglebone:~$ uname -a
Linux beaglebone 4.14.108-ti-r131 #1stretch SMP PREEMPT Tue Mar 24 19:18:37 UTC 2020 armv7l GNU/Linux
</code></pre><p>Change password via <code>passwd</code>.<br>Enlarge the partition from 4GB to 32GB(32 GB disk)</p><pre><code>root@beaglebone:/home/debian# /opt/scripts/tools/grow_partition.sh 
# vim /etc/network/interfaces
auto eth0
iface eth0 inet dhcp
# reboot
##### Checking #############
debian@beaglebone:~$ df -h
Filesystem      Size  Used Avail Use% Mounted on
udev            217M     0  217M   0% /dev
tmpfs            49M  5.2M   44M  11% /run
/dev/mmcblk0p1   30G  2.5G   26G   9% /
</code></pre><p>Edit the sources.list configuration:</p><pre><code>$ cat /etc/apt/sources.list
deb http://mirrors.ustc.edu.cn/debian stretch main contrib non-free
#deb-src http://mirrors.ustc.edu.cn/debian stretch main contrib non-free

deb http://mirrors.ustc.edu.cn/debian stretch-updates main contrib non-free
#deb-src http://mirrors.ustc.edu.cn/debian stretch-updates main contrib non-free

deb http://mirrors.ustc.edu.cn/debian-security stretch/updates main contrib non-free
#deb-src http://mirrors.ustc.edu.cn/debian-security stretch/updates main contrib non-free
#  apt-get update -y 
# apt-get install -y libudev-dev libusb-dev awesome iotop
</code></pre><h3 id=usb-hid>USB HID</h3></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/07/27/quicktipsonterraformandlibvirtdonmultiple2/>QuickTipsOnTerraformAndLibvirtdOnMultiple2</a></h1><span class=post-date>Jul 27, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>紧接上一篇，这一篇引入<code>Cluster Mesh</code>用于暴露全局服务。</p><h3 id=cluster-mesh>Cluster Mesh</h3><p>因为我们上一节搭建的集群中已经设定了<code>cluster-name</code>及<code>cluster-id</code>， 故不需要对其进行更改，如果没有设置，则需要手动更改:</p><pre><code># kubectl -n kube-system edit cm cilium-config
[ ... add/edit ... ]
cluster-name: cluster1
cluster-id: &quot;1&quot;
</code></pre><p>两个集群上分别创建etcd服务的NodePort服务暴露:</p><pre><code>root@mouse-1:/mnt/Rong_cilium# kubectl apply -f etcdNodePort.yaml  -n kube-system
cservice/cilium-etcd-external created
root@mouse-1:/mnt/Rong_cilium# cat etcdNodePort.yaml 
apiVersion: v1
kind: Service
metadata:
  name: cilium-etcd-external
spec:
  type: NodePort
  ports:
  - port: 2379
  selector:
    app: etcd
    etcd_cluster: cilium-etcd
    io.cilium/app: etcd-operator
</code></pre><p>克隆<code> cilium/clustermesh-tools</code> 仓库，它含有用于取出密码及生成Kubernetes secrets配置的脚本文件，</p><pre><code># git clone https://github.com/cilium/clustermesh-tools.git
# cd clustermesh-tools
# cd clustermesh-tools/
# ls
clustermesh.yaml  config  ds.patch  extract-etcd-secrets.sh  generate-name-mapping.sh  generate-secret-yaml.sh
# rm -rf config/
# rm -f clustermesh.yaml 
# rm -f ds.patch 
</code></pre><p>生成<code>etcd-secrets</code>:</p><pre><code>root@mouse-1:/mnt/Rong_cilium/clustermesh-tools# ./extract-etcd-secrets.sh 
Derived cluster-name c1 from present ConfigMap
====================================================
 WARNING: The directory config contains private keys.
          Delete after use.
====================================================
</code></pre><p>在另一集群上重复以上操作。</p><p>从上面解压出来的keys/certificates生成单个Kubernetes secret， 这个secret将含有service IP/etcd节点名等用于访问的键/值。</p><pre><code># ./generate-secret-yaml.sh &gt; clustermesh.yaml
</code></pre><p>两个节点上分别运行以下命令，用于生成用于插入到<code>cilium</code> daemonset中的所需字段：</p><pre><code># # ./generate-name-mapping.sh &gt; ds.patch
root@cilium-1:/mnt/Rong_cilium_2/clustermesh-tools# ./generate-name-mapping.sh &gt; ds.patch
root@cilium-1:/mnt/Rong_cilium_2/clustermesh-tools# cat ds.patch 
spec:
  template:
    spec:
      hostAliases:
      - ip: &quot;10.137.149.72&quot;
        hostnames:
        - c2.mesh.cilium.io
      - ip: &quot;10.137.149.73&quot;
        hostnames:
        - c2.mesh.cilium.io
root@mouse-1:/mnt/Rong_cilium/clustermesh-tools# ./generate-name-mapping.sh &gt; ds.patch
root@mouse-1:/mnt/Rong_cilium/clustermesh-tools# cat ds.patch 
spec:
  template:
    spec:
      hostAliases:
      - ip: &quot;10.137.149.61&quot;
        hostnames:
        - c1.mesh.cilium.io
      - ip: &quot;10.137.149.62&quot;
        hostnames:
        - c1.mesh.cilium.io
</code></pre><p>组合出一个新的文件:</p><pre><code># cat combine_ds.patch 
spec:
  template:
    spec:
      hostAliases:
      - ip: &quot;10.137.149.61&quot;
        hostnames:
        - c1.mesh.cilium.io
      - ip: &quot;10.137.149.62&quot;
        hostnames:
        - c1.mesh.cilium.io
      - ip: &quot;10.137.149.72&quot;
        hostnames:
        - c2.mesh.cilium.io
      - ip: &quot;10.137.149.73&quot;
        hostnames:
        - c2.mesh.cilium.io
</code></pre><p>两个集群上分别apply：</p><pre><code># kubectl -n kube-system patch ds cilium -p &quot;$(cat combine_ds.patch)&quot;
daemonset.apps/cilium patched
</code></pre><p>两个集群上分别apply <code>cluster-mesh</code>:</p><pre><code>root@mouse-1:/mnt/Rong_cilium/clustermesh-tools# kubectl -n kube-system apply -f clustermesh.yaml
secret/cilium-clustermesh created
root@mouse-1:/mnt/Rong_cilium/clustermesh-tools# kubectl -n kube-system apply -f /mnt/Rong_cilium_2/clustermesh-tools/clustermesh.yaml 

root@cilium-1:/mnt/Rong_cilium_2/clustermesh-tools# kubectl -n kube-system apply -f clustermesh.yaml
secret/cilium-clustermesh created
root@cilium-1:/mnt/Rong_cilium_2/clustermesh-tools# kubectl -n kube-system apply -f /mnt/Rong_cilium/clustermesh-tools/clustermesh.yaml 
secret/cilium-clustermesh configured
</code></pre><p>重新启动所有节点上的<code>cilium-agent</code>，以便使用新的<code>cilium-clustermesh</code>密码文件中标识的集群名称、cluster id等，</p><pre><code>root@cilium-1:/mnt/Rong_cilium_2/clustermesh-tools# kubectl -n kube-system delete pod -l k8s-app=cilium
pod &quot;cilium-pb7rd&quot; deleted
pod &quot;cilium-wqdrv&quot; deleted
root@cilium-1:/mnt/Rong_cilium_2/clustermesh-tools# kubectl -n kube-system delete pod -l name=cilium-operator
pod &quot;cilium-operator-7cd598bdf6-42lwl&quot; deleted
root@mouse-1:/mnt/Rong_cilium/clustermesh-tools# kubectl -n kube-system delete pod -l k8s-app=cilium
pod &quot;cilium-5k5rj&quot; deleted
pod &quot;cilium-7m4gv&quot; deleted
root@mouse-1:/mnt/Rong_cilium/clustermesh-tools# kubectl -n kube-system delete pod -l name=cilium-operator
pod &quot;cilium-operator-7cd598bdf6-5s4q9&quot; deleted
</code></pre><p>测试集群pod的连接性:</p><pre><code>root@cilium-1:/home/test# kubectl exec -ti cilium-sqjm9 -n kube-system cilium node list
Name          IPv4 Address    Endpoint CIDR    IPv6 Address   Endpoint CIDR
c1/mouse-1    10.137.149.61   10.233.64.0/24                  
c1/mouse-2    10.137.149.62   10.233.65.0/24                  
c2/cilium-1   10.137.149.72   10.234.65.0/24                  
c2/cilium-2   10.137.149.73   10.234.64.0/24  
</code></pre><pre><code># kubectl run foo1 -it --rm --image  byrnedo/alpine-curl:latest  --command -- sh -c &quot;curl nginx&quot;
kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
# root@mouse-1:/home/test# kubectl logs foo1-78cf747c46-xmk9f
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre><p>如何证明其互通性？可以看到我们创建的curl pod是位于集群1中的，而所有的nginx负载则来自于受控集群(集群2）:</p><pre><code>root@mouse-1:/home/test# kubectl get pods -o wide
NAME                     READY   STATUS             RESTARTS   AGE    IP              NODE           NOMINATED NODE   READINESS GATES
foo-76db7d689b-vzlkg     0/1     CrashLoopBackOff   3          95s    10.233.64.84    mouse-1        &lt;none&gt;           &lt;none&gt;
foo1-78cf747c46-xmk9f    0/1     CrashLoopBackOff   3          118s   10.233.64.31    mouse-1        &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-8cw76   1/1     Running            0          50m    10.234.64.236   admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-c6rkm   1/1     Running            0          50m    10.234.65.98    admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-l2jmj   1/1     Running            0          49m    10.234.64.116   admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-r4p6p   1/1     Running            0          50m    10.234.65.54    admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-sc2jh   1/1     Running            0          49m    10.234.65.139   admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-t5vtx   1/1     Running            0          49m    10.234.64.177   admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-v4lz4   1/1     Running            0          50m    10.234.64.159   admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-wj4xf   1/1     Running            0          50m    10.234.65.82    admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-x4tpv   1/1     Running            0          49m    10.234.65.69    admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-xzfmx   1/1     Running            0          49m    10.234.64.18    admiralty-c2   &lt;none&gt;           &lt;none&gt;
</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/07/27/workingtipsonmulticlusterscheduler/>WorkingTipsOnMultiClusterScheduler</a></h1><span class=post-date>Jul 27, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>前段时间有同事给过来一篇很耸人听闻的文章, 号称能将K8S能力扩充到无限：</p><p><img src=/images/2020_07_27_14_24_35_855x675.jpg alt=/images/2020_07_27_14_24_35_855x675.jpg></p><p>搭建的过程其实蛮繁琐，最终我也没有将这个方案付诸实际。怎么说呢，感觉这个项目的文档非常不规范，缺乏实际落地的可行性。倒是在研究<code>virtual-kubelet</code>的过程中发现了<code>admiraltyio /multicluster-scheduler</code>的解决方案值得一试。搭建过程中也走了不少弯路。以下就是一个从0开始的部署过程，记下来以便以后参考。</p><h3 id=环境说明>环境说明</h3><p>采用双集群方式部署，每个集群由两台机器组成，分别列表展示如下:</p><p>集群1：</p><pre><code>主机名/IP/网关
mouse-1 10.137.149.61 10.137.149.1
mouse-2 10.137.149.62 10.137.149.1
kube_pods_subnet: 10.233.64.0/18
kube_service_addresses: 10.233.0.0/18
</code></pre><p>集群2：</p><pre><code>主机名/IP/网关
cilium-1 10.137.149.72 10.137.149.1
cilium-2 10.137.149.73 10.137.149.1
kube_service_addresses: 10.234.0.0/18
kube_pods_subnet: 10.234.64.0/18
</code></pre><p>采用kubespray的方法来部署集群，两个集群的部署时间因为是虚拟机的缘故大约在40分钟左右。kubespray默认提供了网络组件的安装的，但这里我们并不需要，注释掉了相关的部署脚本，在后面我们将采用手动的方式来部署cilium作为网络组件。同时因为<code>cert-manager</code>认证的原因我们加入了以下标记作为部署时参数:</p><pre><code>kube_network_plugin: cilium
kube_apiserver_enable_admission_plugins:
  - &quot;NamespaceLifecycle&quot;
  - &quot;LimitRanger&quot;
  - &quot;ServiceAccount&quot;
  - &quot;DefaultStorageClass&quot;
  - &quot;DefaultTolerationSeconds&quot;
  - &quot;MutatingAdmissionWebhook&quot;
  - &quot;ValidatingAdmissionWebhook&quot;
  - &quot;Priority&quot;
  - &quot;ResourceQuota&quot;
additional_no_proxy: &quot;.domain,corp,.company.com,.svc,.svc.cluster.local&quot;
</code></pre><p><code>cluster.yml</code>中去掉的关于网络组件部署的条目:</p><pre><code>#    - { role: kubernetes-apps/network_plugin, tags: network }
....
#- hosts: k8s-cluster
#  gather_facts: False
#  any_errors_fatal: &quot;{{ any_errors_fatal | default(true) }}&quot;
#  roles:
#    - { role: kubespray-defaults }
#    - { role: kubernetes/preinstall, when: &quot;dns_mode != 'none' and resolvconf_mode == 'host_resolvconf'&quot;, tags: resolvconf, dns_late: true }
</code></pre><p>部署完成后检查节点:</p><pre><code>集群1: 
root@mouse-1:/mnt/Rong_cilium# kubectl get nodes
NAME      STATUS     ROLES    AGE     VERSION
mouse-1   NotReady   master   4m47s   v1.17.5
mouse-2   NotReady   &lt;none&gt;   2m16s   v1.17.5
root@mouse-1:/mnt/Rong_cilium# kubectl get pods --all-namespaces
NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE
kube-system   coredns-76798d84dd-tl8vb                      0/1     Pending   0          96s
kube-system   dns-autoscaler-85f898cd5c-lsxj5               0/1     Pending   0          85s
kube-system   kube-apiserver-mouse-1                        1/1     Running   0          4m11s
kube-system   kube-controller-manager-mouse-1               1/1     Running   0          4m48s
kube-system   kube-proxy-5jbh7                              1/1     Running   0          2m15s
kube-system   kube-proxy-x75k7                              1/1     Running   0          2m10s
kube-system   kube-scheduler-mouse-1                        1/1     Running   0          4m48s
kube-system   kubernetes-dashboard-857df7d6f7-nrrcj         0/1     Pending   0          69s
kube-system   kubernetes-metrics-scraper-747b4fd5cd-whjpj   0/1     Pending   0          61s
kube-system   metrics-server-754db6c5f7-8hbd4               0/2     Pending   0          31s
kube-system   nginx-proxy-mouse-2                           1/1     Running   1          2m19s

集群2:   
root@cilium-1:/mnt/Rong_cilium_2# kubectl get nodes
NAME       STATUS     ROLES    AGE     VERSION
cilium-1   NotReady   master   4m43s   v1.17.5
cilium-2   NotReady   &lt;none&gt;   2m46s   v1.17.5
root@cilium-1:/mnt/Rong_cilium_2# kubectl get pods --all-namespaces
NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE
kube-system   coredns-76798d84dd-7j5xx                      0/1     Pending   0          2m15s
kube-system   dns-autoscaler-85f898cd5c-grrpc               0/1     Pending   0          2m10s
kube-system   kube-apiserver-cilium-1                       1/1     Running   0          4m24s
kube-system   kube-controller-manager-cilium-1              1/1     Running   0          4m24s
kube-system   kube-proxy-l2slf                              1/1     Running   0          2m52s
kube-system   kube-proxy-zf4l2                              1/1     Running   0          2m50s
kube-system   kube-scheduler-cilium-1                       1/1     Running   0          4m24s
kube-system   kubernetes-dashboard-857df7d6f7-sz72b         0/1     Pending   0          2m8s
kube-system   kubernetes-metrics-scraper-747b4fd5cd-t7cmp   0/1     Pending   0          2m8s
kube-system   metrics-server-754db6c5f7-d9bzv               0/2     Pending   0          107s
kube-system   nginx-proxy-cilium-2                          1/1     Running   0          2m55s
root@cilium-1:/mnt/Rong_cilium_2# helm version
version.BuildInfo{Version:&quot;v3.1.2&quot;, GitCommit:&quot;d878d4d45863e42fd5cff6743294a11d28a9abce&quot;, GitTreeState:&quot;clean&quot;, GoVersion:&quot;go1.13.8&quot;}
root@cilium-1:/mnt/Rong_cilium_2# uname -a
Linux cilium-1 4.15.0-76-generic #86-Ubuntu SMP Fri Jan 17 17:24:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
root@cilium-1:/mnt/Rong_cilium_2# cat /etc/issue
Ubuntu 18.04.4 LTS \n \l
</code></pre><h3 id=cilium创建>cilium创建</h3><p>离线状况下，分别在两个集群中运行以下命令以配置cilium：</p><p>集群1:</p><pre><code>root@mouse-1:/mnt/Rong_cilium/cilium# helm install cilium . --namespace kube-system   --set global.etcd.enabled=true --set global.etcd.managed=true --set global.ipam.operator.clusterPoolIPv4PodCIDR=10.233.64.0/18 --set global.cluster.id=1  --set global.cluster.name=c1
LAST DEPLOYED: Mon Jul 27 14:54:44 2020
NAMESPACE: kube-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
You have successfully installed Cilium.

Your release version is 1.8.1.

For any further help, visit https://docs.cilium.io/en/v1.8/gettinghelp

</code></pre><p>集群2:</p><pre><code>root@cilium-1:/mnt/Rong_cilium_2/cilium# helm install cilium . --namespace kube-system   --set global.etcd.enabled=true --set global.etcd.managed=true --set global.ipam.operator.clusterPoolIPv4PodCIDR=10.234.64.0/18 --set global.cluster.id=2  --set global.cluster.name=c2
</code></pre><p>如果是在线状态下，则改用以下命令(&mldr;&mldr;.后续命令一样):</p><pre><code>$ helm install cilium cilium/cilium --version 1.8.2 --namespace kube-system .........
</code></pre><p>两个节点上均激活CoreDNS的<code>reverse lookup</code>:</p><pre><code># kubectl -n kube-system edit cm coredns
[...]
apiVersion: v1
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
          pods insecure
          upstream
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        proxy . /etc/resolv.conf
        cache 30
    }
</code></pre><p>需要经过大约5～10分钟（视机器性能而异)， 等待所有的服务正常, 以集群1为例:</p><pre><code>root@mouse-1:/mnt/Rong_cilium/cilium# kubectl get pods --all-namespaces
NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE
kube-system   cilium-6txnn                                  1/1     Running   0          10m
kube-system   cilium-8ltpd                                  1/1     Running   0          10m
kube-system   cilium-etcd-dggrz7wn94                        1/1     Running   0          12m
kube-system   cilium-etcd-j7tzh2jcjg                        1/1     Running   0          7m55s
kube-system   cilium-etcd-operator-584788b99c-z5fzp         1/1     Running   0          19m
kube-system   cilium-etcd-rf5vjsrp7n                        1/1     Running   0          13m
kube-system   cilium-operator-7cd598bdf6-5s4q9              1/1     Running   3          19m
kube-system   coredns-76798d84dd-8p7q5                      1/1     Running   0          15m
kube-system   coredns-76798d84dd-tl8vb                      1/1     Running   0          25m
kube-system   dns-autoscaler-85f898cd5c-lsxj5               1/1     Running   0          25m
kube-system   etcd-operator-59cf4cfb7c-p444s                1/1     Running   0          13m
kube-system   kube-apiserver-mouse-1                        1/1     Running   0          28m
kube-system   kube-controller-manager-mouse-1               1/1     Running   0          28m
kube-system   kube-proxy-5jbh7                              1/1     Running   0          26m
kube-system   kube-proxy-x75k7                              1/1     Running   0          26m
kube-system   kube-scheduler-mouse-1                        1/1     Running   0          28m
kube-system   kubernetes-dashboard-857df7d6f7-nrrcj         1/1     Running   0          25m
kube-system   kubernetes-metrics-scraper-747b4fd5cd-whjpj   1/1     Running   0          24m
kube-system   metrics-server-5c55b76c7-p9j47                2/2     Running   1          15m
kube-system   nginx-proxy-mouse-2                           1/1     Running   1          26m
root@mouse-1:/mnt/Rong_cilium/cilium# kubectl get svc --all-namespaces
NAMESPACE     NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
default       kubernetes                  ClusterIP   10.233.0.1      &lt;none&gt;        443/TCP                  28m
kube-system   cilium-etcd                 ClusterIP   None            &lt;none&gt;        2379/TCP,2380/TCP        13m
kube-system   cilium-etcd-client          ClusterIP   10.233.44.240   &lt;none&gt;        2379/TCP                 13m
kube-system   coredns                     ClusterIP   10.233.0.3      &lt;none&gt;        53/UDP,53/TCP,9153/TCP   25m
kube-system   dashboard-metrics-scraper   ClusterIP   10.233.61.59    &lt;none&gt;        8000/TCP                 25m
kube-system   kubernetes-dashboard        ClusterIP   10.233.39.108   &lt;none&gt;        443/TCP                  25m
kube-system   metrics-server              ClusterIP   10.233.41.242   &lt;none&gt;        443/TCP                  24m
</code></pre><p>编辑cilium的ds属性，以使其不调度到将来创建的虚拟节点上:</p><pre><code># kubectl edit ds cilium -n kube-system
spec:
  template:
    spec:
      tolerations:
      - operator: Exists
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: virtual-kubelet.io/provider
                operator: DoesNotExist
</code></pre><h3 id=multicluster-scheduler>multicluster-scheduler</h3><p>这里我们部署的版本是<code>v0.10.0-rc.1</code>, 部署步骤如下:</p><p>在两个集群中分别按以下命令部署cert-manager:</p><pre><code># kubectl apply --validate=false -f  https://raw.githubusercontent.com/jetstack/cert-manager/release-0.12/deploy/manifests/00-crds.yaml
# kubectl create namespace cert-manager
# helm install cert-manager \
    --namespace cert-manager \
    --version v0.12.0 \
    --wait \
    jetstack/cert-manager
</code></pre><p>注:离线情况下命令如下:</p><pre><code>切换到项目根目录
# kubectl apply --validate=false -f 00-crds.yaml
# kubectl create namespace cert-manager
# cd cert-manager/
# helm install cert-manager . --namespace cert-manager
</code></pre><p>分别检查<code>cert-manager</code>安装情况:</p><pre><code>root@cilium-1:/mnt/Rong_cilium_2/cert-manager# kubectl get pods -n cert-manager
NAME                                      READY   STATUS    RESTARTS   AGE
cert-manager-754d9b75d9-4nn96             1/1     Running   0          97s
cert-manager-cainjector-85fbdf788-lbhbp   1/1     Running   0          97s
cert-manager-webhook-76f9b64b45-wv2dv     1/1     Running   0          97s
</code></pre><h3 id=受控集群>受控集群</h3><p>受控集群安装:</p><pre><code># kubectl create ns admiralty
namespace/admiralty created
# helm install multicluster-scheduler admiralty/multicluster-scheduler \
  --namespace admiralty \
  --version 0.10.0-rc.1 \
  --set clusterName=c2
</code></pre><p>离线情况下:</p><pre><code>root@cilium-1:/mnt/Rong_cilium_2/multicluster-scheduler# kubectl create ns admiralty
namespace/admiralty created
root@cilium-1:/mnt/Rong_cilium_2/multicluster-scheduler# helm install multicluster-scheduler  . --namespace admiralty --set clusterName=c2
NAME: multicluster-scheduler
LAST DEPLOYED: Mon Jul 27 15:36:12 2020
NAMESPACE: admiralty
STATUS: deployed
REVISION: 1
TEST SUITE: None
</code></pre><p>检查运行情况:</p><pre><code>root@cilium-1:/mnt/Rong_cilium_2/multicluster-scheduler# kubectl get pods  -n admiralty
NAME                                                          READY   STATUS    RESTARTS   AGE
multicluster-scheduler-candidate-scheduler-64bf58cbc8-v5mks   1/1     Running   0          7m38s
multicluster-scheduler-controller-manager-5844784fb8-bxpbx    1/1     Running   0          7m38s
multicluster-scheduler-proxy-scheduler-65d58b4548-k7hkb       1/1     Running   0          7m38s
</code></pre><h3 id=控制集群>控制集群</h3><p>控制集群，即主集群安装:</p><pre><code># kubectl create ns admiralty
namespace/admiralty created
# helm install multicluster-scheduler admiralty/multicluster-scheduler \
  --namespace admiralty \
  --version 0.10.0-rc.1 \
  --set clusterName=c1 \
  --set targetSelf=true \
  --set targets[0].name=c2
</code></pre><p>离线情况下:</p><pre><code>root@mouse-1:/mnt/Rong_cilium# kubectl  create namespace admiralty
namespace/admiralty created
root@mouse-1:/mnt/Rong_cilium/multicluster-scheduler# helm install multicluster-scheduler . --namespace admiralty  --set clusterName=c1 --set targetSelf=true --set targets[0].name=c2
NAME: multicluster-scheduler
LAST DEPLOYED: Mon Jul 27 15:41:38 2020
NAMESPACE: admiralty
STATUS: deployed
REVISION: 1
TEST SUITE: None
</code></pre><p>此时检查运行情况:</p><pre><code>root@mouse-1:/mnt/Rong_cilium/multicluster-scheduler# kubectl get pods -n admiralty
NAME                                                         READY   STATUS              RESTARTS   AGE
multicluster-scheduler-candidate-scheduler-bc484d49d-dkfjq   1/1     Running             0          2m47s
multicluster-scheduler-controller-manager-7d84b5dc69-2l2nx   0/1     ContainerCreating   0          2m47s
multicluster-scheduler-proxy-scheduler-bf9c844df-6pdcq       0/1     ContainerCreating   0          2m47s
</code></pre><p><code>controller-manager</code>和<code>proxy-scheduler</code> pod一直处于<code>ContainerCreating</code>状态是正确的，因为在控制集群内我们还没有引入受控集群的secret。</p><h3 id=交换service-account>交换Service Account</h3><p><code>受控集群</code>下，运行以下命令:</p><pre><code># kubectl apply -f https://raw.githubusercontent.com/ibuildthecloud/klum/v0.0.1/deploy.yaml
# cat &lt;&lt;EOF | kubectl --context &quot;$CLUSTER2&quot; apply -f -
kind: User
apiVersion: klum.cattle.io/v1alpha1
metadata:
  name: c1
spec:
  clusterRoles:
    - multicluster-scheduler-source
    - multicluster-scheduler-cluster-summary-viewer
EOF
</code></pre><p>离线情况下，部署的命令为:</p><pre><code># kubectl apply -f deploy-klum.yaml 
namespace/klum created
deployment.apps/klum created
serviceaccount/klum created
clusterrolebinding.rbac.authorization.k8s.io/klum-cluster-admin created
root@cilium-1:/mnt/Rong_cilium_2# cat apply-klum.yaml 
kind: User
apiVersion: klum.cattle.io/v1alpha1
metadata:
  name: c1
spec:
  clusterRoles:
    - multicluster-scheduler-source
    - multicluster-scheduler-cluster-summary-viewer
root@cilium-1:/mnt/Rong_cilium_2# kubectl apply -f apply-klum.yaml 
user.klum.cattle.io/c1 created
</code></pre><p>检查运行情况:</p><pre><code>root@cilium-1:/mnt/Rong_cilium_2# kubectl get pods -n klum
NAME                    READY   STATUS    RESTARTS   AGE
klum-74f54765d4-zqv4b   1/1     Running   0          92s
</code></pre><p>下载<code>kubemcsa</code>二进制文件，以便于快速导出kubeconfig secret文件:</p><pre><code># MCSA_RELEASE_URL=https://github.com/admiraltyio/multicluster-service-account/releases/download/v0.6.1
# OS=linux # or darwin (i.e., OS X) or windows
# ARCH=amd64 # if you're on a different platform, you must know how to build from source
# curl -Lo kubemcsa &quot;$MCSA_RELEASE_URL/kubemcsa-$OS-$ARCH&quot;
# chmod +x kubemcsa
</code></pre><p>运行<code>kubemcsa export</code>生成对应的密码文件:</p><pre><code> ./kubemcsa-linux-amd64 export -n klum c1 --as c2&gt;c1asc2.yaml
</code></pre><p>传输<code>c1asc2.yaml</code>文件至<code>控制节点</code>上并apply， 此时上节中处于<code>Creating</code>状态的pod会正常运行:</p><pre><code>root@mouse-1:/mnt/Rong_cilium# kubectl -n admiralty apply -f /mnt/Rong_cilium_2/c1asc2.yaml
secret/c2 created
root@mouse-1:/mnt/Rong_cilium# kubectl get pods -n admiralty
NAME                                                         READY   STATUS    RESTARTS   AGE
multicluster-scheduler-candidate-scheduler-bc484d49d-dkfjq   1/1     Running   0          12m
multicluster-scheduler-controller-manager-7d84b5dc69-qc98k   1/1     Running   0          49s
multicluster-scheduler-proxy-scheduler-bf9c844df-fg5hk       1/1     Running   0          39s
</code></pre><p>此时在控制集群的<code>kube-master</code>节点上刷新集群节点信息可以看到虚拟节点已经被加入到集群:</p><pre><code>root@mouse-1:/mnt/Rong_cilium# kubectl get nodes -o wide
NAME           STATUS   ROLES     AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
admiralty-c1   Ready    cluster   67s             &lt;none&gt;          &lt;none&gt;        &lt;unknown&gt;            &lt;unknown&gt;           &lt;unknown&gt;
admiralty-c2   Ready    cluster   67s             &lt;none&gt;          &lt;none&gt;        &lt;unknown&gt;            &lt;unknown&gt;           &lt;unknown&gt;
mouse-1        Ready    master    69m   v1.17.5   10.137.149.61   &lt;none&gt;        Ubuntu 18.04.4 LTS   4.15.0-76-generic   docker://19.3.9
mouse-2        Ready    &lt;none&gt;    66m   v1.17.5   10.137.149.62   &lt;none&gt;        Ubuntu 18.04.4 LTS   4.15.0-76-generic   docker://19.3.9
</code></pre><h3 id=控制集群节点上的multi-cluster部署>控制集群节点上的Multi-Cluster部署</h3><p>Multicluster-scheduler的pod admission controller会查看标注为<code>multicluster-scheduler=enabled</code>的命名空间，在控制集群的<code>kube-master</code>节点上，标记<code>default</code>空间:</p><pre><code>root@mouse-1:/mnt/Rong_cilium# kubectl label namespace default multicluster-scheduler=enabled
namespace/default labeled
</code></pre><p>接着创建一个NGINX应用，注意中间使用了<code>election</code>标记:</p><pre><code>root@mouse-1:/mnt/Rong_cilium# cat deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 10
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
      annotations:
        multicluster.admiralty.io/elect: &quot;&quot;
    spec:
      containers:
      - name: nginx
        image: nginx:1.17
        resources:
          requests:
            cpu: 100m
            memory: 32Mi
        ports:
        - containerPort: 80
root@mouse-1:/mnt/Rong_cilium# kubectl apply -f deploy.yaml 
deployment.apps/nginx created
</code></pre><p>检查运行情况:</p><pre><code>root@cilium-1:/home/test# kubectl get pods
NAME                           READY   STATUS    RESTARTS   AGE
nginx-7bb89ffbfb-bs45b-l468n   1/1     Running   0          89s
nginx-7bb89ffbfb-h6btc-4587q   1/1     Running   0          2m
nginx-7bb89ffbfb-mjpf6-n2gc7   1/1     Running   0          2m3s
nginx-7bb89ffbfb-p77d4-phf9f   1/1     Running   0          82s
nginx-7bb89ffbfb-pbj69-hrjqs   1/1     Running   0          117s


root@mouse-1:/mnt/Rong_cilium# kubectl get pods -o wide
NAME                           READY   STATUS    RESTARTS   AGE    IP              NODE           NOMINATED NODE   READINESS GATES
nginx-7bb89ffbfb-55px2         1/1     Running   0          101s   10.233.64.91    admiralty-c1   &lt;none&gt;           &lt;none&gt;
nginx-7bb89ffbfb-55px2-dglzq   1/1     Running   0          88s    10.233.64.91    mouse-1        &lt;none&gt;           &lt;none&gt;
nginx-7bb89ffbfb-59tl5         1/1     Running   0          101s   10.233.64.68    admiralty-c1   &lt;none&gt;           &lt;none&gt;
nginx-7bb89ffbfb-59tl5-mjl5b   1/1     Running   0          92s    10.233.64.68    mouse-1        &lt;none&gt;           &lt;none&gt;
nginx-7bb89ffbfb-bs45b         1/1     Running   0          101s   10.234.64.214   admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-7bb89ffbfb-dj27d         1/1     Running   0          101s   10.233.64.55    admiralty-c1   &lt;none&gt;           &lt;none&gt;
nginx-7bb89ffbfb-dj27d-8tfwn   1/1     Running   0          53s    10.233.64.55    mouse-1        &lt;none&gt;           &lt;none&gt;
nginx-7bb89ffbfb-h6btc         1/1     Running   0          101s   10.234.64.199   admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-7bb89ffbfb-hx7fc         1/1     Running   0          101s   10.233.64.159   admiralty-c1   &lt;none&gt;           &lt;none&gt;
nginx-7bb89ffbfb-hx7fc-tbgcr   1/1     Running   0          84s    10.233.64.159   mouse-1        &lt;none&gt;           &lt;none&gt;
nginx-7bb89ffbfb-lgtmw         1/1     Running   0          101s   10.233.64.223   admiralty-c1   &lt;none&gt;           &lt;none&gt;
nginx-7bb89ffbfb-lgtmw-hp4ww   1/1     Running   0          74s    10.233.64.223   mouse-1        &lt;none&gt;           &lt;none&gt;
nginx-7bb89ffbfb-mjpf6         1/1     Running   0          102s   10.234.64.202   admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-7bb89ffbfb-p77d4         1/1     Running   0          101s   10.234.64.110   admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-7bb89ffbfb-pbj69         1/1     Running   0          101s   10.234.64.103   admiralty-c2   &lt;none&gt;           &lt;none&gt;
</code></pre><p>加入一个<code>region</code> label 用于调度:</p><pre><code>控制集群
# kubectl label nodes -l virtual-kubelet.io/provider!=admiralty topology.kubernetes.io/region=us
受控集群
# kubectl label nodes -l virtual-kubelet.io/provider!=admiralty topology.kubernetes.io/region=eu
</code></pre><p>调度到<code>c2</code>集群上，因c2集群的<code>region</code> 为 <code>eu</code>:</p><pre><code>root@mouse-1:/mnt/Rong_cilium# kubectl patch deployment nginx -p '{
  &quot;spec&quot;:{
    &quot;template&quot;:{
      &quot;spec&quot;: {
        &quot;nodeSelector&quot;: {
          &quot;topology.kubernetes.io/region&quot;: &quot;eu&quot;
        }
      }
    }
  }
}'
</code></pre><p>检查调度情况:</p><pre><code>root@cilium-1:/mnt/Rong_cilium_2# kubectl get pods
NAME                           READY   STATUS    RESTARTS   AGE
nginx-58b97d4885-8cw76-65zz7   1/1     Running   0          94s
nginx-58b97d4885-c6rkm-dns4r   1/1     Running   0          112s
nginx-58b97d4885-l2jmj-5hlvf   1/1     Running   0          60s
nginx-58b97d4885-r4p6p-wj98b   1/1     Running   0          110s
nginx-58b97d4885-sc2jh-89d6t   1/1     Running   0          82s
nginx-58b97d4885-t5vtx-6n2nn   1/1     Running   0          46s
nginx-58b97d4885-v4lz4-qfctr   1/1     Running   0          101s
nginx-58b97d4885-wj4xf-hjx6v   1/1     Running   0          89s
nginx-58b97d4885-x4tpv-shvrh   1/1     Running   0          68s
nginx-58b97d4885-xzfmx-vmz8h   1/1     Running   0          34s

root@mouse-1:/mnt/Rong_cilium# kubectl get pods -o wide
NAME                     READY   STATUS    RESTARTS   AGE    IP              NODE           NOMINATED NODE   READINESS GATES
nginx-58b97d4885-8cw76   1/1     Running   0          103s   10.234.64.236   admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-c6rkm   1/1     Running   0          104s   10.234.65.98    admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-l2jmj   1/1     Running   0          54s    10.234.64.116   admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-r4p6p   1/1     Running   0          104s   10.234.65.54    admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-sc2jh   1/1     Running   0          79s    10.234.65.139   admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-t5vtx   1/1     Running   0          39s    10.234.64.177   admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-v4lz4   1/1     Running   0          104s   10.234.64.159   admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-wj4xf   1/1     Running   0          103s   10.234.65.82    admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-x4tpv   1/1     Running   0          61s    10.234.65.69    admiralty-c2   &lt;none&gt;           &lt;none&gt;
nginx-58b97d4885-xzfmx   1/1     Running   0          35s    10.234.64.18    admiralty-c2   &lt;none&gt;           &lt;none&gt;
</code></pre><p>由上可以看到，虚拟节点及c2物理节点上存在一一对应, 而更改为us则全部会切换回控制集群。</p></div><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/29/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/29/>29</a></li><li class="page-item active"><a class=page-link href=/page/30/>30</a></li><li class=page-item><a class=page-link href=/page/31/>31</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/224/>224</a></li><li class=page-item><a href=/page/31/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/224/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div><script src=http://purplepalmdash.github.io/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>