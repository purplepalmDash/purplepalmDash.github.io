<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta name=generator content="Hugo 0.64.0"><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Dash &#183; Dash</title><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/poole-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-overrides.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/hyde-a.css?ref=abc124"><link rel=stylesheet href="https://purplepalmdash.github.io/css/custom-additions.css?ref=abc124"><link rel=stylesheet href=https://purplepalmdash.github.io/css/highlight/googlecode.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/docco.min.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js></script><script type=text/javascript src=/js/html2canvas.js></script><script type=text/javascript>function genPostShot(){var rightNow=new Date();var imageName=rightNow.toISOString().slice(0,16).replace(/(-)|(:)|(T)/g,"");imageName+='.jpg'
html2canvas(document.getElementsByClassName('post'),{background:'#FFFFFF',onrendered:function(canvas){$('#test').attr('href',canvas.toDataURL("image/jpeg"));$('#test').attr('download',imageName);$('#test')[0].click();}});};</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=apple-touch-icon-precomposed sizes=144x144 href="http://purplepalmdash.github.io/touch-icon-144-precomposed.png?ref=abc124"><link href="http://purplepalmdash.github.io/favicon.png?ref=abc124" rel=icon><link href=%7balternate%20%7bRSS%20application/rss+xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20http://purplepalmdash.github.io/index.xml%7d rel=alternate type=application/rss+xml title="Dash &#183; Dash"><meta name=description content><meta name=keywords content="unix,virtualization,embedded,linux"></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><img src=http://purplepalmdash.github.io/images/mylogo.jpeg alt=gravatar><h1><a href=http://purplepalmdash.github.io/>很惭愧，就做了一点微小的工作</a></h1><a href=http://purplepalmdash.github.io/><p>Dash</p></a></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/post/>All Posts</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/technology/>Technology</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/life/>Life</a></li><li class=sidebar-nav-item><a href=http://purplepalmdash.github.io/categories/linuxtips/>LinuxTips</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/purplepalmdash><i class="fa fa-github-square fa-3x"></i></a><a href=https://cn.linkedin.com/in/yang-feipeng-1b909319><i class="fa fa-linkedin-square fa-3x"></i></a><a href=https://plus.google.com/u/0/106572959364703833986><i class="fa fa-google-plus-square fa-3x"></i></a><a href=https://www.facebook.com/yang.feipeng><i class="fa fa-facebook-square fa-3x"></i></a><a href=https://twitter.com/dashwillfly><i class="fa fa-twitter-square fa-3x"></i></a></li></ul></div></div><div class="content container"><div class=posts><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/02/14/ovirt-hyperconverged-inair-gapped-environment/>Ovirt HyperConverged InAir-Gapped Environment</a></h1><span class=post-date>Feb 14, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=0-aim>0. AIM</h3><p>For deploying Ovirt HyperConverged in air-gapped environment.<br>For some companies, their inner environment is air-gapped, e.g OA network. In
such air-gapped environment we could only use ISO and take some packages in
cd-roms for taking into their intra-network. How to deploy a ovirt drivened
private cloud in air-gapped room, I will take some experiment and try the
solution out.</p><h3 id=1-environment>1. Environment</h3><p>In this chapter the environment will be available for ovirt deployment with glusterfs.</p><h4 id=11-hardware>1.1 Hardware</h4><p>I use my home machine for building the environment, the hardware is listed as:</p><pre><code>CPU: Intel(R) Core(TM) i5-4460  CPU @ 3.20GHz
Memory: DDR3 1600 32G
Disk: 1T HDD.
</code></pre><h4 id=12-osnetworkingsoftware>1.2 OS/Networking/Software</h4><p>My home machine runs ArchLinux, with nested virtualization.<br>Use qemu and virt-manager for setting the environment.</p><pre><code># qemu-system-x86_64 --version                                                                                                           
QEMU emulator version 4.2.0
Copyright (c) 2003-2019 Fabrice Bellard and the QEMU Project developers
# virt-manager --version
2.2.1
</code></pre><p>I setup a isolated networking in virt-manager, cidr is 10.20.30.0/24, 3 vms
will use this isolated networking for emulating the air-gapped environment, its name is <code>ovirt-isolated</code>:</p><p><img src=/images/2020_02_14_14_51_10_545x560.jpg alt=/images/2020_02_14_14_51_10_545x560.jpg></p><h4 id=13-vms-preparation>1.3 VMs Preparation</h4><p>I use 3 vms for setting up the environment, each of them have:</p><pre><code>2 vcpus
10240 MB memory
vda: 100 GB, for installing the system. 
vdb: 300 GB, for setting up the storage network.   
NIC: 1x, attached to ovirt-isolated networking. 
</code></pre><p>hostname - IP is listed as following:</p><pre><code>instance1.com	10.20.30.31
instance2.com	10.20.30.32
instance3.com	10.20.30.33
engineinstance.com	10.20.30.34
</code></pre><p>For setting up the ip address, use <code>nmtui</code> in terminal, take instance1.com for example:</p><p><img src=/images/2020_02_14_15_20_56_601x304.jpg alt=/images/2020_02_14_15_20_56_601x304.jpg></p><p>For setting up the hostname, also use <code>nmtui</code>:</p><p><img src=/images/2020_02_14_15_22_33_450x232.jpg alt=/images/2020_02_14_15_22_33_450x232.jpg></p><p>Login to each machine and enable the password-less login, take instance1 for example:</p><pre><code># ssh-keygen
# vim /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.20.30.31     instance1.com
10.20.30.32     instance2.com
10.20.30.33     instance3.com
10.20.30.34	engineinstance.com
# ssh-copy-id root@instance1.com
# ssh-copy-id root@instance2.com
# ssh-copy-id root@instance3.com
</code></pre><p>Also add following items(engine vm&rsquo;s hostname and ip address) into host machine(archLinux)&lsquo;s <code>/etc/hosts</code>:</p><pre><code>10.20.30.31     instance1.com
10.20.30.32     instance2.com
10.20.30.33     instance3.com
10.20.30.34	engineinstance.com
</code></pre><h3 id=2-deploy-glusterfs>2. Deploy Glusterfs</h3><p>Use firefox for visiting <code>https://10.20.30.31:9090</code>:</p><p><img src=/images/2020_02_14_15_39_07_796x287.jpg alt=/images/2020_02_14_15_39_07_796x287.jpg></p><p>use root for login, enter the <code>instance1.com</code>'s cockpit web:</p><p><img src=/images/2020_02_14_15_39_38_728x474.jpg alt=/images/2020_02_14_15_39_38_728x474.jpg></p><p>Click <code>V</code>-><code>Hosted Engine</code>, then click the <code>start</code> button under <code>Hyperconverged</code>:</p><p><img src=/images/2020_02_14_15_42_07_1040x611.jpg alt=/images/2020_02_14_15_42_07_1040x611.jpg></p><p>Click <code>Run Gluster Wizard</code>:</p><p><img src=/images/2020_02_14_15_43_46_665x134.jpg alt=/images/2020_02_14_15_43_46_665x134.jpg></p><p>Fill in 3 nodes&rsquo;s hostname, click <code>next</code>:</p><p><img src=/images/2020_02_14_15_45_01_890x403.jpg alt=/images/2020_02_14_15_45_01_890x403.jpg></p><p>In <code>Additional Hosts</code>, click <code>Use same hostnames as in previous step</code>, thus Host2 and Hosts3 will be added automatically:</p><p><img src=/images/2020_02_14_15_47_45_880x515.jpg alt=/images/2020_02_14_15_47_45_880x515.jpg></p><p>In <code>Packages</code> we keep the default empty items and click next for continue.</p><p>Keep the default volumn setting, and enable the Arbiter for <code>data</code> and <code>vmstore</code>:</p><p><img src=/images/2020_02_14_15_57_02_843x459.jpg alt=/images/2020_02_14_15_57_02_843x459.jpg></p><p>Here we adjust the LV device name to <code>vdb</code>, and adjust the size as <code>80,80,80</code>, click next for continue:</p><p>The volume size for running engine vm should be at least 58GB(ovirt default minimum size, actually takes more than this number. )</p><p><img src=/images/2020_02_14_16_00_34_809x611.jpg alt=/images/2020_02_14_16_00_34_809x611.jpg></p><p>Review and click deploy:</p><p><img src=/images/2020_02_14_16_02_53_870x648.jpg alt=/images/2020_02_14_16_02_53_870x648.jpg></p><p>The ansible tasks will run until you see this hint:</p><p><img src=/images/2020_02_14_16_09_18_602x375.jpg alt=/images/2020_02_14_16_09_18_602x375.jpg></p><p>Click <code>Continue to hosted engine deployment</code> to continue.</p><h3 id=3-hosted-engine>3. Hosted Engine</h3><p>Before continue, manually install the rpms in <code>instance1.com</code>:</p><pre><code># yum install -y ./ovirt-engine-appliance-4.3-20200127.1.el7.x86_64.rpm
# rpm -qa | grep ovirt-engine-appliance
ovirt-engine-appliance-4.3-20200127.1.el7.x86_64
</code></pre><p>Fill the engine vm&rsquo;s configuration infos:</p><p><img src=/images/2020_02_14_16_23_51_518x881.jpg alt=/images/2020_02_14_16_23_51_518x881.jpg></p><p>Fill in admin portal password(this password will be used in web login) and continue:</p><p><img src=/images/2020_02_14_16_25_19_817x557.jpg alt=/images/2020_02_14_16_25_19_817x557.jpg></p><p>Examine the configuration and click <code>Prepare VM</code>:</p><p><img src=/images/2020_02_14_16_25_19_817x557.jpg alt=/images/2020_02_14_16_25_19_817x557.jpg></p><p>Wait for about half an hour to see deployment successful:</p><p><img src=/images/2020_02_14_17_00_27_713x388.jpg alt=/images/2020_02_14_17_00_27_713x388.jpg></p><p>Keep the default configuration:</p><p>engine vm&rsquo;s storage configuration will use Gluster, path will be Gluster&rsquo;s
engine volumn, and its parameter is:</p><p><code>backup-volfile-servers=instance2.com:instance3.com</code></p><p>for preventing the single-node issue for Gluster.</p><p><img src=/images/2020_02_14_17_17_52_772x455.jpg alt=/images/2020_02_14_17_17_52_772x455.jpg></p><p>Click <code>Finish deployment</code>, and wait for a break:</p><p><img src=/images/2020_02_14_17_20_30_817x433.jpg alt=/images/2020_02_14_17_20_30_817x433.jpg></p><p>Seeing this means deploy succeeded:</p><p><img src=/images/2020_02_14_17_40_22_591x535.jpg alt=/images/2020_02_14_17_40_22_591x535.jpg></p><p>Refresh the status:</p><p><img src=/images/2020_02_14_17_43_43_1058x513.jpg alt=/images/2020_02_14_17_43_43_1058x513.jpg></p><h3 id=4-portal>4. Portal</h3><p>Visit <code>engineinstance.com</code> in host machine(ArchLinux):</p><p><img src=/images/2020_02_14_17_47_13_767x501.jpg alt=/images/2020_02_14_17_47_13_767x501.jpg></p><p>Click <code>Administration Portal</code>:</p><p><img src=/images/2020_02_14_17_48_30_499x308.jpg alt=/images/2020_02_14_17_48_30_499x308.jpg></p><p>admin page is like following:</p><p><img src=/images/2020_02_14_17_50_57_1221x561.jpg alt=/images/2020_02_14_17_50_57_1221x561.jpg></p><p>ssh into engine vm and check the disk partitions:</p><pre><code># ssh root@10.20.30.34
root@10.20.30.34's password:
Last login: Fri Feb 14 17:25:51 2020 from 192.168.1.1
[root@engineinstance ~]#df -h
Filesystem               Size  Used Avail Use% Mounted on
devtmpfs                 1.9G     0  1.9G   0% /dev
tmpfs                    1.9G   12K  1.9G   1% /dev/shm
tmpfs                    1.9G  8.9M  1.9G   1% /run
tmpfs                    1.9G     0  1.9G   0% /sys/fs/cgroup
/dev/mapper/ovirt-root   8.0G  2.3G  5.8G  29% /
/dev/mapper/ovirt-home  1014M   33M  982M   4% /home
/dev/mapper/ovirt-tmp    2.0G   33M  2.0G   2% /tmp
/dev/mapper/ovirt-var     20G  437M   20G   3% /var
/dev/vda1               1014M  157M  858M  16% /boot
/dev/mapper/ovirt-log     10G   45M   10G   1% /var/log
/dev/mapper/ovirt-audit 1014M   34M  981M   4% /var/log/audit
tmpfs                    379M     0  379M   0% /run/user/0
</code></pre><h3 id=5-create-the-first-vm>5. Create The First VM</h3><h4 id=51-add-iso-storage-domain>5.1 Add ISO storage Domain</h4><p>Login in to <code>instance1.com</code>, configure nfs share storage for holding ISO
images:</p><pre><code>[root@instance1 ]# mkdir -p /isoimages
[root@instance1 ]# chown 36:36 -R /isoimages/
[root@instance1 ]# chmod 0755 -R /isoimages/
[root@instance1 ]# vi /etc/exports
[root@instance1 ]# cat /etc/exports
/isoimages *(rw,sync,no_subtree_check,all_squash,anonuid=36,anongid=36)
[root@instance1 ]# systemctl enable --now  nfs.service   
Created symlink from /etc/systemd/system/multi-user.target.wants/nfs-server.service to /usr/lib/systemd/system/nfs-server.service.
</code></pre><p>In ovirt manager portal , click <code>Storage</code>-><code>Storage Domain</code>, click <code>New Domain</code>:</p><p><img src=/images/2020_02_14_19_29_49_1046x337.jpg alt=/images/2020_02_14_19_29_49_1046x337.jpg></p><p>Fill in name and path information:</p><p><img src=/images/2020_02_14_19_42_32_1006x355.jpg alt=/images/2020_02_14_19_42_32_1006x355.jpg></p><p>Finished adding isoimages:</p><p><img src=/images/2020_02_14_19_32_52_919x263.jpg alt=/images/2020_02_14_19_32_52_919x263.jpg></p><h4 id=52-upload-iso>5.2 Upload iso</h4><p>Login to engien vm(engineinstance.com), download the iso from official site,
we take ubuntu16.04.6 for example:</p><pre><code>[root@engineinstance ~]# ovirt-iso-uploader -i isoimages upload ./ubuntu-16.04.6-server-amd64.iso 
Please provide the REST API password for the admin@internal oVirt Engine user (CTRL+D to abort): 
Uploading, please wait...
INFO: Start uploading ./ubuntu-16.04.6-server-amd64.iso 
Uploading: [########################################] 100%
INFO: ./ubuntu-16.04.6-server-amd64.iso uploaded successfully
</code></pre><h4 id=53-create-vm>5.3 Create VM</h4><p>Compute-> Virtual Machines, click <code>new</code> button:</p><p><img src=/images/2020_02_14_19_48_14_846x313.jpg alt=/images/2020_02_14_19_48_14_846x313.jpg>
Fill in informations:</p><p><img src=/images/2020_02_14_19_50_24_673x641.jpg alt=/images/2020_02_14_19_50_24_673x641.jpg></p><p>Click advanced options, select <code>Boot Options</code>, then attach uploaded iso:</p><p><img src=/images/2020_02_14_19_52_09_917x511.jpg alt=/images/2020_02_14_19_52_09_917x511.jpg></p><p>Click Disks, then click <code>new</code>:</p><p><img src=/images/2020_02_14_20_00_20_1178x316.jpg alt=/images/2020_02_14_20_00_20_1178x316.jpg></p><p>Fill in options:</p><p><img src=/images/2020_02_14_20_01_54_731x394.jpg alt=/images/2020_02_14_20_01_54_731x394.jpg></p><p>Click this new machine, and select <code>run->run once</code>:</p><p><img src=/images/2020_02_14_19_53_19_852x380.jpg alt=/images/2020_02_14_19_53_19_852x380.jpg></p><p>Click OK for installation:</p><p><img src=/images/2020_02_14_19_54_06_599x528.jpg alt=/images/2020_02_14_19_54_06_599x528.jpg></p><p>The installation image will be shown:</p><p><img src=/images/2020_02_14_19_55_49_641x534.jpg alt=/images/2020_02_14_19_55_49_641x534.jpg></p><p>Configure installation options and wait until installation finished.<br>Since we use nested virtualization, the installation step will take a very
long time(>1h) for installing the os. For speedup, considering use NVME ssd
for locating the vm&rsquo;s qcow2 files. Or use 3 physical servers.</p><p>On vm portal we could see our newly created vm:</p><p><img src=/images/2020_02_14_21_03_33_764x485.jpg alt=/images/2020_02_14_21_03_33_764x485.jpg></p><p>Examine the vms on instance1.com:</p><pre><code>[root@instance1 isoimages]# virsh -r list
 Id    Name                           State
----------------------------------------------------
 2     HostedEngine                   running
 4     ubuntu1604                     running
</code></pre><h3 id=6-create-vm-using-template>6. Create vm using template</h3><h4 id=61-create-template>6.1 Create template</h4><p>Create template via:</p><p><img src=/images/2020_02_14_21_49_05_743x666.jpg alt=/images/2020_02_14_21_49_05_743x666.jpg></p><p>Check the status of template:</p><p><img src=/images/2020_02_14_21_50_02_733x224.jpg alt=/images/2020_02_14_21_50_02_733x224.jpg></p><h4 id=62-create-vm>6.2 Create vm</h4><p>Create new vm using template:</p><p><img src=/images/2020_02_14_21_57_36_912x668.jpg alt=/images/2020_02_14_21_57_36_912x668.jpg></p><p>Start the machine and check result:</p><p><img src=/images/2020_02_14_22_05_30_1148x360.jpg alt=/images/2020_02_14_22_05_30_1148x360.jpg></p><h3 id=7-add-hosts>7. Add hosts</h3><p>In engine vm, add following items:</p><pre><code>
</code></pre><p>Then we add hosts of <code>instance2.com</code> and <code>instance3.com</code>:</p><p><img src=/images/2020_02_14_22_16_23_904x687.jpg alt=/images/2020_02_14_22_16_23_904x687.jpg></p><p>Result:</p><p><img src=/images/2020_02_14_22_17_48_860x208.jpg alt=/images/2020_02_14_22_17_48_860x208.jpg></p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/02/10/upgradekernelforrhel74/>UpgradeKernelForRHEL74</a></h1><span class=post-date>Feb 10, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=online-steps>Online Steps</h3><p>rhel74, default kernel is:</p><pre><code># uname -a
Linux node 3.10.0-693.el7.x86_64 #1 SMP Thu Jul 6 19:56:57 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux
</code></pre><p>Configure repo and install newer kernel:</p><pre><code># rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
# wget https://www.elrepo.org/elrepo-release-7.0-4.el7.elrepo.noarch.rpm
# rpm -ivh elrepo-release-7.0-4.el7.elrepo.noarch.rpm
# yum update -y
# yum --disablerepo=&quot;*&quot; --enablerepo=&quot;elrepo-kernel&quot; list available
Loaded plugins: product-id, search-disabled-repos, subscription-manager
This system is not registered with an entitlement server. You can use subscription-manager to register.
elrepo-kernel                                                                                                                          | 2.9 kB  00:00:00     
elrepo-kernel/primary_db                                                                                                               | 1.9 MB  00:00:58     
Available Packages
kernel-lt.x86_64                                                               4.4.213-1.el7.elrepo                                              elrepo-kernel
kernel-lt-devel.x86_64                                                         4.4.213-1.el7.elrepo                                              elrepo-kernel
kernel-lt-doc.noarch                                                           4.4.213-1.el7.elrepo                                              elrepo-kernel
kernel-lt-headers.x86_64                                                       4.4.213-1.el7.elrepo                                              elrepo-kernel
kernel-lt-tools.x86_64                                                         4.4.213-1.el7.elrepo                                              elrepo-kernel
kernel-lt-tools-libs.x86_64                                                    4.4.213-1.el7.elrepo                                              elrepo-kernel
kernel-lt-tools-libs-devel.x86_64                                              4.4.213-1.el7.elrepo                                              elrepo-kernel
kernel-ml-devel.x86_64                                                         5.5.2-1.el7.elrepo                                                elrepo-kernel
kernel-ml-doc.noarch                                                           5.5.2-1.el7.elrepo                                                elrepo-kernel
kernel-ml-headers.x86_64                                                       5.5.2-1.el7.elrepo                                                elrepo-kernel
kernel-ml-tools.x86_64                                                         5.5.2-1.el7.elrepo                                                elrepo-kernel
kernel-ml-tools-libs.x86_64                                                    5.5.2-1.el7.elrepo                                                elrepo-kernel
kernel-ml-tools-libs-devel.x86_64                                              5.5.2-1.el7.elrepo                                                elrepo-kernel
perf.x86_64                                                                    5.5.2-1.el7.elrepo                                                elrepo-kernel
python-perf.x86_64                                                             5.5.2-1.el7.elrepo                                                elrepo-kernel
# yum --enablerepo=elrepo-kernel install kernel-ml
# sudo sed -i 's/^GRUB_DEFAULT.*/GRUB_DEFAULT=0/' /etc/default/grub
# sudo grub2-mkconfig -o /boot/grub2/grub.cfg
# sudo reboot
</code></pre><p>Check the kernel version is <code>5.5.2-1</code>:</p><pre><code># uname -a
Linux node 5.5.2-1.el7.elrepo.x86_64 #1 SMP Tue Feb 4 16:29:48 EST 2020 x86_64 x86_64 x86_64 GNU/Linux
# cat /etc/redhat-release 
Red Hat Enterprise Linux Server release 7.4 (Maipo)
</code></pre><h3 id=offline-steps>Offline Steps</h3><p>Scp the rpm into the server and install it via:</p><pre><code># scp ./kernel-ml-5.5.2-1.el7.elrepo.x86_64.rpm vagrant@xxx.xxx.xxx.xxx:/home/vagrant
# ssh into xxx.xxx.xxx.xxx
...................
$ sudo sed -i 's/^GRUB_DEFAULT.*/GRUB_DEFAULT=0/' /etc/default/grub
$ sudo grub2-mkconfig -o /boot/grub2/grub.cfg
Generating grub configuration file ...
Found linux image: /boot/vmlinuz-5.5.2-1.el7.elrepo.x86_64
Found initrd image: /boot/initramfs-5.5.2-1.el7.elrepo.x86_64.img
Found linux image: /boot/vmlinuz-3.10.0-693.el7.x86_64
Found initrd image: /boot/initramfs-3.10.0-693.el7.x86_64.img
Found linux image: /boot/vmlinuz-0-rescue-61b21bac36de423f82052de06e3a892b
Found initrd image:
/boot/initramfs-0-rescue-61b21bac36de423f82052de06e3a892b.img
done
$ sudo reboot
</code></pre><p>Check:</p><pre><code>$ uname -a
Linux node 5.5.2-1.el7.elrepo.x86_64 #1 SMP Tue Feb 4 16:29:48 EST 2020 x86_64 x86_64 x86_64 GNU/Linux
$ cat /etc/redhat-release 
Red Hat Enterprise Linux Server release 7.4 (Maipo)
</code></pre><h3 id=related-packages>Related packages</h3><p>Manually download them from
<code>http://elrepo.reloumirrors.net/kernel/el7/x86_64/RPMS/</code>, related rpms is
listed as:</p><pre><code># pwd
/media/sda/rhel74NewKernel
# ls
kernel-ml-5.5.2-1.el7.elrepo.x86_64.rpm
kernel-ml-tools-5.5.2-1.el7.elrepo.x86_64.rpm
kernel-ml-devel-5.5.2-1.el7.elrepo.x86_64.rpm
kernel-ml-tools-libs-devel-5.5.2-1.el7.elrepo.x86_64.rpm
</code></pre><h3 id=中文更新步骤>中文更新步骤</h3><p>主线内核可从
<code>http://elrepo.reloumirrors.net/kernel/el7/x86_64/RPMS/</code>下载，文件列表如下：</p><pre><code># pwd
/media/sda/rhel74NewKernel
# ls
kernel-ml-5.5.2-1.el7.elrepo.x86_64.rpm
kernel-ml-headers-5.5.2-1.el7.elrepo.x86_64.rpm
kernel-ml-tools-libs-devel-5.5.2-1.el7.elrepo.x86_64.rpm
kernel-ml-devel-5.5.2-1.el7.elrepo.x86_64.rpm
kernel-ml-tools-5.5.2-1.el7.elrepo.x86_64.rpm
</code></pre><p>注： 如果只需要升级内核，则只需要 <code>kernel-ml-5.5.2-1.el7.elrepo.x86_64.rpm</code>
一个包就足够，如果编译时需要内核头文件依赖，则有可能需要其他几个包。可以根据需要自行安装。</p><p>离线更新，上传包到rhel74服务器上，:</p><pre><code># scp ./*.rpm vagrant@xxx.xxx.xxx.xxx:/home/vagrant
# ssh into xxx.xxx.xxx.xxx
...................
$ sudo yum install -y ./kernel-ml-5.5.2-1.el7.elrepo.x86_64.rpm
$ sudo sed -i 's/^GRUB_DEFAULT.*/GRUB_DEFAULT=0/' /etc/default/grub
$ sudo grub2-mkconfig -o /boot/grub2/grub.cfg
$ sudo reboot
</code></pre><p>检查更新后结果:</p><pre><code>$ uname -a
Linux node 5.5.2-1.el7.elrepo.x86_64 #1 SMP Tue Feb 4 16:29:48 EST 2020 x86_64 x86_64 x86_64 GNU/Linux
$ cat /etc/redhat-release 
Red Hat Enterprise Linux Server release 7.4 (Maipo)
</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/01/21/promoxzfstips/>Promoxzfstips</a></h1><span class=post-date>Jan 21, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=steps>Steps</h3><p>之前:</p><p><img src=/images/2020_01_21_16_18_33_1171x255.jpg alt=/images/2020_01_21_16_18_33_1171x255.jpg></p><p>ssh登录:</p><p><code>./MegaCli64 -LDInfo -LALL -aAll</code>查看VD信息， 其中VD0（600G）不需要动.</p><p><img src=/images/2020_01_21_16_20_59_871x328.jpg alt=/images/2020_01_21_16_20_59_871x328.jpg></p><p><code>./MegaCli64 -PDList -aAll | grep -i adapter</code>得到adapter数值:</p><p><img src=/images/2020_01_21_16_22_31_783x47.jpg alt=/images/2020_01_21_16_22_31_783x47.jpg></p><p>删除VD1-VD3:</p><pre><code># ./MegaCli64 -cfglddel -L1 -a0
# ./MegaCli64 -cfglddel -L2 -a0
# ./MegaCli64 -cfglddel -L3 -a0
</code></pre><p>当前VD：</p><p><img src=/images/2020_01_21_16_23_26_850x391.jpg alt=/images/2020_01_21_16_23_26_850x391.jpg></p><p>查看PD对应磁盘:</p><pre><code># ./MegaCli64 -PDList -aAll | more
两个600G的是0和1, 其他的随便动
</code></pre><p>查看多少块盘:</p><pre><code># ./MegaCli64 -PDList -aAll |  grep 'Slot Number'
</code></pre><p><img src=/images/2020_01_21_16_25_57_813x589.jpg alt=/images/2020_01_21_16_25_57_813x589.jpg></p><p>这里注意,<code>2,3</code> 是没有，从<code>4~27</code>为slot number.</p><p>得到Enclosure ID:</p><pre><code># ./MegaCli64 -PDList -aAll | grep 'Enclosure'
为9
</code></pre><p>开始做24个raid0:</p><pre><code># ./MegaCli64 -CfgLdAdd -r0 [9:4] -a0
# ./MegaCli64 -CfgLdAdd -r0 [9:5] -a0
......
# ./MegaCli64 -CfgLdAdd -r0 [9:26] -a0
# ./MegaCli64 -CfgLdAdd -r0 [9:27] -a0
</code></pre><p>脚本:</p><p><img src=/images/2020_01_21_16_30_40_450x557.jpg alt=/images/2020_01_21_16_30_40_450x557.jpg></p><p><code>lsblk</code>查看磁盘信息:</p><p><img src=/images/2020_01_21_16_31_31_562x891.jpg alt=/images/2020_01_21_16_31_31_562x891.jpg></p><p>删除多余分区, sdb/sdn/sdr:</p><p><img src=/images/2020_01_21_16_32_53_643x819.jpg alt=/images/2020_01_21_16_32_53_643x819.jpg></p><h3 id=add-zfs-pool>add zfs pool</h3><p>命令行下添加:</p><pre><code># zpool create -f -o ashift=12 vmpool raidz2 /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi
# zpool add -f -o ashift=12 vmpool raidz2 /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq
# zpool add -f -o ashift=12 vmpool raidz2 /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx /dev/sdy
# zpool list
NAME     SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
vmpool   130T  1.97M   130T         -     0%     0%  1.00x  ONLINE  -
# zfs list
NAME     USED  AVAIL  REFER  MOUNTPOINT
vmpool   819K  89.9T   205K  /vmpool
</code></pre><p>Add in proxmox:</p><p><img src=/images/2020_01_21_16_34_13_609x424.jpg alt=/images/2020_01_21_16_34_13_609x424.jpg></p><p>设置参数:</p><p><img src=/images/2020_01_21_16_34_50_619x228.jpg alt=/images/2020_01_21_16_34_50_619x228.jpg></p><p>可用：</p><p><img src=/images/2020_01_21_16_35_18_891x159.jpg alt=/images/2020_01_21_16_35_18_891x159.jpg></p><p>使用方法:</p><p><img src=/images/2020_01_21_16_37_23_712x533.jpg alt=/images/2020_01_21_16_37_23_712x533.jpg></p></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/01/16/tipsonnextcloud/>tipsOnnextcloud</a></h1><span class=post-date>Jan 16, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><p>Dockerfile:</p><pre><code>version: '3' 

services:

  proxy:
    image: jwilder/nginx-proxy:alpine
    labels:
      - &quot;com.github.jrcs.letsencrypt_nginx_proxy_companion.nginx_proxy=true&quot;
    container_name: nextcloud-proxy
    networks:
      - nextcloud_network
    ports:
      - 0.0.0.0:80:80
      - 0.0.0.0:443:443
    volumes:
      - ./proxy/conf.d:/etc/nginx/conf.d:rw
      - ./proxy/vhost.d:/etc/nginx/vhost.d:rw
      - ./proxy/html:/usr/share/nginx/html:rw
      - ./proxy/certs:/etc/nginx/certs:ro
      - /etc/localtime:/etc/localtime:ro
      - /var/run/docker.sock:/tmp/docker.sock:ro
    restart: unless-stopped
  
  letsencrypt:
    image: jrcs/letsencrypt-nginx-proxy-companion
    container_name: nextcloud-letsencrypt
    depends_on:
      - proxy
    networks:
      - nextcloud_network
    volumes:
      - ./proxy/certs:/etc/nginx/certs:rw
      - ./proxy/vhost.d:/etc/nginx/vhost.d:rw
      - ./proxy/html:/usr/share/nginx/html:rw
      - /etc/localtime:/etc/localtime:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    restart: unless-stopped
  db:
    image: mariadb
    container_name: nextcloud-mariadb
    networks:
      - nextcloud_network
    volumes:
      - db:/var/lib/mysql
      - /etc/localtime:/etc/localtime:ro
    environment:
    # Create a root password for the maraiadb instance.
      - MYSQL_ROOT_PASSWORD=engine123
    # Create a password for the nextcloud users.  If you have to manually connect your database you would use the nextcloud user and this password.
      - MYSQL_PASSWORD=engine123
      - MYSQL_DATABASE=nextcloud
      - MYSQL_USER=nextcloud
    restart: unless-stopped
  
  app:
    image: nextcloud:latest
    container_name: nextcloud-app
    networks:
      - nextcloud_network
    depends_on:
      - letsencrypt
      - proxy
      - db
    volumes:
      - nextcloud:/var/www/html
      - ./app/config:/var/www/html/config
      - ./app/custom_apps:/var/www/html/custom_apps
      - ./app/data:/var/www/html/data
      - ./app/themes:/var/www/html/themes
      - /etc/localtime:/etc/localtime:ro
    environment:
    # The VIRTUAL_HOST and LETSENCRYPT_HOST should use the same publically reachable domain for your nextlcloud instance.
      - VIRTUAL_HOST=mynextcloud.mooo.com
      - LETSENCRYPT_HOST=mynextcloud.mooo.com
    # This needs to be a real email as it will be used by let's encrypt for your cert and is used to warn you about renewals.
      - LETSENCRYPT_EMAIL=feipyang@gmail.com
    restart: unless-stopped
  collab:
    image: collabora/code
    container_name: nextcloud-collab
    networks:
      - nextcloud_network
    depends_on:
      - proxy
      - letsencrypt
    cap_add:
     - MKNOD
    ports:
      - 0.0.0.0:9980:9980
    environment:
    # This nees to be the same as what you set your app domain too (ex: cloud.domain.tld).
    #- domain=cloud\\.DOMAIN\\.TDL
      - domain=mynextcloud\.mooo\.com
      - username=admin
    # Create a passoword for the collabora office admin page.
    #- password=CREATE-A-SECURE-PASSWORD-HERE
      - password=engine123
      - VIRTUAL_PORT=9980
      - extra_params=--o:ssl.enable=false --o:ssl.termination=true
#      - VIRTUAL_PROTO=https
#      - VIRTUAL_PORT=443
    # The VIRTUAL_HOST and LETSENCRYPT_HOST should use the same publically reachable domain for your collabora instance (ex: office.domain.tld).
      - VIRTUAL_HOST=myoffice.mooo.com
      - LETSENCRYPT_HOST=myoffice.mooo.com
    # This needs to be a real email as it will be used by let's encrypt for your cert and is used to warn you about renewals.
      - LETSENCRYPT_EMAIL=feipyang@gmail.com
    restart: unless-stopped 
volumes:
  nextcloud:
  db: 
  
networks:
  nextcloud_network:

</code></pre><p>For installing apps:</p><pre><code>sudo docker cp Client.php fc18391f0a0a:/var/www/html/lib/private/Http/Client/Client.php
Timeout from 30 to 300, then you could install apps. 
</code></pre></div><div class=post><h1 class=post-title><a href=http://purplepalmdash.github.io/blog/2020/01/13/harborarm64issue/>HarborArm64Issue</a></h1><span class=post-date>Jan 13, 2020<br><a class=a_cat href=http://purplepalmdash.github.io/categories/technology>Technology</a></span><h3 id=issue>Issue</h3><p>harbor-log instance launched first, but it complains:</p><pre><code>You are required to change your password immediately (password expired)
</code></pre><p>This is because we build the container images only have 90 days limitation.</p><h3 id=solution>Solution</h3><p>Rebuild all of the harbor images:</p><pre><code>root@node:~/harbor# cat harbor-core/Dockerfile 
FROM f9e2034f3a6d
COPY login.defs /etc/login.defs
COPY shadow /etc/shadow
root@node:~/harbor# cat harbor-core/shadow 
root:x:18074:0:99999:7:::
bin:x:18074:0:99999:7:::
daemon:x:18074:0:99999:7:::
messagebus:x:18074:0:99999:7:::
systemd-bus-proxy:x:18074:0:99999:7:::
systemd-journal-gateway:x:18074:0:99999:7:::
systemd-journal-remote:x:18074:0:99999:7:::
systemd-journal-upload:x:18074:0:99999:7:::
systemd-network:x:18074:0:99999:7:::
systemd-resolve:x:18074:0:99999:7:::
systemd-timesync:x:18074:0:99999:7:::
nobody:x:18074:0:99999:7:::
syslog:!:18074::::::

</code></pre><p>Then we build the image via following command:</p><pre><code># docker build -t goharbor/harbor-db:1.7.0-arm64 harbor-db/
</code></pre><p>We have to build all of the images:</p><pre><code>  docker build -t goharbor/chartmuseum-photon:v0.7.1-1.7.0-arm64 chartmuseum-photon/
  docker build -t goharbor/redis-photon:1.7.0-arm64 redis-photon/
  docker build -t goharbor/clair-photon:v2.0.7-1.7.0-arm64 clair-photon/
  docker build -t  goharbor/notary-server-photon:v0.6.1-1.7.0-arm64 notary-server-photon/
  docker build -t goharbor/notary-signer-photon:v0.6.1-1.7.0-arm64 notary-signer-photon
  docker build -t goharbor/harbor-registryctl:1.7.0-arm64 registry-photon
  docker build -t goharbor/registry-photon:v2.6.2-1.7.0-arm64 registry-photon/
  docker build -t goharbor/harbor-registryctl:1.7.0-arm64 harbor-registryctl/
  docker build -t goharbor/nginx-photon:1.7.0-arm64 nginx-photon/
  docker build -t goharbor/harbor-jobservice:1.7.0-arm64 harbor-jobservice/
  docker build -t goharbor/harbor-core:1.7.0-arm64 harbor-core/
  docker build -t goharbor/harbor-portal:1.7.0-arm64 harbor-portal/
  docker build -t goharbor/harbor-adminserver:1.7.0-arm64 harbor-adminserver/
  docker build -t goharbor/harbor-db:1.7.0-arm64 harbor-db/
</code></pre></div><ul class=pagination><li class=page-item><a href=/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/page/51/ class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a class=page-link href=/>1</a></li><li class=page-item><a class=page-link href=/page/2/>2</a></li><li class=page-item><a class=page-link href=/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/51/>51</a></li><li class="page-item active"><a class=page-link href=/page/52/>52</a></li><li class=page-item><a class=page-link href=/page/53/>53</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class=page-item><a class=page-link href=/page/240/>240</a></li><li class=page-item><a href=/page/53/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/page/240/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div><script src=http://purplepalmdash.github.io/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>